[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Demographic Data Analysis",
    "section": "",
    "text": "Preface"
  },
  {
    "objectID": "index.html#why-a-book-on-statistics-for-demographers",
    "href": "index.html#why-a-book-on-statistics-for-demographers",
    "title": "Applied Demographic Data Analysis",
    "section": "Why a book on statistics for demographers?",
    "text": "Why a book on statistics for demographers?\nDemographers have always been a mixture of sociologists, economists, statisticians, health researchers, and other broad sub-disciples of social science. As such, we bring with us a large amount of baggage from our respective academic life courses, and we often are trained by a wide variety of mentors and professors. It’s my perspective that our interdisciplinary experience is one of our greatest strengths as a group. Given that our training is often in one of a core set of home disciplines, we often have methodological training from said discipline, and this may not be a broad enough perspective to firmly ground us in the types of methods that demographers commonly employ. This is not the fault of the departments that trained us, it’s just a historical fact. So, why am I writing a book on statistics and data analysis aimed at demographers? I will give you three reasons:\n\nDemographers have to go beyond the sample. This is to say that our results and research is generally representative of a larger national or international population, and we do this explicitly in our models.\nWe demographers don’t use random samples for our analysis. Statistics books the world over are based on assumptions of random sampling and independence, while the data that we often have to, or desire to use, comes more than likely from a data source that was collected using a complex survey design. This is a big deal and we have to have training materials that instill this in our students early on in their careers.\nWeird data. As demographers, we often use data from lots of different places and if you were trained up to this point to believe that the linear model is the end-all be-all of statistical inference, I’ve got news for you friends, you’ve been misled. Categorical outcomes, counts, hierarchically structured, longitudinally collected, spatially referenced, just to name a few of such oddities, are ubiquitous in our field, and part of what makes our discipline so cool and interesting to newcomers.\n\nMy goal for this book is to take the lessons I’ve learned teaching statistics to a diverse and often cursorily trained group of students who have problems they care about, that they need to bring demographic data to bear upon. This is a challenge, and I have always been a stalwart proponent of teaching statistics and data analysis in a very applied manner. As such, this book won’t be going into rigorous proofs of estimators or devoting pages to expositions of icky algebra; instead it will focus on exploring modern methods of data analysis that in used by demographers every day, but not always taught in our training programs.\nAs someone who has learned much more of these methods by personal exploration than by formal study, I find that many of these methods are absent from the canon of social science statistics, but are both in great demand from people who hire us, and absolutely necessary to the demographer’s analytic toolkit. It’s a major goal of this book to de-mystify the process and to make it accessible to a wide audience, so I will always strive to illustrate the key aspects of the methods described herein, and ground the discussion of methods in applications.\n\nis this a cookbook?\n\n\nBroader picture of what i’ll cover\n\n\nWhat’s a demographer?\n\n\nWhat is applied demography?\n\n\nWhy we need to see this stuff?\n\n\nWhy R is a good option for applied demography?\n\nwhy write code?\n\n\n\nMention packages earlier - talk about later"
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "1  Preface",
    "section": "",
    "text": "2 R in applied demography"
  },
  {
    "objectID": "preface.html#why-r",
    "href": "preface.html#why-r",
    "title": "1  Preface",
    "section": "2.1 Why R?",
    "text": "2.1 Why R?\nI’ve used R for twenty years. I was also trained in SPSS and SAS along the way, by various mentors. Some tried to get me to learn more general purpose languages like Delphi (of all things) or Perl, or Basic, and I’ve been chastised for not knowing the depths of Python, but R presents a nimble and rigorous platform to do demography. My top three reasons for teaching and using R are:\n\nIt’s free - This is important, because, why should we pass along more costs to people, especially our students? This also make R code accessible to people, worldwide.\nIt’s the hotbed of methodological development. The R ecosystem has thousands of packages that represent the bleeding edge of data analysis, visualization and data science. This makes R attractive because it can pivot quickly to adopt new methods, which often lag in their development in other environments.\nIt has a supportive community of users. While there are some debates over how friendly some R users are to new users, overall, after spending 20 years in the R community, I’ve personally assisted hundreds of users, and been personally helped by many others. The open source nature of R lends itself to sharing of ideas and collaboration between users.\n\n\n2.1.1 My assumptions in this book\nIn statistics we always make assumptions, often these are wrong, but we adapt to our mistakes daily. My assumptions about who is reading this book are:\n\nYou are interested in learning more about R.\nYou are likely a student or professional interested in demography or population research.\nYou have likely been exposed to other statistical platforms and are curious about R, in conjunction with 1 and 2 above.\nYou may be an avid R user from another strange and exotic discipline, but are interested in how demographers do research.\nYou want to see how to do things instead of being bombarded with theoretical and often unnecessary gate-keeping mathematical treatments of statistical models.\n\nI think if any of these assumptions are true, you’re in the right place. That being said, this book is not a review of all of statistics, nor is it an encyclopedic coverage of the R language and ecosystem. I image the latter being on the same scale of hopelessness as the search for the Holy Grail or the fountain of youth. People have died for such fool hearty quests, I’m not falling on my sword here folks."
  },
  {
    "objectID": "rintro.html",
    "href": "rintro.html",
    "title": "2  Introduction to R",
    "section": "",
    "text": "3 Introduction to R\nThis chapter is devoted to introducing R to new users. R was first implemented in the early 1990’s by Robert Gentleman and Ross Ihaka, both faculty members at the University of Auckland. It is an open source software system that specializes in statistical analysis and graphics. R is its own programming language and the R ecosystem includes over 18,000 user-contributed additions to the software, known as packages."
  },
  {
    "objectID": "rintro.html#welcome-to-r.",
    "href": "rintro.html#welcome-to-r.",
    "title": "2  Introduction to R",
    "section": "3.1 Welcome to R.",
    "text": "3.1 Welcome to R.\nIf you’re coming to R from SAS, there is no data step. There are no procs. The SAS and R book Kleinman and Horton (2014) is very useful for going between the two programs.\nIf you’re coming from SPSS and you’ve been using the button clicking method, be prepared for a steep learning curve. If you’ve been writing syntax in SPSS, you’re at least used to having to code. There’s a good book for SAS and SPSS users by Bob Meunchen at the Univ. of Tennessee here, which may be of some help.\nStata users have fewer official publications at their fingertips to ease the transition to R, but I have always thought that the two were very similar. If you search the internet for information related to R and Stata, you will find a myriad of (somewhat dated) blog posts on which one is better for “data science”, how to “get started” with either and so forth. What is really needed is a text similar to Kleinman and Horton (2014) which acts as a crosswalk between the two programs."
  },
  {
    "objectID": "rintro.html#r-and-rstudio",
    "href": "rintro.html#r-and-rstudio",
    "title": "2  Introduction to R",
    "section": "3.2 R and Rstudio",
    "text": "3.2 R and Rstudio\nThe Rgui is the base version of R, but is not very good to program in. Rstudio is much better, as it gives you a true integrated development environment (IDE), where you can write code in one window, see results in others, see locations of files, and see objects you’ve created. To get started, you should download the R installer for your operating system. Windows and Mac have installer files, and Linux users will install R using their preferred package manager.\nDownload R from CRAN. If you’re on Windows, I would also highly recommend you install Rtools, because it gives you c++ and Fortran compilers, which many packages need to be installed.\nRstudio can be downloaded for free here.\nI would recommend installing the base R program from CRAN first then (for Windows users) install Rtools, then install Rstudio, in that order."
  },
  {
    "objectID": "rintro.html#introduction-to-rstudio",
    "href": "rintro.html#introduction-to-rstudio",
    "title": "2  Introduction to R",
    "section": "3.3 Introduction to Rstudio",
    "text": "3.3 Introduction to Rstudio\nAgain, each operating system has its own binary for Rstudio, so pick the one that matches your operating system. Rstudio typically has 4 sub-windows open at any given time.\nRstudio is an open source Integrated Development Environment (IDE) for R. It is a much better interface for using R because it allows you to write code in multiple languages, navigate your computer’s files, and see your output in a very nice single place. The Rstudio IDE has several components that we will explore.\n\n\n3.3.1 Code window/Source editor pane\n\nThis is where you write your R code. You can write R code in a few different file types (more on this later), but the basic one is an R script, with file extension .R\nThe code window allows you to write and execute your code one line at a time, or to run an entire script at once. I use this to develop new code and when I want to test if things work (a VERY common exercise when writing any code).\nTo run a single line of code, put your cursor on the line and hit Ctrl-Enter (on Mac CMD-Enter also does this)\nTo run multiple lines of code, highlight the lines you want to run and do the same thing\n\n\n\n3.3.2 Console Pane\n\nThis is where most of your non-graphical output will be shown. Any numeric output will appear here, as well as any warnings or error messages. In R a warning doesn’t necessarily mean something went wrong, its just R’s polite way of telling you to pay attention.\nAn Error means something did go wrong. This is often because you left off a ) or a, sometimes because you misspelled something. I routinely spell length as lenght which causes R to print an error message. If you see an error, don’t worry, R will print some kind of message telling you what went wrong.\nR’s output is in plain text, although we can produce much prettier output using other output methods, and we’ll talk more about that later.\nYou can type commands or code into the console as well, and you’ll immediately get the result, versus if you write it in the Source/Code window, you have to run it to see the result. I will often work in the console when I want to get “fast” answers, meaning little checks that I will often do to see the value of something.\n\n\n\n3.3.3 Environment or Workspace browser pane\n\nThe R environment is where any object you create is stored. In R, anything you read in or create with your code is called an object, and R is said to be an object oriented programming language.\nDepending on the type of object something is, you may be able to click on the object in the environment and see more about it.\nFor instance if the object is a data frame, R will open it in a viewer where you can explore it like a spreadsheet, and sort and filter it as well.\nOther objects may not do anything when you click on them.\nThere is also a useful History tab here that shows you recently executed lines of code from the console or the code pane.\n\n\n\n3.3.4 Files/Output/Help pane\n\nThe files and output area is where you can interact with files on your local computer, such as data files or code files, or images that R can open.\nThis area also has a plots window that will display plots you create in R either via typing directly into the console or by running a line(s) of code from the source/code pane.\nThere is also a very valuable part of this pane that lets you access the help system in R. If you are either looking for something, or you just want to explore the functions, you can get access to all of this here.\n\n\n\n3.3.5 R file types\n.R files R uses a basic text file with the .R extension. This type of file is useful if you’re going to write a function or do some analysis and don’t want to have formatted output or text. You can use these files for everything, but they are limited in their ability to produce reports and formatted output, so I recommend people work with R Markdown files instead.\n.Rmd files Rstudio uses a form of the markdown formatting language, called R Markdown, for creating formatted documents that include code, tables, figures and statistical output. This book is written in R Markdown!\nR Markdown is nice for lots of reasons, such as the ability to insert latex equations into documents.\n\\[\n{y_i \\sim Normal (x'\\beta, \\sigma_2)}\n\\]\nor to include output tables directly into a document:\n\nlibrary(broom)\nlibrary(pander)\nfit &lt;- lm(imr~tfr+pcturban+pctlt15_2018+pctwomcontra_all, \n          data = prb)\npander(broom::tidy(fit))\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n6.209\n6.551\n0.9478\n0.3446\n\n\ntfr\n3.392\n2.006\n1.691\n0.09274\n\n\npcturban\n-0.0923\n0.04553\n-2.028\n0.04425\n\n\npctlt15_2018\n0.8699\n0.2441\n3.564\n0.0004798\n\n\npctwomcontra_all\n-0.2114\n0.06018\n-3.512\n0.0005757\n\n\n\n\n\nThis allows you to make tables in Rmarkdown without having to do non-repeatable tasks in Word or some other program. You can basically do your entire analysis, or a sideshow for a presentation, or an entire paper, including bibliography, in Rstudio.\n\n\n3.3.6 R projects\nRstudio allows you to create a R project, which basically sets up a specific location to store R code for a given project you may be doing. For instance, this book is a single R project, which helps me organize all the chapters, bibliographies, figures, etc.\nR projects also allow you to use version control, including Git and SVN, to collaborate and share code and data with others.\n\n\n3.3.7 R data files\nR allows you to read and write its own native data formats, as well as read and write text formatted files and data files from other statistical software packages. Two native R data formats are .rds and .rdata formats. .rds files allow you to save a single R object to an external files, while .rdata files allow you to save one or more objects to a file.\nHere is a short example of doing this, where I create 2 vectors, x and y and save them.\n\nx &lt;- c(1, 2,3)\n\ny &lt;- c(4, 5, 6)\n\nsaveRDS(x, \n        file=\"~/x.rds\")\n\nsave(list=c(\"x\",\"y\"),\n     file=\"xy.rdata\")\n\nI can also load these into R again:\n\nreadRDS(file = \"~/x.rds\")\n\n[1] 1 2 3\n\nload(\"xy.rdata\")\n\nStandard methods for importing text data such as comma separated value or tab delimited files can be read into R using read.csv() or read.table() and similar writing functions are available.\nTo read in a dataset from another statistical package, I recommend using the haven package. It allows you to read and write SAS (both sas7bdat and xpt files), Stata, SPSS (both .por and .sav files).\nFor example, here I write out a dataframe containing x and y from above to a SAS version 7 file:\n\nxy &lt;- data.frame(x = x, y = y)\nxy\n\n  x y\n1 1 4\n2 2 5\n3 3 6\n\nlibrary(haven)\n\nwrite_sas(data = xy,\n          path = \"~/xy.sas7bdat\")\n\nWarning: `write_sas()` was deprecated in haven 2.5.2.\nℹ Please use `write_xpt()` instead.\n\n\nI will describe dataframes more later in the chapter.\nR also has packages for reading/writing such data formats as JSON, ESRI Shapefiles, Excel spreadsheets, Google Spreadsheets, DBF files, in addition to tools for connecting to SQL databases, and for interfacing with other statistics packages, such as Mplus, OpenBUGS, WinBUGS and various Geographic Information Systems."
  },
  {
    "objectID": "rintro.html#getting-help-in-r",
    "href": "rintro.html#getting-help-in-r",
    "title": "2  Introduction to R",
    "section": "3.4 Getting help in R",
    "text": "3.4 Getting help in R\nI wish I had a nickel for every time I ran into a problem trying to do something in R, that would be a lot of nickles. Here are some good tips for finding help in R:\n\nIf you know the name of a function you want to use, but just need help using it, try ?\n\n?lm\n\nIf you need to find a function to do something, try ??\n\n??\"linear model\"\n\nYou can also search the history of other R users questions by tapping into the RSiteSearch website, which is an archive of user questions to the R list serve. This can be used by tying RSiteSearch()\n\nRSiteSearch(\"heteroskedasticity\")\n\nSpeaking of which, there are multiple R user email list serves that you can ask questions to, or subscribe to daily digests from. These typically want an example of what you’re trying to do, referred to as a reproducible example. I wish I also had nickles for each question I’ve asked and answered on these forums.\nA good source for all things programming is the statistics branch of Stack Exchange, which has lots of contributed questions and answers, although many answers are either very snarky or wrong or for an old version of a library, so caveat emptor.\nYour local R guru or R user group. You would be surprised at how many people are R users, there may be one just down the hall, or in the cubicle next door. I relish the opportunity to talk to other R users, mostly because, even though I’ve used R for more than 20 years, I still learn so much by talking to others about how they use R.\n\nLastly, I want to be clear that there are often more than one way to do everything in R. Simple things like reading and writing a CSV data file can be accomplished by any of a handful of different functions found in different packages. If someone tells you that there is only one way to do something, they are usually wrong in such a statement, regarding R at least."
  },
  {
    "objectID": "rintro.html#r-packages",
    "href": "rintro.html#r-packages",
    "title": "2  Introduction to R",
    "section": "3.5 R packages",
    "text": "3.5 R packages\nR uses packages to store functions that do different types of analysis, so we will need to install lots of different packages to do different things. There are over 20,000 different packages currently for R. These are hosted on one of a number of repositories, such as the Comprehensive R Archive Network, or CRAN, which is the official repository for R packages. Other locations where authors store packages include R-Forge and BioconductoR. Many authors host packages in Github repositories, especially for development purposes.\nPackages are often organized into Task Views, which CRAN uses to organize packages into thematic areas. You can find a list of these Task Views here. There is not a task view for Demography, but there are ones for the Social Sciences, Econometrics, and Spatial Data to name a few. Task views allow users to download a lot of thematically linked packages in a single command, through the package ctv, or Cran Task Views. You can install this package by typing:\ninstall.packages(\"ctv\")\ninto Rstudio. Then you have to load the package by using the library() command:\nlibrary(ctv)\nwhich gives you access to the functions in that package. You don’t need to install the package again, unless you update your R software, but each time you start a new session (i.e. open Rstudio), you will have to load the library again. If you want to install all of the packages in the Social Science task view, you would type:\ninstall.views(\"SocialSciences\")\ninto R and it will install all of the packages under that task view, as of the writing of this sentence, include over 80 packages.\nI strongly recommend you install several packages prior to us beginning to use R, so you will not be distracted by this later. I’ve written a short script on my Github repository and you can use it by running:\n\nsource(\"https://raw.githubusercontent.com/coreysparks/Rcode/master/install_first_short.R\")\n\nThis will install a few dozen R packages that are commonly used for social science analysis and some other packages I find of use.\nYou only have to install a package once, but each time you start a R session, you must load the package to use its functions. You should also routinely update your installed packages using update.packages(ask = FALSE). This will update any packages that have new versions on CRAN. These often will contain bug fixes and new features. On CRAN, each package will have a README file that tells what has changed in the various versions. Here is one for one of my favorite packages tidycensus.\n\n3.5.1 Functions within packages\nEach package will have one or more functions within it, each doing a specific task. The default way to access all functions within a given package is to use the command library(packagename) to access the functions. Once loaded, all the functions will be accessible to you. Sometimes, different packages have functions with the same name, for example the base R library has the function lag(), which lag’s a time series, the dplyr library also has a function lag(), which does a similar task, but with different function arguments. If you have the dplyr library loaded, R will default to use its lag() function. If you want to access a specific function within a specific library, sometimes it is safest to use the library::function() syntax. So if I want to use base R’s lag() function, I could do\nstats::lag()\nto access that function specifically. How do you know when this happens? When you load a library, you will often see messages from R that functions have conflicts. For example, if I load dplyr, I see:\n\n\n\nConflict messages\n\n\nAs you use R more, you will learn which packages have conflicts, and often the developers of the packages will do this and rename the commonly conflicting functions. For example, the function to recode variables in the car package, car::recode() was renamed to car::Recode() to avoid conflicts with the dplyr::recode() function, as both are often used in the same analysis.\n\n3.5.1.1 More notes on functions\nFunctions in R are bits of code that do something, what they do depends on the code within them. For instance, the median() function’s underlying code can be seen by:\n\ngetAnywhere(median.default())\n\nA single object matching 'median.default' was found\nIt was found in the following places\n  package:stats\n  registered S3 method for median from namespace stats\n  namespace:stats\nwith value\n\nfunction (x, na.rm = FALSE, ...) \n{\n    if (is.factor(x) || is.data.frame(x)) \n        stop(\"need numeric data\")\n    if (length(names(x))) \n        names(x) &lt;- NULL\n    if (na.rm) \n        x &lt;- x[!is.na(x)]\n    else if (any(is.na(x))) \n        return(x[NA_integer_])\n    n &lt;- length(x)\n    if (n == 0L) \n        return(x[NA_integer_])\n    half &lt;- (n + 1L)%/%2L\n    if (n%%2L == 1L) \n        sort(x, partial = half)[half]\n    else mean(sort(x, partial = half + 0L:1L)[half + 0L:1L])\n}\n&lt;bytecode: 0x117f9e678&gt;\n&lt;environment: namespace:stats&gt;\n\n\nThis seems like a lot, I know, but it allows you to see all of the code under the hood of any function. Obviously, the more complicated the function, the more complicated the code. For instance, I can write my own simple function to find the mean of a sample:\n\nmymean &lt;- function(x,\n                   na.rm = FALSE){\n  sx &lt;- sum(x, \n            na.rm = FALSE)\n  nx &lt;- length(x)\n  mu &lt;- sx/nx\n  mu\n}\n\nmymean(x = c(1,2,3))\n\n[1] 2\n\n\nThis function only includes the basic machinery to calculate the arithmetic mean of a vector \\(x\\). The function has 2 arguments, x and na.rm. All R functions have one or more arguments that users must enter for the function to operate. Some arguments are required, while some are optional, also some arguments, such as the na.rm = FALSE, have default values. As mentioned earlier, to see all the information for a function’s arguments, use the help operator, ?. For example ?mean will show you the help documents for the mean() function\n\n\n\nMean Function Help\n\n\nWhen using a new function, it’s always advised to check out the help file to see all the arguments the function can take, because this is where you can choose alternative specifications for models and methods. These help files also contain the original citations for methods, so you can immediately check the source of the algorithms. The help files also contain a working example of how to use the function on data contained in R."
  },
  {
    "objectID": "rintro.html#your-r-user-environment",
    "href": "rintro.html#your-r-user-environment",
    "title": "2  Introduction to R",
    "section": "3.6 Your R user environment",
    "text": "3.6 Your R user environment\nWhen you begin an R session (generally by opening Rstudio) you will begin in your home directory. This is traditionally, on Windows, at 'C:/Users/yourusername/Documents' on Mac at '/Users/yourusername', and on Linux at '/users/yourusername'. There are files you can add to your home directory to specify starting options for R.\nYou can find information on setting up .Rprofile and .Renviron files on CRAN’s website. This allows you to setup packages that load every time R starts, to save API keys and other various options. These are completely optional and many R users never touch these.\nIf you’re not sure where you are you can type getwd(), for get working directory, and R will tell you:\n\ngetwd()\n\nIf you don’t like where you start, you can change it, by using setwd(), to set your working directory to a new location.\n\nsetwd(\"~\")\ngetwd()\n\nR projects will typically set the home folder for the project at the directory location of the project, so files associate with the project will always be in the same place. You can set this at the beginning of your R code file to ensure the code will look for data in a specific location."
  },
  {
    "objectID": "rintro.html#some-simple-r-examples",
    "href": "rintro.html#some-simple-r-examples",
    "title": "2  Introduction to R",
    "section": "3.7 Some Simple R examples",
    "text": "3.7 Some Simple R examples\nBelow we will go through a simple R session where we introduce some concepts that are important for R. I’m running these in an Rstudio session, in the\n\n3.7.1 R is a calculator\n\n#addition and subtraction\n3+7\n\n[1] 10\n\n3-7\n\n[1] -4\n\n\n\n#multiplication and division\n3*7\n\n[1] 21\n\n3/7\n\n[1] 0.4285714\n\n\n\n#powers\n3^2\n\n[1] 9\n\n3^3\n\n[1] 27\n\n\n\n#common math functions\nlog(3/7)\n\n[1] -0.8472979\n\nexp(3/7)\n\n[1] 1.535063\n\nsin(3/7)\n\n[1] 0.4155719\n\n\nR allows users to write custom functions as well. In general, if you find yourself writing the same code over and over again, you should probably just write a function and save it to your local user environment.\nFor example a very simple function is given below, it takes a variable \\(x\\) as an argument, and then exponentiates the value of the variable.\n\n#custom functions\nmyfun &lt;- function(x){\n  exp(x)\n}\n\nmyfun(.5)\n\n[1] 1.648721\n\nmyfun(-.1)\n\n[1] 0.9048374\n\n\nYou may want to save this function for future use, so you don’t have to write it over again. In general, this is why people write R packages, to store custom functions, but you can also save the function to an R script. One such way to do this is to use the dump() command.\n\ndump(\"myfun\", \n     file=\"myfun1.R\")\n\nOne way to load this function when you want to use it is to use the source() command, which loads any code in a given R script.\n\nsource(\"myfun1.R\")\n\nWhich will load this function into your local environment and you can use it. If you are interested in writing your own packages, I would highly recommend reading Wickham (n.d.)."
  },
  {
    "objectID": "rintro.html#variables-and-objects",
    "href": "rintro.html#variables-and-objects",
    "title": "2  Introduction to R",
    "section": "3.8 Variables and objects",
    "text": "3.8 Variables and objects\nIn R we assign values to objects (object-oriented programming). These can generally have any name, but some names are reserved for R. For instance you probably wouldn’t want to call something ‘mean’ because there’s a ‘mean()’ function already in R. For instance:\n\nx &lt;- 3\ny &lt;- 7\nx+y\n\n[1] 10\n\n\n\nx*y\n\n[1] 21\n\n\n\nlog(x*y)\n\n[1] 3.044522\n\n\nThe [1] in the answer refers to the first element of a vector, which brings us to…\n\n3.8.1 Vectors\nR thinks many objects are like a matrix, or a vector, meaning a row or column that contains either numbers or characters. One of R’s big selling points is that much of it is completely vectorized. Meaning, I can apply an operation along all elements of a vector without having to write a loop.\nFor example, if I want to multiply a vector of numbers by a constant, in SAS, I could do:\nfor (i in 1 to 5) x[i] &lt;- y[i]*5 end;\nbut in R, I can just do:\n\nx &lt;- c(3, 4, 5, 6, 7)\n#c() makes a vector\ny &lt;- 7\n\nx*y\n\n[1] 21 28 35 42 49\n\n\nR is also very good about using vectors, let’s say I wanted to find the third element of x:\n\nx[3]\n\n[1] 5\n\n\nor if I want to test if this element is 10\n\nx[3] == 10\n\n[1] FALSE\n\nx[3] != 10\n\n[1] TRUE\n\n\nor is it larger than another number:\n\nx[3] &gt; 3\n\n[1] TRUE\n\n\nor is any element of the whole vector greater than 3\n\nx &gt; 3\n\n[1] FALSE  TRUE  TRUE  TRUE  TRUE\n\n\nIf you want to see what’s in an object, use str(), for structure\n\nstr(x)\n\n num [1:5] 3 4 5 6 7\n\n\nand we see that x is numeric, and has the values that we made.\nWe can also see different characteristics of x\n\n#how long is x?\nlength(x)\n\n[1] 5\n\n#is x numeric?\nis.numeric(x)\n\n[1] TRUE\n\n#is x full of characters?\nis.character(x)\n\n[1] FALSE\n\n#is any element of x missing?\nis.na(x)\n\n[1] FALSE FALSE FALSE FALSE FALSE\n\n\n\n#now i'll modify x\nx &lt;- c(x, NA) #combine x and a missing value ==NA\nx\n\n[1]  3  4  5  6  7 NA\n\n#Now ask if any x's are missing\nis.na(x)\n\n[1] FALSE FALSE FALSE FALSE FALSE  TRUE\n\n\n\n3.8.1.1 Replacing elements of vectors\nAbove, we had a missing value in X, let’s say we want to replace it with another value. He we will use basic conditional logic, which exists in any programming language. The ifelse() function will evaluate a test statement, and depending on if that statement is true, it will assign a value, if the statement is false, R will assign another value. Here, we replace the missing value with \\(\\sqrt{7.2}\\), and leave the other values as they are.\n\nx &lt;- ifelse(test = is.na(x) == TRUE,\n            yes =  sqrt(7.2),\n            no =  x)\nx\n\n[1] 3.000000 4.000000 5.000000 6.000000 7.000000 2.683282"
  },
  {
    "objectID": "rintro.html#variable-types",
    "href": "rintro.html#variable-types",
    "title": "2  Introduction to R",
    "section": "3.9 Variable types",
    "text": "3.9 Variable types\nR stores data differently depending on the type of information contained. Common variables types in R are numeric, character, integer and factor.\nNumeric variables are just that, numbers. They can be whole numbers or decimal values. The best way to see if a variable is numeric is to use is.numeric(x), and R will return TRUE if the variable is numeric and FALSE if it is not.\n\nis.numeric(x)\n\n[1] TRUE\n\n\nLikewise, you can use is.character(), is.integer(), and is.factor to identify if a variable is of a given type. The class() function will also do this more generally:\n\nclass(x)\n\n[1] \"numeric\"\n\n\nCharacter and factor variables often store the same kind of information, and R (until recently) would always convert character variables to factors when data were read into R. This is the option getOption(\"stringsAsFactors\"), which used to default to True, but has recently changed. What’s the difference you ask? Character variables store information on strings, or text. This is one way to code categorical variables that are strings. Factors, on the other hand can store strings OR numbers as categorical variables, and can be ordered or unordered. Factors also allow for specific categories of the variable to be considered as reference categories, as are often used in many statistical procedures. Factor variables have “levels” which are the different values of the categorical variable, this implied a more complicated structure than simple character variables, which lack these qualities.\nYou can manipulate variables of one type into another, with some notable things to watch out for.\nHere are some examples:\n\n#create at numeric vector\n\nz &lt;-  c(1,2,3,4)\nclass(z)\n\n[1] \"numeric\"\n\n\nWe can convert this to a character vector using as.character()\n\nzc &lt;-  as.character(z)\nzc\n\n[1] \"1\" \"2\" \"3\" \"4\"\n\n\nLikewise, we can convert it to a factor type:\n\nzf &lt;- as.factor(z)\nzf\n\n[1] 1 2 3 4\nLevels: 1 2 3 4\n\nclass(zf)\n\n[1] \"factor\"\n\nis.ordered(zf)\n\n[1] FALSE\n\n\nand as an ordered factor:\n\nzfo &lt;- factor(zf, \n              ordered = TRUE)\nzfo\n\n[1] 1 2 3 4\nLevels: 1 &lt; 2 &lt; 3 &lt; 4\n\n\nAnother very useful variable type is the logical type. In R a logical variable is either a TRUE or FALSE value. I personally use this a lot in my work in both preliminary data analysis and data checking. We saw this used above, when we did is.na(x) == TRUE to check if the x variable was missing. We can see how this translates into a logical variable here:\n\nx &lt;- c(3, 4, 5, 6, 7, NA)\n\nz&lt;-is.na(x) #check if x is missing\n\nz\n\n[1] FALSE FALSE FALSE FALSE FALSE  TRUE\n\nclass(z)\n\n[1] \"logical\"\n\n\nIn practice, I use this with the I() function (more on this below) to do quick binary codes of a value:\n\nx &lt;- c(3, 4, 5, 6, 7)\n\ntable( I(x &gt;= 5) )\n\n\nFALSE  TRUE \n    2     3"
  },
  {
    "objectID": "rintro.html#dataframes",
    "href": "rintro.html#dataframes",
    "title": "2  Introduction to R",
    "section": "3.10 Dataframes",
    "text": "3.10 Dataframes\nTraditionally, R organizes variables into data frames, these are like a spreadsheet. The columns can have names, and the dataframe itself can have data of different types.\nHere we make a short data frame with three columns, two numeric and one factor:\n\nmydat &lt;- data.frame(\n  x = c(1,2,3,4,5, 6, 7, 8),\n  y = c(10, 20, 35, 57, 37, 21, 23, 25),\n  group = factor(c(\"A\", \"A\" ,\"A\", \"B\", \"B\", \"C\",\"C\",\"C\"))\n)\n\n#See the size of the dataframe\ndim(mydat)\n\n[1] 8 3\n\n#Open the dataframe in a viewer and just print it\nprint(mydat)\n\n  x  y group\n1 1 10     A\n2 2 20     A\n3 3 35     A\n4 4 57     B\n5 5 37     B\n6 6 21     C\n7 7 23     C\n8 8 25     C\n\n\n\n3.10.1 Accessing variables in dataframes\nR has a few different ways to get a variable from a data set. One way is the $ notation, used like dataset$variable, and another is to provide the column index or name of the variable. These three methods are illustrated below. The first tells R to get the variable named group from the data. The second tells R to get the column named group from the data, and the third tells R to get the third column from the data.\n\nmydat$group\n\n[1] A A A B B C C C\nLevels: A B C\n\nmydat['group'] \n\n  group\n1     A\n2     A\n3     A\n4     B\n5     B\n6     C\n7     C\n8     C\n\nmydat[,3]\n\n[1] A A A B B C C C\nLevels: A B C\n\n\nThe names() function is very useful for seeing all the column names of a dataset, without having to print any of the data.\n\nnames(mydat)\n\n[1] \"x\"     \"y\"     \"group\"\n\n\nR has several useful function for previewing the contents of a dataframe or variable. The head() function shows the first 6 observations of a dataframe or variable, and tail() shows the last 6 observations. You can also show a custom number of observations by using the n= argument in either function. These are illustrated below:\n\nhead(mydat)\n\n  x  y group\n1 1 10     A\n2 2 20     A\n3 3 35     A\n4 4 57     B\n5 5 37     B\n6 6 21     C\n\nhead(mydat, n = 2)\n\n  x  y group\n1 1 10     A\n2 2 20     A\n\n\n\nhead(mydat$group)\n\n[1] A A A B B C\nLevels: A B C\n\n\n\ntail(mydat)\n\n  x  y group\n3 3 35     A\n4 4 57     B\n5 5 37     B\n6 6 21     C\n7 7 23     C\n8 8 25     C\n\ntail(mydat, n = 2)\n\n  x  y group\n7 7 23     C\n8 8 25     C\n\ntail(mydat$group)\n\n[1] A B B C C C\nLevels: A B C\n\n\n\n\n3.10.2 Nicer looking tables\nR can also produce nicely formatted HTML and LaTeX tables. There are several packages that do this, but the knitr package has some basic table creation functions that do a good job for simple tables.\nlibrary(knitr)\n\nkable(mydat,\n      caption = \"My basic table\",\n      align = 'c',  \n      format = \"html\")\nlibrary(knitr)\nkable(mydat,\n      caption = \"My basic table\",\n      align = 'c',  \n      format=\"latex\"  )\n\n\n\nMuch more advanced tables can be created using the gt package Iannone, Cheng, and Schloerke (2020), which allows for highly customized tables.\n\nlibrary(gt, \n        quietly = TRUE)\nlibrary(dplyr,\n        quietly = TRUE)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nmydat%&gt;%\n  gt()%&gt;%\n  tab_header(title= \"My simple gt table\",\n             subtitle = \"With a subtitle\")\n\n\n\n\n\n  \n    \n      My simple gt table\n    \n    \n      With a subtitle\n    \n    \n      x\n      y\n      group\n    \n  \n  \n    1\n10\nA\n    2\n20\nA\n    3\n35\nA\n    4\n57\nB\n    5\n37\nB\n    6\n21\nC\n    7\n23\nC\n    8\n25\nC"
  },
  {
    "objectID": "rintro.html#real-data-example",
    "href": "rintro.html#real-data-example",
    "title": "2  Introduction to R",
    "section": "3.11 Real data example",
    "text": "3.11 Real data example\nNow let’s open a ‘real’ data file. This is the 2018 World population data sheet from the Population Reference Bureau. It contains summary information on many demographic and population level characteristics of nations around the world in 2018.\nI’ve had this entered into a Comma Separated Values file by some poor previous research assistant of mine and it lives happily on Github now for all the world to see. CSV files are a good way to store data coming out of a spreadsheet, because R can read them without any other packages. R can also read Excel files, but it requires external packages to do so, such as readxl.\nI can read it from Github directly by using a function in the readr library, or with the base R function read.csv(), both accomplish the same task.\n\nprb &lt;- read.csv(file = \"https://github.com/coreysparks/r_courses/raw/master/data/2018_WPDS_Data_Table_FINAL.csv\",\n    stringsAsFactors = TRUE)\n\nThat’s handy. If the file lived on our computer in your working directory, I could read it in like so:\n\nprb &lt;- read_csv(\"path/to/file/2018_WPDS_Data_Table_FINAL.csv\")\n\nSame result.\nThe haven library Wickham and Miller (2020) can read files from other statistical packages easily, so if you have data in Stata, SAS or SPSS, you can read it into R using those functions, for example, the read_dta() function reads Stata files, read_sav() to read SPSS data files.\n\nlibrary(haven)\nprb_stata &lt;- read_dta(\"path/to/file/prb2018.dta\")\n\nprb_spss &lt;- read_sav(\"path/to/file/prb_2018.sav\")"
  },
  {
    "objectID": "rintro.html#basic-descriptive-analysis-of-data",
    "href": "rintro.html#basic-descriptive-analysis-of-data",
    "title": "2  Introduction to R",
    "section": "3.12 Basic Descriptive analysis of data",
    "text": "3.12 Basic Descriptive analysis of data\nOne of the key elements of analyzing data is the initial descriptive analysis of it. In subsequent chapters, I will go into more depth about this process, but for now, I want to illustrate some simple but effective commands for summarizing data.\n\n3.12.1 Dataframe summaries\nThe summary() function is very useful both in terms of producing numerical summaries of individual variables, but also for shows summaries of entire dataframes. Its output differs based on the type of variable you give it, for character variables it does not return any summary. For factor variables, it returns a frequency table, and for numeric variables, it returns the five number summary plus the mean.\n\nsummary(prb$region)\n\n       CARIBBEAN  CENTRAL AMERICA     CENTRAL ASIA        EAST ASIA \n              17                8                5                8 \n  EASTERN AFRICA   EASTERN EUROPE    MIDDLE AFRICA  NORTHERN AFRICA \n              20               10                9                7 \nNORTHERN AMERICA  NORTHERN EUROPE          OCEANIA    SOUTH AMERICA \n               2               11               17               13 \n      SOUTH ASIA   SOUTHEAST ASIA  SOUTHERN AFRICA  SOUTHERN EUROPE \n               9               11                5               15 \n  WESTERN AFRICA     WESTERN ASIA   WESTERN EUROPE \n              16               18                9 \n\nsummary(as.factor(prb$continent))\n\n          AFRICA             ASIA           EUROPE NORTHERN AMERICA \n              57               51               45               27 \n         OCEANIA    SOUTH AMERICA \n              17               13 \n\nsummary(prb$tfr)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   1.600   2.300   2.709   3.750   7.200 \n\n\nI find this function to be very useful when I’m initially exploring a data set, so I can easily see the min/max values of a variable. There are many alternatives to this base function, including psych::describe(), Hmisc::describe(), and skimr::skim(), all of which produce summaries of dataframes or variables\n\ndesc1  &lt;-  psych::describe(prb[, 1:8],\n                           fast = FALSE)\nprint(desc1,\n      short = TRUE)\n\n     vars n mean sd median trimmed mad min max range skew kurtosis se\n [ reached 'max' / getOption(\"max.print\") -- omitted 8 rows ]\n\n\n\ndesc2 &lt;-  Hmisc::describe(prb[, 1:8],\n                          tabular= FALSE)\nhead(desc2)\n\ndesc3 &lt;- skimr::skim(prb[, 1:8])\ndesc3\nThe skimr::skim() function is very good at doing summaries of both numeric and categorical data, while the other functions are perhaps best suited to numeric data.\nThe summary() function, as well as the other three functions in other packages can be used on a single variable within a dataframe as well, or on a simple vector:\n\nsummary(prb$tfr)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   1.600   2.300   2.709   3.750   7.200 \n\n\n\nsummary(zf)\n\n1 2 3 4 \n1 1 1 1 \n\n\nFrom this summary, we see that the mean is 2.7085714, there is one country missing the Total fertility rate variable. The minimum is 1 and the maximum is 7.2 children per woman.\n\n\n3.12.2 Frequency tables\nA basic exploration of data, especially if your data have categorical or nominal variables, includes the extensive use of frequency tables. If you’re simply looking at the number of observations in each level of a categorical variable, or using frequency tables to aggregate data, they are some of the most useful basic statistical summaries around. The basic function for constructing simple tables is table() in base R. More sophisticated table construction is allowed in xtabs()\nLet’s have a look at some descriptive information about the data:\n\n#Frequency Table of # of Countries by Continent\ntable(prb$continent)\n\n\n          AFRICA             ASIA           EUROPE NORTHERN AMERICA \n              57               51               45               27 \n         OCEANIA    SOUTH AMERICA \n              17               13 \n\n\nFrequency of TFR over 3 by continent:\n\ntable(I(prb$tfr &gt; 3),\n      prb$continent)\n\n       \n        AFRICA ASIA EUROPE NORTHERN AMERICA OCEANIA SOUTH AMERICA\n  FALSE     11   40     45               27       7            12\n  TRUE      46   11      0                0      10             1\n\n\nTwo things to notice in the above code, first we have to use the $ operator to extract each variable from the prb dataframe. Second, the I() operator is used. This is honestly one of my favorite things in base R. I() is the indicator function, it evaluates to TRUE or FALSE depending on the argument inside of it. This also allows for fast construction of binary variables on the fly in any function. Here’s another example:\n\nx &lt;- c(1, 3, 4, 5, 7, 19)\nI(x &gt; 5)\n\n[1] FALSE FALSE FALSE FALSE  TRUE  TRUE\n\ntable(I(x &gt; 5))\n\n\nFALSE  TRUE \n    4     2 \n\n\nSo we see how this works, I checks if x is greater than 5, if it is, I() returns TRUE. When we feed this to table(), we can count up the TRUE and FALSE responses.\nLater in the book, we will see how to employ the xtabs() function to quickly aggregate data from individual level to aggregate level.\n\n\n3.12.3 More basic statistical summaries\nNow, we will cover some basic descriptive statistical analysis including basic measures of central tendency and variability.\n\n\n3.12.4 Measures of central tendency\nWe can use graphical methods to describe what data ‘look like’ in a visual sense, but graphical methods are rarely useful for comparative purposes. In order to make comparisons, you need to rely on a numerical summary of data vs. a graphical one.\nNumerical measures tell us a lot about the form of a distribution without resorting to graphical methods. The first kind of summary statistics we will see are those related to the measure of central tendency. Measures of central tendency tell us about the central part of the distribution\n\n\n3.12.5 Mean and median\nHere is an example from the PRB data.\n\nmean(prb$tfr)\n\n[1] 2.708571\n\n\nWhoops! What happened? This means that R can’t calculate the mean because there’s a missing value, which we saw before. We can tell R to automatically remove missing values by:\n\nmean(prb$tfr,\n     na.rm = TRUE)\n\n[1] 2.708571\n\n\nWhich works without an error. Many R functions will fail, or do listwise deletion of observations when NAs are present, so it’s best to look at the documentation for the function you’re wanting to use to see what it’s default na action is. The mean() function defaults to na.rm = FALSE, which indicates that it does not remove missing values by default.\nWe can also calculate the median TFR\n\nmedian(prb$tfr,\n       na.rm = TRUE)\n\n[1] 2.3\n\n\n\n\n3.12.6 Measures of variation\nOne typical set of descriptive statistics that is very frequently used is the so-called five number summary and it consists of : the Minimum, lower quartile, median, upper quartile and maximum values. This is often useful if the data are not symmetric or skewed. This is what you get when you use the fivenum() function, or we can include the mean if we use the summary() function.\n\nfivenum(prb$tfr) \n\n[1] 1.0 1.6 2.3 3.8 7.2\n\n\n\nsummary(prb$tfr)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   1.600   2.300   2.709   3.750   7.200 \n\n\n\n3.12.6.1 Variance\nTo calculate the variance and standard deviation of a variable:\n\nvar(prb$tfr, \n   na.rm = TRUE) #variance\n\n[1] 1.806338\n\nsd(prb$tfr,\n   na.rm = TRUE) #standard deviation\n\n[1] 1.344001\n\nsqrt(var(prb$tfr)) #same as using sd()\n\n[1] 1.344001\n\n\nThe above sections have shown some basic ways to summarize data in R, along with many handy functions that are pervasive in my own general work flow. Is this everything R will do, No. Are these the only way to do things in R? Never. I’m constantly marveled at how many new functions I see my students using in their own work and this reminds me how much of the R ecosystem I have yet to explore, even after twenty-plus years of using it."
  },
  {
    "objectID": "rintro.html#the-tidyverse",
    "href": "rintro.html#the-tidyverse",
    "title": "2  Introduction to R",
    "section": "3.13 The tidyverse",
    "text": "3.13 The tidyverse\nSo far, most of the functions I have discussed have been from the base R ecosystem, with some specific functions from other downloadable packages. One of the biggest changes to R in recent years has been the explosion in popularity of the tidyverse Wickham et al. (2019). The tidyverse is a large collection of related packages that share a common philosophy of how data and programming relate to one another and work together to produce a more streamlined, literate way of programming with data.\nTo get the core parts of the tidyverse, install it using install.packages(\"tidyverse\") in your R session. This will install the core components of the tidyverse that can then be used throughout the rest of the book 1.\nTwo of the workhorses in the tidyverse are the packages dplyr Wickham et al. (2020) and ggplot2 Wickham (2016). The dplyr package is very thoroughly described in the book R for Data Science Wickham and Grolemund (2017), and the ggplot2 package also has a book-length description in the book ggplot2: Elegant Graphics for Data Analysis Wickham (2016), so I won’t waste time and space here with complete descriptions. Instead, I will show some pragmatic examples of how these work in my own work flow, and also use these packages together to produce some descriptive data visualizations.\n\n3.13.1 Basic dplyr\nThe dplyr package has many functions that work together to produce succinct, readable and highly functional code. I often say about base R packages in comparison to things like SAS, that I can do something in R in about 10 lines of code compared to 50 in SAS. Using dplyr, you can do even more, faster.\nThe package consists of core “verbs” that are used to clean, reshape, and summarize data. Using “pipes”, the user can chain these verbs together so that you only have to name the data being used once, which makes for more efficient code, since you’re not constantly having to name the dataframe. The pipes also allow for all variables within a dataframe to be accessed, without using the $ or [] notation described earlier in this chapter.\nPerhaps a short tour of using dplyr would be good at this point, and we will see it used throughout the book. In the following code, I will use the prb data from earlier, and I will do a series of tasks. First, I will create a new variable using the mutate() function, then group the data into groups (similar to SAS’s ‘by’ processing) , then do some statistical summaries of other variables using the summarise() function.\nHere we go:\n\nlibrary(dplyr)\n\nprb %&gt;%\n  mutate(high_tfr = ifelse(test = tfr &gt; 3,\n                           yes =  \"high\",\n                           no =  \"low\") )%&gt;%\n  group_by(high_tfr) %&gt;%\n  summarise(mean_e0 = mean(e0male, na.rm = TRUE))\n\n# A tibble: 2 × 2\n  high_tfr mean_e0\n  &lt;chr&gt;      &lt;dbl&gt;\n1 high        62.8\n2 low         73.6\n\n\nThe prb%&gt;% line says, take the prb data and feed it into the next verb using the pipe. The next line mutate(high_tfr = ifelse(test = tfr &gt; 3,yes =  \"high\", no =  \"low\") )%&gt;% tells R to create a new variable called high_tfr, the value of the variable will be created based on conditional logic. If the value of the tfr is over 3, the value will be \"high\" and if the value of the tfr is less than 3, the value of the variable will be \"low\".\nThe group_by(high_tfr)%&gt;% line tells R to form a “grouped data frame”, basically this is how dplyr segments data into discrete groups, based off a variable, and then performs operations on those groups. This is the same thing as stratification of data.\nThe final command summarise(mean_e0 = mean(e0male, na.rm = TRUE)) tells R to take the mean of the e0male variable, in this case it will be calculated for each of the high_tfr groups.\nFinally, we ungroup() the dataframe to remove the grouping, this is customary whenever using the group_by() verb.\nWe can also summarize multiple variables at the same time using the across() command. In the code below, I find the mean (specified by .fns = mean) for each of the four variables e0male, e0female, gnigdp and imr for each of the high_tfr groups.\n\nprb %&gt;%\n  mutate(high_tfr = ifelse(test = tfr &gt; 3,\n                           yes =  \"high\",\n                           no =  \"low\") )%&gt;%\n  group_by(high_tfr) %&gt;%\n  summarise(n = n(),\n            across(.cols = c(e0male, e0female, gnigdp, imr),\n                   .fns = mean,\n                   na.rm = TRUE))%&gt;%\n  ungroup()\n\nWarning: There was 1 warning in `summarise()`.\nℹ In argument: `across(...)`.\nℹ In group 1: `high_tfr = \"high\"`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\n# A tibble: 2 × 6\n  high_tfr     n e0male e0female gnigdp   imr\n  &lt;chr&gt;    &lt;int&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 high        68   62.8     66.4  5329.  43.2\n2 low        142   73.6     78.9 27216.  11.9\n\n\nThe line summarise(n=n() , across(.cols = c(e0male, e0female, gnigdp, imr), .fns = mean, na.rm = TRUE)) tells R to first count the number of cases in each group n = n(), then summarize multiple variables, in this case male and female life expectancy at birth, GDP, and the infant mortality rate, by each of the levels of the high_tfr variable. The summary I want to do is the mean of each variable, being sure to remove missing values before calculating the mean.\nWe see then the estimates of the four other indicators for countries that have TFR over 3, versus countries with a TFR under 3.\nThis is a basic dplyr use, but it is far from what the package can do. Throughout the rest of the book, this process will be used to do calculations, aggregate data, present model results and produce graphics. This example was trying to show a simple workflow in dplyr, and introduce the pipe concept.\nNext, we will explore some basic uses of dplyr in conjunction with the ggplot2 package."
  },
  {
    "objectID": "rintro.html#basic-ggplot",
    "href": "rintro.html#basic-ggplot",
    "title": "2  Introduction to R",
    "section": "3.14 Basic ggplot",
    "text": "3.14 Basic ggplot\nLet’s say that we want to compare the distributions of income from the above examples graphically. Since the ggplot2 library is part of the tidyverse, it integrates directly with dplyr and we can do plots within pipes too.\nIn generally, ggplot() has a few core statements.\n\nggplot() statement - This tells R the data and the basic aesthetic that will be plotted, think x and y axis of a graph. The aesthetic is defined using the aes() function. This is where you pass values to be plotted to the plot device.\nDefine the geometries you want to use to plot your data, there are many types of plots you can do, some are more appropriate for certain types of data\nPlot annotations - Titles, labels etc. This allows you to customize the plot with more information to make it more easily understandable.\n\nNow I will illustrate some basic ggplot examples, and I’m going to use the PRB data that I have been using for other examples. In order to better illustrate the code, I will walk through a very minimal example, line by line.\nlibrary(ggplot2) Loads the ggplot package\nggplot(data = prb, mapping = aes(x = tfr))+ Use the ggplot function, on the prb dataframe. The variable we are plotting is the total fertility rate, tfr. In this case, it is the only variable we are using. I include a + at the end of the line to tell R that more elements of the plot are going to be added.\ngeom_histogram()+ Tells R that the geometry we are using is a histogram, again we have the + at the end of the line to indicate that we will add something else to the plot, in this case a title.\nggtitle(label = \"Distribution of the Total Fertility Rate, 2018\") Tells R the primary title for the plot, which describes what is being plotted. I’m also going to add an additional annotation to the x-axis to indicate that it is showing the distribution of the TFR:\nxlab(label = \"TFR\")\nNow, let’s see all of this together:\n\nlibrary(ggplot2)\n\nggplot(data=prb,\n       mapping=aes(x = tfr))+\n  geom_histogram()+\n  ggtitle(label = \"Distribution of the Total Fertility Rate, 2018\")+\n  xlab(label = \"TFR\")\n\n\n\n\nThe above example named the data frame explicitly in the ggplot() call, but we can also use dplyr to pipe data into the plot:\n\nprb%&gt;%\n  ggplot(mapping=aes(x = tfr))+\n  geom_histogram()+\n  ggtitle(label = \"Distribution of the Total Fertility Rate, 2018\")+\n  xlab(label = \"TFR\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nWe can likewise incorporate a dplyr workflow directly into our plotting, using the example from before, we will create histograms for the high and low fertility groups using the facet_wrap() function.\n\nprb%&gt;%\n  mutate(high_tfr = ifelse(test = tfr &gt; 3,\n                           yes = \"high\",\n                           no = \"low\") )%&gt;%\n  group_by(high_tfr)%&gt;%\n  ggplot(mapping=aes(x = imr))+\n  geom_histogram(aes( fill = high_tfr))+\n  facet_wrap( ~ high_tfr)+\n  ggtitle(label = \"Distribution of the Infant Mortality Rate, 2018\",\n          subtitle = \"Low and High Fertility Countries\")+\n  xlab(label = \"Infant Mortality Rate\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 1 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nYou also notice that I used the aes(fill = high_tfr) to tell R to color the histogram bars according to the variable high_tfr. The aes() function allows you to modify colors, line types, and fills based of values of a variable.\nAnother way to display the distribution of a variable is to use geom_density() which calculates the kernel density of a variable. Again, I use a variable, this time the continent a country is on, to color the lines for the plot.\n\nprb%&gt;%\nggplot(mapping = aes(tfr,\n                     colour = continent,\n                     stat = ..density..))+\n  geom_density()+\n  ggtitle(label = \"Distribution of the Total Fertility Rate by Continent\",\n          subtitle = \"2018 Estimates\")+\n  xlab(label = \"TFR\")\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n3.14.1 Stem and leaf plots/Box and Whisker plots\nAnother visualization method is the stem and leaf plot, or box and whisker plot. This is useful when you have a continuous variable you want to display the distribution of across levels of a categorical variable. This is basically a graphical display of Tukey’s 5 number summary of data.\n\nprb%&gt;%\n  ggplot( mapping = aes(x = continent, y = tfr))+\n  geom_boxplot()+\n  ggtitle(label = \"Distribution of the Total Fertility Rate by Continent\",\n          subtitle = \"2018 Estimates\")\n\n\n\n\nYou can flip the axes, by adding coord_flip()\n\nprb%&gt;%\nggplot( mapping = aes( x = continent,\n                       y = tfr))+\n  geom_boxplot()+\n  ggtitle(label = \"Distribution of the Total Fertility Rate by Continent\",\n          subtitle = \"2018 Estimates\")+\n  coord_flip()\n\n\n\n\nYou can also color the boxes by a variable, Here, I will make a new variable that is the combination of the continent variable with the region variable, using the paste() function. It’s useful for combining values of two strings.\n\nprb%&gt;%\n  mutate(newname = paste(continent, region, sep = \"-\"))%&gt;%\n  ggplot(aes(x = newname,\n             y = tfr,\n             fill = continent))+\n  geom_boxplot()+\n  coord_flip()+\n  ggtitle(label = \"Distribution of the Total Fertility Rate by Continent\",\n          subtitle = \"2018 Estimates\")\n\n\n\n\n\n\n3.14.2 X-Y Scatter plots\nThese are useful for finding relationships among two or more continuous variables. ggplot() can really make these pretty. The geom_point() geometry adds points to the plot.\nHere are a few riffs using the PRB data:\n\nprb%&gt;%\nggplot(mapping= aes(x = tfr,\n                    y = imr))+\n  geom_point()+\n  ggtitle(label = \"Relationship between Total Fertility and Infant Mortality\",\n          subtitle = \"2018 Estimates\")+\n  xlab(label = \"TFR\")+\n  ylab(label = \"IMR\")\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\nR also makes it easy to overlay linear and spline smoothers for the data (more on splines later).\n\nprb%&gt;%\nggplot(mapping = aes(x = tfr,\n                    y = imr))+\n  geom_point()+\n  geom_smooth(method = \"lm\",\n              color = \"black\",\n              se = F)+ #linear regression fit\n  geom_smooth(color = \"blue\",\n              method = \"loess\",\n              se = FALSE)+\n  ggtitle(label = \"Relationship between Total Fertility and Infant Mortality\",\n          subtitle = \"2018 Estimates\")+\n  xlab(label = \"TFR\")+\n  ylab(label = \"IMR\")\n\n\n\n\nNow we color the points by continent\n\nprb%&gt;%\nggplot(mapping = aes(x = tfr, \n                     y = imr,\n                     color =continent))+\n  geom_point()+\n  geom_smooth(method = \"lm\",\n              se = FALSE)+\n  ggtitle(label = \"Relationship between Total Fertility and Infant Mortality\",\n          subtitle = \"2018 Estimates\")+\n  xlab(label = \"TFR\")+\n  ylab(label = \"IMR\")\n\n\n\n\n\n\n3.14.3 Facet plots\nFacet plots are nice, they allow you to create a plot separately based on a grouping variable. This allows you to visualize if the relationship is constant across those groups. Here, I repeat the plot above, but I facet on the continent, and include the regression line for each continent.\n\nprb%&gt;%\nggplot(mapping= aes(x = tfr,\n                    y = imr,\n                    color = continent))+\n  geom_point()+\n  geom_smooth(method = \"lm\",\n              se = FALSE,\n              color = \"black\")+\n  facet_wrap( ~ continent)+\n  ggtitle(label = \"Relationship between Total Fertility and Infant Mortality\",\n          subtitle = \"2018 Estimates\")+\n  xlab(label = \"TFR\")+\n  ylab(label = \"IMR\")\n\n\n\n\nAnother example, this time of a bad linear plot! ggplot makes it easy to examine if a relationship is linear or curvilinear, at least visually.\n\nggplot(data = prb,mapping = aes(x = tfr, y = pctlt15_2018))+\n  geom_point()+\n  geom_smooth( method = \"lm\",\n               se = FALSE,\n               color = \"black\")+\n  geom_smooth( method = \"loess\",\n               se = FALSE,\n               color = \"blue\")+\n  ggtitle(label = \"Relationship between Total Fertility and Percent under age 15\",\n          subtitle = \"2018 Estimates- Linear & Loess fit\")+\n  xlab(label = \"Percent under age 15\")+\n  ylab(label = \"IMR\")\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "rintro.html#chapter-summary",
    "href": "rintro.html#chapter-summary",
    "title": "2  Introduction to R",
    "section": "3.15 Chapter summary",
    "text": "3.15 Chapter summary\nIn this chapter, I have introduced R and Rstudio and some basic uses of the software for accessing data and estimating some summary statistics. The R ecosystem is large and complex, and the goal of this book is to show you, the user, how to use R for analyzing data from demographic data sources. In the chapters that follow, I will show how to use R within two large universes of data, the macro and the micro. The macro level sections will focus on using R on data that come primarily from places - nations, regions, administrative areas. The micro level sections will focus on analyzing complex survey data on individual responses to demographic surveys. The final section will discuss approaches that merge these two levels into a multi-level framework and describe how such models are estimated and applied."
  },
  {
    "objectID": "rintro.html#references",
    "href": "rintro.html#references",
    "title": "2  Introduction to R",
    "section": "3.16 References",
    "text": "3.16 References\n\n\n\n\nIannone, Richard, Joe Cheng, and Barret Schloerke. 2020. Gt: Easily Create Presentation-Ready Display Tables. https://CRAN.R-project.org/package=gt.\n\n\nKleinman, Ken, and Nicholas J. Horton. 2014. SAS and r: Data Management, Statistical Analysis, and Graphics, 2nd Edition. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. https://nhorton.people.amherst.edu/sasr2/.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\n———. n.d. R Packages. 1st ed. O’Reily. https://r-pkgs.org/.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2020. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 1st ed. Paperback; O’Reilly Media. http://r4ds.had.co.nz/.\n\n\nWickham, Hadley, and Evan Miller. 2020. Haven: Import and Export ’SPSS’, ’Stata’ and ’SAS’ Files. https://CRAN.R-project.org/package=haven."
  },
  {
    "objectID": "rintro.html#footnotes",
    "href": "rintro.html#footnotes",
    "title": "2  Introduction to R",
    "section": "",
    "text": "If you followed the script at the beginning of this chapter, the tidyverse will already be installed.↩︎"
  },
  {
    "objectID": "survey.html",
    "href": "survey.html",
    "title": "3  Analysis of Survey Data",
    "section": "",
    "text": "4 Survey Data"
  },
  {
    "objectID": "survey.html#demographic-survey-data",
    "href": "survey.html#demographic-survey-data",
    "title": "3  Analysis of Survey Data",
    "section": "4.1 Demographic Survey data",
    "text": "4.1 Demographic Survey data\nThe majority of demographic research relies on two or three main sources of information. First among these are population enumerations or censuses, followed by vital registration data on births and deaths and last but not least, data from surveys. Censuses and other population enumerations are typically undertaken by federal statistical agencies and demographers use this data once it’s disseminated from these agencies. Similarly, vital registration data are usually collected by governmental agencies, who oversee the collection and data quality for the data. Survey data on the other hand can come from a wide variety of sources.\nIt’s not uncommon for us to go and collect our own survey data specific to a research project we have, typically on a specialized population that we are interested in learning about, but surveys can also be quite general in their scope and collect information on a wide variety of subjects. Owing to the mix of small and large-scale survey data collection efforts, survey data are often available on many different topics, locales and time periods. Of course we as demographers are typically interested in population-level analysis or generalization from our work, so the survey data we try to use are collected in rigorous manners, with much attention and forethought paid to ensure the data we collect can actually be representative of the target population we are trying to describe.\nIn this chapter, I will introduce the nature of survey sampling as is often used in demographic data sources, and describe what to look for when first using a survey data source for you research. These topics are geared towards researchers and students who have not worked with survey data much in the past and will go over some very pragmatic things to keep in mind. Following this discussion, I will use a specific example from the US Census Bureau’s American Community Survey and illustrate how to apply these principals to this specific source. The final goal of this chapter is to show how to use R to analyze survey data and produce useful summaries from our surveys, both tabular and graphically.\nMy goal in this chapter is to introduce you to common statistical sampling terms and principles, and show how to analyze complex survey design data using R. I hope that the general workflow I use in this chapter allows you to identify the key elements of your survey data source, use R to begin analyzing your data."
  },
  {
    "objectID": "survey.html#basics-of-survey-sampling",
    "href": "survey.html#basics-of-survey-sampling",
    "title": "3  Analysis of Survey Data",
    "section": "4.2 Basics of survey sampling",
    "text": "4.2 Basics of survey sampling\nTo begin this section, I want to go over some of the simple terms from sampling that are very important to those of us who rely on survey data for our work. For many of the concepts from this chapter, I strongly recommend Lohr (2019) for the theoretical portions and Lumley (2010) for discussion of how R is used for complex survey data.\nThe target population is the population that our survey has been designed to measure. For large national surveys, these are typically the population of the country of interest. For example, the Demographic and Health Survey (DHS) has it’s primary target population as women of childbearing ages in women of reproductive age and their young children living in households. Our observational units are the level at which we are collecting data, for surveys this is typically a person or a household, and our survey documentation will tell us what its unit of observation is. Sampling Units refer to the units that can serve for us to collect data from, for example we may not have a list of every school age child, but we may have a list of schools, so we may use schools as our sampling units and sample children within them. The sampling frame is the set of sampling units containing distinct sets of population members, this is usually the most recent population census, ideally the entire population, or following our school example from above, the entire listing of schools.\nThese terms are ubiquitous in sampling, but other terminology also exists in many surveys and these terms relate to the nature of how the survey was actually carried out. Many times the surveys we end up using are not themselves simple random samples, but are instead some blend of stratified or cluster sample. Simple random samples are the basis for much of statistical insight, and most methods in statistics assume your data are drawn from a population at random. Certainly, methods for collecting random samples exist, such as random digit dialing for phone samples, but often times these samples themselves are not truly random, they are stratified by a geographic area or by type of phone (mobile vs land line). For example, the DHS uses a stratified, cluster sample to collect its information. Strata refer to relatively homogeneous areas within the place we are trying to collect data. In the DHS, these are typically rural or urban areas of a country, as identified by the census. Within each strata, the DHS will choose clusters from which to sample from, this is a two-stage sampling method, where first the sampling frame is stratified, then clusters are selected. Clusters in the DHS are usually neighborhoods in urban areas and smaller towns or villages in rural areas.\nFigure 1 shows a cartoon of how this process works, with multiple potential cluster that can be sampled (boxes), and within the cluster are our observational units, some of which are sampled, and some of which are unsampled.\n\n\n\nFigure 1"
  },
  {
    "objectID": "survey.html#simple-versus-complex-survey-designs",
    "href": "survey.html#simple-versus-complex-survey-designs",
    "title": "3  Analysis of Survey Data",
    "section": "4.3 Simple versus complex survey designs",
    "text": "4.3 Simple versus complex survey designs\nHow the data we’re using is sampled has a major implication for how we analyze it. The majority of statistical tools assume that data come from simple random samples, because most methods assume independence of observations, regardless of which distribution or test statistic you are using. Violations of this assumption are a big problem when we go to analyze our data, because the non-independence of survey data are automatically in violation of a key assumption of any test. The stratified and clustered nature of many survey samples may also present problems for methods such as linear regression analysis which assume errors in the model are homoskedastic, or constant. When data are collected in a stratified or clustered method, the data may have less variation than a simple random sample, because individuals who live closely to one another often share other characteristics in common as well. Our statistical models don’t do well with this type of reduction in variation and we often have to resort to manipulations of our model parameters or standard errors of our statistics in order to make them coincide with how the data were collected.\nNot to fear! Data collected using public funds are typically required to be made available to the public with information on how to use them. Most surveys come with some kind of code book or user manual which describes how the data were collected and how you should go about using them. In these cases, it pays to read the manual because it will tell you the names of the stratification and clustering variables in the survey data. This will allow you to use the design of the survey in your analysis so that your statistical routines are corrected for the non-randomness and homogeneity in the survey data.\nHe’s not heavy, he’s my brother\nAnother important aspect of survey data are the use of weighting variables. Whenever we design a survey, we have our target population, or universe of respondents in mind. In the DHS, again, this is traditionally1 women of childbearing age and their children (International 2012). When we collect a sample from this population, or sample may be, and typically is, imperfect. It is imperfect for many reasons, owing to the difficulty of sampling some members of the population, or their unwillingness to participate in our study. Part of designing an effective survey is knowing your universe or population, and its characteristics. This will let you know the probability of a particular person being in the sample. Of course, the more complicated the survey, the more complicated it is to know what this probability is. For example, if we were to sample people in the United States, using a stratified design based on rural and urban residence, we would need to know how many people lived in rural and urban areas within the country, as this would effect the probability of sampling a person in each type of area. This inclusion probability tells us how likely a given person is of being sampled. The inverse of the inclusion probability is called the sampling weight:\n\\(w_i = \\frac{1} {\\pi_i}\\)\nwhere \\(\\pi_i\\) is the inclusion probability.\nSampling weights are what we use to make our analyses of a survey representative of the larger population. They serve many purposes including unequal inclusion probabilities, differences in sample characteristics compared to the larger population, and differences in response rates across sample subgroups. All of these situations make the sample deviate from the population by affecting who the actual respondents included in the survey are. Differences in our sample when compared to the larger population can affect most all of our statistical analysis since again, most methods assume random sampling. The weights that are included in public data are the result of a rigorous process conducted by those who designed and implemented the survey itself, and most surveys in their user manuals or code books describe the process of how the weights are created. For example, the US Center for Disease Control and Prevention’s Behavioral Risk Factor Surveillance System (BRFSS) provides a very thorough description of how their final person weights are calculated (CDC 2020). These weights include three primary factors, the stratum weight, which is a combination of the number of records in a sample strata and the density of phone lines in a given strata, combined with the number of phones in a sampled household and the number of adults in the household to produce the final design weight. These weights are then raked to eight different marginal totals, based on age, race/ethnicity, education, marital status, home ownership, gender by race/ethnicity, age by race/ethnicity and phone ownership(CDC 2020). After this process, weights are interpretable as the number of people a given respondent in the survey represents in the population. So, if a respondent’s weight in the survey data is 100, they actually represent 100 people in the target population.\nOther types of weights also exist, and are commonly seen in federal data sources. A common kind of weight that includes information on both the probability of inclusion AND the stratified design of the survey are replicate weights. Replicate weights are multiple weights for each respondent, and there are as many weights as there are different levels of the stratification variable. Later in this chapter, we will discuss how replicate weights are used, as compared to single design weights in an example."
  },
  {
    "objectID": "survey.html#characteristics-of-your-survey",
    "href": "survey.html#characteristics-of-your-survey",
    "title": "3  Analysis of Survey Data",
    "section": "4.4 Characteristics of YOUR survey",
    "text": "4.4 Characteristics of YOUR survey\nSurvey data that come from reputable sources, such as most federal agencies or repositories such as the Inter-university Consortium for Political and Social Research (ICPSR) at the University of Michigan in the United States, are accompanied by descriptions of the data source including when and where it was collected, what it’s target population is, and information on the design of the survey. This will include information on sample design, such as stratum or cluster variables, and design or replicate weights to be used when you conduct your analysis. I cannot stress enough that learning how your particular survey data source is designed, and how the designers recommend you use provided survey variables for your analysis, is imperative to ensure your analysis is correctly specified."
  },
  {
    "objectID": "survey.html#example-from-the-american-community-survey",
    "href": "survey.html#example-from-the-american-community-survey",
    "title": "3  Analysis of Survey Data",
    "section": "4.5 Example from the American Community Survey",
    "text": "4.5 Example from the American Community Survey\nLet’s look at an example of these ideas in a real data source. Throughout the book I will use several complex survey design data sources to illustrate various topics, in this chapter I will use data from the US Census Bureau’s American Community Survey (ACS) public use microdata sample (PUMS). We can actually use the tidycensus package (Walker and Herman 2021) to download ACS PUMS directly from the Census Bureau.\nThis example shows how to extract the 2018 single-year PUMS for the state of Texas, and only keep variables related to person-records. The ACS has information on both people and households, but for now we’ll only look at the person records. Help on these functions can be found by typing ?pums_variables and ?get_pums in R\n\nlibrary(tidycensus)\nlibrary(tidyverse)\n\npums_vars_18&lt;- pums_variables %&gt;%\n  filter(year== 2018, survey == \"acs1\") %&gt;%\n  distinct(var_code, var_label, data_type, level) %&gt;%\n  filter(level == \"person\")\n\nTX_pums &lt;- get_pums(\n  variables = c(\"PUMA\", \"SEX\", \"AGEP\", \"CIT\", \"JWTR\",\"JWRIP\", \"HISP\"),\n  state = \"AL\",\n  survey = \"acs1\",\n  year = 2018)\n\nknitr::kable(\n  head(TX_pums),\n  format = 'html'\n  )\nThese data are also easily available from the Integrated Public Use Microdata Series (IPUMS) project housed at the University of Minnesota (Ruggles et al. 2021). The IPUMS version of the data adds additional information to the data and homogenizes the data across multiple years to make using it easier. The following example will use the ipumsr package to read in an extract from IPUMS-USA.\nAfter you create an IPUMS extract, right click on the DDI link and save that file to your computer. Then repeat this for the .DAT file. If you need help creating an IPUMS extract, their staff have created a tutorial for doing so (https://usa.ipums.org/usa/tutorials.shtml).\nThis will save the xml file that contains all the information on the data (what is contained in the data file) to your computer. When using IPUMS, it will have a name like usa_xxxxx.xml where the x’s represent the extract number.\nYou will also need to download the data file, by right clicking the Download.DAT link in the above image. This will save a .gz file to your computer, again with a name like: usa_xxxxx.dat.gz. Make sure this file and the xml file from above are in the same folder.\n\nThe fundamentals of using ipumsr is to specify the name of your .xml file from your extract, and as long as your .tar.gz file from your extract is in the same location, R will read the data. The files on my computer:\n\n\nlibrary(ipumsr)\n\nddi &lt;- read_ipums_ddi(ddi_file = \"data/usa_00097.xml\")\nipums &lt;- read_ipums_micro(ddi = ddi)\n\nWill read in the data, in this case, it is a subset of the 2008 to 2012 single year ACS. This extract is not all of the variables from the ACS, as that would be a very large file and for my purposes here, I don’t need that. My goal for the rest of the chapter is to illustrate how to use the IPUMS as an example of a complex survey design data set and the steps necessary to do so."
  },
  {
    "objectID": "survey.html#basics-of-analyzing-survey-data",
    "href": "survey.html#basics-of-analyzing-survey-data",
    "title": "3  Analysis of Survey Data",
    "section": "4.6 Basics of analyzing survey data",
    "text": "4.6 Basics of analyzing survey data\nA fundamental part of analyzing complex survey data are knowing the variables within the data that contain the survey design information. The US Census Bureau has documented the design of the survey in a publication (US Census Bureau 2014) The IPUMS version of the ACS has two variables STRATA and CLUSTER that describe the two stage process by which the data are collected. Here are a the first few lines of these from the data:\n\noptions(scipen = 999)\nlibrary(knitr)\n  \nkable(head(ipums[, c(\"SERIAL\", \"STRATA\", \"CLUSTER\")],\n           n=10),\n      digits = 14 )\n\nFor the ACS, the strata variable is named, ironically STRATA and the cluster variable CLUSTER. The IPUMS creates the STRATA variable based on the sampling strata in the ACS, and the CLUSTER variable based on households within a stratum. Often in surveys, the clusters may not be households, they could be smaller population aggregates, such as neighborhoods and villages, as in the DHS.\nThe data also come with housing unit weights and person unit weights, so your analysis can be either representative of housing units or people.\n\nkable(head(ipums[, c(\"SERIAL\", \"STRATA\", \"CLUSTER\", \"HHWT\", \"PERWT\")],\n           n=10),\n      digits = 14 )\n\n\n\n\nSERIAL\nSTRATA\nCLUSTER\nHHWT\nPERWT\n\n\n\n\n1189369\n330148\n2019011893691\n67\n67\n\n\n1189370\n231948\n2019011893701\n43\n43\n\n\n1189371\n690048\n2019011893711\n114\n114\n\n\n1189372\n430248\n2019011893721\n34\n34\n\n\n1189373\n650048\n2019011893731\n35\n35\n\n\n1189374\n680548\n2019011893741\n19\n19\n\n\n1189375\n340048\n2019011893751\n18\n18\n\n\n1189376\n60048\n2019011893761\n37\n37\n\n\n1189377\n440048\n2019011893771\n76\n76\n\n\n1189378\n462348\n2019011893781\n10\n10\n\n\n\n\n\nAs can be seen in the first few cases, the HHWT variable is the same for everyone in a given household, but each person has a unique person weight showing that they each represent different numbers of people in the population. Further investigation of the housing and person weights allow us to see what these values actually look like.\n\nsummary(ipums$PERWT)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    1.0    49.0    78.0   106.3   128.0  2376.0 \n\n\nHere we see the minimum person weight is 1 and the maximum is 2376, which tells us that at least on person in the data represents 2376 people in the population that year. A histogram of the weights can also show us the distribution of weights in the sample.\n\nlibrary(ggplot2)\n\nipums%&gt;%\n  ggplot(aes(x = PERWT)) +\n    geom_histogram() + \n  labs(title = \"Histogram of ACS Person Weights, 2019\")\n\n\n\n\nWe can see how the weights inflate each person or household to the population by summing the weights. Below, I sum the person weights for the state of Texas, the sum is 28,995,881 million people, which is the same as the official estimate of the population in 2019 (https://www.census.gov/quickfacts/TX), we also see, by using the n() function, that there were 272,776 persons in the sample in 2019 living in Texas.\n\nlibrary(dplyr)\nipums%&gt;%\n  filter(STATEFIP == 48)%&gt;%\n  summarize(tot_pop = sum( PERWT ) , n_respond = n())\n\n# A tibble: 1 × 2\n   tot_pop n_respond\n     &lt;dbl&gt;     &lt;int&gt;\n1 28995881    272776\n\n\nFor housing units, we have to select a single person from the household in order for the same process to work, otherwise we would misrepresent the number of households in the state. We see there are 10,585,803 million housing units, and 114,016 unique households in the data.\n\nipums%&gt;%\n  filter(STATEFIP == 48,\n         PERNUM == 1)%&gt;%\n  summarize(tothh = sum( HHWT ) , n_housing = n())\n\n# A tibble: 1 × 2\n     tothh n_housing\n     &lt;dbl&gt;     &lt;int&gt;\n1 10585803    114016\n\n\nThis total is nearly identical to that from the Census’s ACS estimate.\nThis exercise shows that by using the provided weights in the survey, we can estimate the population size the sample was supposed to capture effectively. The survey package and the newer tidyverse package srvyr are designed to fully implement survey design and weighting and perform a wide variety of statistical summaries.\nThe way these packages work, is that you provide the name of your data frame, and the survey design variables that are in your data and the package code performs the requested analysis, correcting for survey design and weighting to the appropriate population. The code below illustrates how to enter the survey design for the IPUMS-USA ACS. Some surveys will not have both a cluster and stratification variable, so again, it’s important to consult your survey documentation to find these for your data.\nThe function as_survey_design() in this case takes three arguments, since we are piping the 2019 ACS into it, we don’t have to specify the name of the data. ids is the argument for the cluster variable, if your survey doesn’t have one, just leave it out. strata is where you specify the name of the survey stratification variable, and weights is where you specify the name of the appropriate weighting variable. In this case, I’m replicating the estimate of the housing units in Texas from above, so I’ll use the HHWT variable. The easiest way to get a total population estimate is to use the survey_total() function, which is equivalent to summing the weights as shown above, although in the case of the survey analysis commands in the survey and srvyr packages, the total will also be estimated with a standard error of the estimate.\n\nlibrary(srvyr)\nlibrary(survey)\n\nipums%&gt;%\n  filter(STATEFIP == 48, PERNUM == 1)%&gt;%\n  as_survey_design(cluster= CLUSTER,\n                   strata = STRATA,\n                   weights = HHWT)%&gt;%\n  summarize(tothh = survey_total())\n\n\n\nA short aside about survey design options The core definition of the ACS survey design is shown in the code above, and I highly recommend that you inspect the help file for the survey design functions ?as_survey_design or ?svydesign. An important option that often has to be specified is the nest=TRUE option. This if often necessary if PSU identifiers are not unique across strata. For example, the fictional data shown below has the PSU’s values the same across strata.}\n\nfake_survey&lt;- data.frame(\n  strata = c(1,1,1,1,1,1,\n             2,2,2,2,2,2),\n  psu = c(1,1,1,2,2,3,\n          1,1,2,2,3,3),\n  weight = rpois(n = 12, lambda = 20))\n\nknitr::kable(fake_survey)\n\n\n\n\nstrata\npsu\nweight\n\n\n\n\n1\n1\n9\n\n\n1\n1\n17\n\n\n1\n1\n24\n\n\n1\n2\n19\n\n\n1\n2\n19\n\n\n1\n3\n20\n\n\n2\n1\n14\n\n\n2\n1\n23\n\n\n2\n2\n20\n\n\n2\n2\n22\n\n\n2\n3\n28\n\n\n2\n3\n16\n\n\n\n\n\nIf we attempt to make a survey design from this, R would show an error.\n\nfake_design&lt;- fake_survey%&gt;%\n  as_survey_design(ids = psu,\n                    strata=strata,\n                    weights = weight)\n\nError in svydesign.default(ids, probs, strata, variables, fpc, .data, : Clusters not nested in strata at top level; you may want nest=TRUE.\n\n\nBut if we include the nest = TRUE option, R doesn’t give us the error:\n\nfake_design&lt;- fake_survey%&gt;%\n  as_survey_design(ids = psu,\n                  strata=strata,\n                  weights = weight, \n                  nest = TRUE)\nfake_design\n\nStratified 1 - level Cluster Sampling design (with replacement)\nWith (6) clusters.\nCalled via srvyr\nSampling variables:\n - ids: psu\n - strata: strata\n - weights: weight\nData variables: strata (dbl), psu (dbl), weight (int)\n\n\nThe ACS from IPUMS has unique CLUSTERs across strata, so we don’t have to specify that argument when we declare our survey design.\n\nBack to our housing estimates.\nIn this case, our tothh estimate is identical to summing the weights, but new we also have an estimate of the precision of the estimate, so we could produce a more informed statistical estimate that in 2019, there were 10,585,803 \\(\\pm\\) 27,274.96 occupied housing units in the state.\nIf your data come with replicate weights instead of strata and cluster variables, this can be specified using the as_survey_rep() command instead as_survey_design(). In this case, we have to specify all of the columns which correspond to the replicate weights in the data. There are likely many ways to do this, but below, I use a method that matches the column names using a regular expression, where we are looking for the string REPWT, followed by any number of numeric digits, that is what the [0-9]+ portion tells R to do. Also, the ACS uses a balanced replicate weight construction, which also requires the case weight as well (Ruggles et al. 2021), so we specify the replicate weight type as BRR. Again, this is specific to the ACS, and you need to consult your own code book for your survey for your design information.\nIn this case, we get the same estimate for the total number of housing units, but a smaller variance in the estimate, which is often seen when using replicate weights.\n\nipums%&gt;%\n  filter(STATEFIP == 48, PERNUM == 1)%&gt;%\n  as_survey_rep(weight = HHWT,\n            repweights =matches(\"REPWT[0-9]+\"),\n            type = \"JK1\",\n            scale = 4/80,\n            rscales = rep(1, 80),\n            mse = TRUE)%&gt;%\n  summarize(tothh = survey_total())\n\n# A tibble: 1 × 2\n     tothh tothh_se\n     &lt;dbl&gt;    &lt;dbl&gt;\n1 10585803   15479.\n\n\n\nrw&lt;-ipums%&gt;%\n      filter(STATEFIP==48, PERNUM==1)%&gt;%\n      select(REPWT1:REPWT50)\n\nt1&lt;-survey::svrepdesign(data=ipums[ipums$STATEFIP==48&ipums$PERNUM==1,],\n                        repweights = rw,\n                        weights = ipums$HHWT[ipums$STATEFIP==48&ipums$PERNUM==1],\n                        type=\"JK1\",scale = .05, rscales = rep(1, ncol(rw)), mse= TRUE)\n\nt1$variables$ones&lt;-1\nlibrary(survey)\n\nsvytotal(~ones, design=t1)\n\n        total    SE\nones 10585803 11850\n\n\nWe can also define the survey design outside of a dplyr pipe if we want using the survey package.\n\nacs_design &lt;- svydesign(ids = ~ CLUSTER, \n                        strata= ~ STRATA, \n                        weights = ~ PERWT, \n                        data=ipums)\n\nacs_design\n\nStratified 1 - level Cluster Sampling design (with replacement)\nWith (114016) clusters.\nsvydesign(ids = ~CLUSTER, strata = ~STRATA, weights = ~PERWT, \n    data = ipums)\n\n\nOf course we typically want to do more analysis than just estimate a population size, and typically we are interested in using survey data for comparisons and regression modeling. To carry out any sort of statistical testing on survey data, we must not only weight the data appropriately but we must also calculate all measures of variability correctly as well. Since surveys are stratified, the traditional formula for variances is not correct because under stratified sampling, all estimates are not only a function of the total sample, but also the within-strata sample averages and sample sizes. We can estimate the variances in our estimates using the design variables and sample weights in the survey analysis procedures, but there are options."
  },
  {
    "objectID": "survey.html#replicates-and-jack-knifes-and-expansions-oh-my",
    "href": "survey.html#replicates-and-jack-knifes-and-expansions-oh-my",
    "title": "3  Analysis of Survey Data",
    "section": "4.7 Replicates and jack knifes and expansions, oh my!",
    "text": "4.7 Replicates and jack knifes and expansions, oh my!\nWhen conducting your analysis, you may not have any choices of whether you should use replicate weights or design weights, because your survey may only have one of these. There are two main strategies to estimate variances in survey data, the Taylor Series Approximation also referred to as linearization and the use of replicate weights. The Taylor Series, or linearization method is an approximation to the true variance, but is likely the most commonly used technique when analyzing survey data using regression methods. Lohr (2019) describes the calculation of variances from simple and clustered random samples in her book, and by her admission, once one has a clustered random sample the variance calculations for simple calculations becomes much more complex.\nThe problem is that we often want much more complicated calculations in our work and the variance formulas for anything other than simple ratios are not analytically known. The Taylor series approximation to the variance for complex and nonlinear terms such as ratios or estimates of regression parameters. The survey package in R will do this if you specify a survey design that includes strata or clusters, while if you specify replicate weights then it will use an appropriate technique depending on how the data were collected.\nTypical replicate methods include balanced replicates, where there are exactly two clusters within each stratum, jackknife methods, which effectively remove one cluster from the strata and perform all calculations without that cluster in the analysis, then average across all replicates, and bootstrap methods which randomly sample clusters within strata with replacement a large number of times to get an estimate of the quantities of interest."
  },
  {
    "objectID": "survey.html#descriptive-analysis-of-survey-data",
    "href": "survey.html#descriptive-analysis-of-survey-data",
    "title": "3  Analysis of Survey Data",
    "section": "4.8 Descriptive analysis of survey data",
    "text": "4.8 Descriptive analysis of survey data\nThe survey library allows many forms of descriptive and regression analysis."
  },
  {
    "objectID": "survey.html#weighted-frequencies-and-rates",
    "href": "survey.html#weighted-frequencies-and-rates",
    "title": "3  Analysis of Survey Data",
    "section": "4.9 Weighted frequencies and rates",
    "text": "4.9 Weighted frequencies and rates\nBasic frequency tables are very useful tools for examining bivariate associations in survey data. In the survey analysis packages in R, the basic tools for doing this are the svytable() function in survey, or via the survey_total() function in srvyr. First I will recode two variables in the ACS, the employment status to indicate if a respondent is currently employed, and the MET2013 variable, which is the metropolitan area where the respondent was living. This will give us the ACS estimate for the employed and unemployed population in each Texas MSA. I first have to filter the data to be people of working age, who are in the labor force and living in a MSA.\n\nipums %&gt;%\n  filter(EMPSTAT %in% 1:2,\n         AGE &gt;= 16 & AGE &lt;= 65, \n         MET2013 != 0) %&gt;%\n  mutate(employed = as.factor(case_when(.$EMPSTAT == 1 ~ \"Employed\",\n                              .$EMPSTAT == 2 ~ \"Unemployed\" )),\n         met_name = haven::as_factor(MET2013)) %&gt;%\n  as_survey_design(cluster = CLUSTER,\n                   strata = STRATA,\n                   weights = PERWT) %&gt;%\n  group_by(met_name, employed)%&gt;%\n  summarize(emp_rate = survey_total()) %&gt;%\n  head()\n\n# A tibble: 6 × 4\n# Groups:   met_name [3]\n  met_name                 employed   emp_rate emp_rate_se\n  &lt;fct&gt;                    &lt;fct&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n1 Amarillo, TX             Employed     114201       2824.\n2 Amarillo, TX             Unemployed     3761        843.\n3 Austin-Round Rock, TX    Employed    1193654      10735.\n4 Austin-Round Rock, TX    Unemployed    47998       3247.\n5 Beaumont-Port Arthur, TX Employed     163936       3513.\n6 Beaumont-Port Arthur, TX Unemployed     5851        774.\n\n\nThis is OK, but if we want the totals in columns versus rows, we need to reshape the data. To go from the current “long” form of the variables to a wide form, we can use pivot_wider in dplyr.\n\nwide&lt;-ipums %&gt;%\n  filter(EMPSTAT %in% 1:2,\n         AGE &gt;= 16 & AGE &lt;= 65, \n         MET2013 != 0) %&gt;%\n  mutate(employed = as.factor(case_when(.$EMPSTAT == 1 ~ \"Employed\",\n                              .$EMPSTAT == 2 ~ \"Unemployed\" )),\n         met_name = haven::as_factor(MET2013)) %&gt;%\n  as_survey_design(cluster = CLUSTER,\n                   strata = STRATA,\n                   weights = PERWT) %&gt;%\n  group_by(met_name, employed)%&gt;%\n  summarize(emp_rate = survey_total()) %&gt;%  \n  pivot_wider(id_cols = met_name,\n              names_from = employed,\n              values_from = c(emp_rate, emp_rate_se) ) %&gt;%\n  head()\n  \nwide\n\n# A tibble: 6 × 5\n# Groups:   met_name [6]\n  met_name            emp_rate_Employed emp_rate_Unemployed emp_rate_se_Employed\n  &lt;fct&gt;                           &lt;dbl&gt;               &lt;dbl&gt;                &lt;dbl&gt;\n1 Amarillo, TX                   114201                3761                2824.\n2 Austin-Round Rock,…           1193654               47998               10735.\n3 Beaumont-Port Arth…            163936                5851                3513.\n4 Brownsville-Harlin…            161922                8153                3500.\n5 College Station-Br…            111576                3517                3132.\n6 Corpus Christi, TX             208801               12365                4222.\n# ℹ 1 more variable: emp_rate_se_Unemployed &lt;dbl&gt;\n\n\n\n\nA note about pivoting data The previous example used the pivot_wider() function to take observations that were in rows and pivot them into columns. This process, along with the pivot_longer() function are fundamental to working with many types of demographic data where we either need to format the data so observations are distributed over columns (as in the above example), or where we have multiple columns and we need to put those into multiple rows. The pivot_wider() function has lots of arguments that can be used, but the key ones are id_cols which identifies the unique identifier for each observation, here being the city or met_name. After that is identified, the names_from option tells R what the labels are in the current rows that will become separate column names. Here these are Employed and Unemployed. From these new columns, we can have multiple values provided to values_from, provided as a vector of current column names. Here these are emp_rate and the standard error emp_rate_se.\nTo return the data to their original form, with repeated observations in the rows, we can use the pivot_longer() function. In this case I will just provide the columns in the wide data that I want to be in rows in the long data:\n\nwide%&gt;%\n  pivot_longer(cols = starts_with(\"emp_rate_\"), \n               names_to = c(\".value\", \"rate\"),\n               names_sep = \"\\\\._\")%&gt;%\n  select(-rate)\n\n# A tibble: 6 × 5\n# Groups:   met_name [6]\n  met_name            emp_rate_Employed emp_rate_Unemployed emp_rate_se_Employed\n  &lt;fct&gt;                           &lt;dbl&gt;               &lt;dbl&gt;                &lt;dbl&gt;\n1 Amarillo, TX                   114201                3761                2824.\n2 Austin-Round Rock,…           1193654               47998               10735.\n3 Beaumont-Port Arth…            163936                5851                3513.\n4 Brownsville-Harlin…            161922                8153                3500.\n5 College Station-Br…            111576                3517                3132.\n6 Corpus Christi, TX             208801               12365                4222.\n# ℹ 1 more variable: emp_rate_se_Unemployed &lt;dbl&gt;\n\n\nWhich gets me back to a very similar data frame that I started with above. We will also see in the chapter on micro demography examples of using these functions when assemlbing longitudinal data sets.\n\nOf course, if we want rates, this would imply us having to divide these columns to calculate the rate, but we can also get R to do this for us using survey_mean(). Since the employed variable is dichotomous, if we take the mean of its various levels, we get a proportion, in this case the employment and unemployment rates, respectively.\n\nipums %&gt;%\n  filter(EMPSTAT %in% 1:2,\n         AGE &gt;= 16 & AGE &lt;= 65, \n         MET2013 != 0) %&gt;%\n  mutate(employed = as.factor(case_when(.$EMPSTAT == 1 ~ \"Employed\",\n                              .$EMPSTAT == 2 ~ \"Unemployed\" )),\n         met_name = haven::as_factor(MET2013)) %&gt;%\n  as_survey_design(cluster = CLUSTER,\n                   strata = STRATA,\n                   weights = PERWT) %&gt;%\n  group_by(met_name, employed)%&gt;%\n  summarize(emp_rate = survey_mean()) %&gt;%  \n  pivot_wider(id_cols = met_name,\n              names_from = employed,\n              values_from = c(emp_rate, emp_rate_se) ) %&gt;%\n  head()\n\n# A tibble: 6 × 5\n# Groups:   met_name [6]\n  met_name            emp_rate_Employed emp_rate_Unemployed emp_rate_se_Employed\n  &lt;fct&gt;                           &lt;dbl&gt;               &lt;dbl&gt;                &lt;dbl&gt;\n1 Amarillo, TX                    0.968              0.0319              0.00706\n2 Austin-Round Rock,…             0.961              0.0387              0.00259\n3 Beaumont-Port Arth…             0.966              0.0345              0.00461\n4 Brownsville-Harlin…             0.952              0.0479              0.00735\n5 College Station-Br…             0.969              0.0306              0.00599\n6 Corpus Christi, TX              0.944              0.0559              0.00684\n# ℹ 1 more variable: emp_rate_se_Unemployed &lt;dbl&gt;\n\n\nWhich gets us the employment rate and the unemployment rate for each metropolitan area, with their associated standard errors. This is a general process that would work well for any grouping variable. If we create an object from this calculation, in this case I’ll call it tx_rates, then we can also easily feed it into ggplot to visualize the rates with their associated 95% confidence intervals. The geom_errorbar addition to a ggplot object can add errors to estimates, which are great because we convey the uncertainty in the rates.\n\ntx_rates%&gt;%\n  ggplot()+\n  geom_bar(aes(x=met_name, y = emp_rate_Unemployed), stat = \"identity\")+\n  geom_errorbar(aes(x=met_name,\n                    ymin=emp_rate_Unemployed-1.96*emp_rate_se_Unemployed,\n                    ymax= emp_rate_Unemployed+1.96*emp_rate_se_Unemployed),\n                width=.25)+\n  scale_y_continuous(labels = scales::percent)+\n  labs(x = \"MSA\", \n       y = \"Unemployment Rate\",\n       title = \"Unmployment rate in Texas MSAs\")+\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\nWe see that among the first 6 MSAs in the state (that are in the ACS microdata), Corpus Christi has the highest unemployment rate a %, and College Station-Bryan has the lowest unemployment rate at %.\nWe can also use these functions for continuous variables, say with incomes. In the code below, I pipe all the elements of the analysis together to illustrate the workflow that we can do to calculate the median income in each MSA and plot it along with its error.\n\nipums %&gt;%\n  filter(EMPSTAT %in% 1:2,\n         AGE &gt;= 16 & AGE &lt;= 65, \n         MET2013 != 0) %&gt;%\n  mutate(income = ifelse(INCWAGE &lt;= 0, NA, INCWAGE),\n         met_name = haven::as_factor(MET2013)) %&gt;%\n  as_survey_design(cluster = CLUSTER,\n                   strata = STRATA,\n                   weights = PERWT) %&gt;%\n  group_by(met_name)%&gt;%\n  summarize(median_wage = survey_median(income, na.rm=T)) %&gt;%  \n  head() %&gt;%\n  ggplot()+\n  geom_bar(aes(x=met_name, y = median_wage), stat = \"identity\")+\n  geom_errorbar(aes(x=met_name,\n                    ymin=median_wage-1.96*median_wage_se,\n                    ymax= median_wage+1.96*median_wage_se),\n                width=.25)+\n  scale_y_continuous(labels = scales::dollar)+\n  labs(x = \"MSA\", \n       y = \"Median Wage\",\n       title = \"Median wage in Texas MSAs\")+\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\nWhich shows that Austin-Round Rock has the highest median wage and Brownsville-Harlingen has the lowest median wage."
  },
  {
    "objectID": "survey.html#creating-tables-from-survey-data-analysis",
    "href": "survey.html#creating-tables-from-survey-data-analysis",
    "title": "3  Analysis of Survey Data",
    "section": "4.10 Creating tables from survey data analysis",
    "text": "4.10 Creating tables from survey data analysis\nTabular output from our survey data analysis is possible through several different means. When using dplyr, our intermediate output of the analysis is always a data frame, and so any R method for printing data frames would work for simple tabular display. For instance, if we just use knitr::kable() on our workflow from above instead of piping into a plot we would get something like this:\nipums %&gt;%\n  filter(EMPSTAT %in% 1:2,\n         AGE &gt;= 16 & AGE &lt;= 65, \n         MET2013 != 0) %&gt;%\n  mutate(income = ifelse(INCWAGE &lt;= 0, NA, INCWAGE),\n         met_name = haven::as_factor(MET2013)) %&gt;%\n  as_survey_design(cluster = CLUSTER,\n                   strata = STRATA,\n                   weights = PERWT) %&gt;%\n  group_by(met_name)%&gt;%\n  summarize(median_wage = survey_median(income, na.rm=T)) %&gt;%  \n  head() %&gt;%\n  knitr::kable(format = \"latex\",\n               digits = 0,\n               caption = \"Median Wages in Texas MSAs\",\n               align = 'c',\n               col.names =c(\"MSA Name\", \"Median Wage\", \"Median Wage SE\"))\n\n\n\nWhich is OK, but there are other ways to make tables for reports. The gt package (Iannone, Cheng, and Schloerke 2021) is built using tidyverse principles, and build tables in much the same way that ggplot builds plots, and fits easily into a dplyr workflow. Here, I use gt to produce a similar table to that from knitr::kable from above.\n\nlibrary(gt, quietly = T)\n\nipums %&gt;%\n  filter(EMPSTAT %in% 1:2,\n         AGE &gt;= 16 & AGE &lt;= 65, \n         MET2013 != 0) %&gt;%\n  mutate(income = ifelse(INCWAGE &lt;= 0, NA, INCWAGE),\n         met_name = haven::as_factor(MET2013)) %&gt;%\n  as_survey_design(cluster = CLUSTER,\n                   strata = STRATA,\n                   weights = PERWT) %&gt;%\n  group_by(met_name)%&gt;%\n  summarize(median_wage = survey_median(income, na.rm=T)) %&gt;%  \n  head()%&gt;%\n  gt() %&gt;%\n    tab_header(title = \"Median Wages in Texas MSAs\")%&gt;%\n    cols_label(met_name = \"MSA Name\",\n                 median_wage = \"Median Wage\",\n                 median_wage_se = \"Median Wage SE\")%&gt;%\n    fmt_number(columns = c( median_wage,  median_wage_se), \n                 decimals = 0, use_seps = TRUE)\n\n\n\n\n\n  \n    \n      Median Wages in Texas MSAs\n    \n    \n    \n      MSA Name\n      Median Wage\n      Median Wage SE\n    \n  \n  \n    Amarillo, TX\n33,000\n1,529\n    Austin-Round Rock, TX\n42,000\n1,020\n    Beaumont-Port Arthur, TX\n40,000\n1,784\n    Brownsville-Harlingen, TX\n25,200\n1,020\n    College Station-Bryan, TX\n30,000\n1,529\n    Corpus Christi, TX\n34,000\n1,428\n  \n  \n  \n\n\n\n\nIn general, the gt tables are much easier to make look nice, compared to basic tables, because they’re much more customizable. The gtsummary package extends the table functionality by combining the summary functions like dplyr with the table structures of gt. Additionally, it will recognize survey design objects so that information can also be integrated into your workflow. The gtsummary presents a more descriptive statistical summary of the variables included, and actually uses dplyr tools under the hood of the package.\nIn the code below, I first filter and mutate the IPUMS data to contain working age people who are employed, and who live in the six Texas cities featured in the examples above. I also create a new income variable that excludes all zero incomes, and drop levels of the MET2013 variable that aren’t in the list I specified. This pipes into the survey design function from the survey package, which pipes into the tbl_svysummary function which summarizes income for each MSA. This function has a lot of options to specify its output, and I recommend you consult the examples at the author’s website2.\n\nlibrary(gtsummary)\n\nipums %&gt;%\n filter(EMPSTAT == 1,\n         AGE &gt;= 16 & AGE &lt;= 65, \n         MET2013 != 0, \n         MET2013 %in% c(11100, 12420, 13140, 15180, 17780, 18580)) %&gt;% \n  mutate(income = ifelse(INCWAGE &lt;= 0, NA, INCWAGE),\n         met_name = haven::as_factor(MET2013))%&gt;%\n  select(met_name,  income, CLUSTER, STRATA, PERWT)%&gt;%\n  droplevels()%&gt;%\n  survey::svydesign(id = ~ CLUSTER,\n                   strata = ~ STRATA,\n                   weights = ~ PERWT,\n                data = .) %&gt;%\n   tbl_svysummary(by = \"met_name\",\n                  missing = \"no\",\n                  include = c(met_name, income), \n                  label = list(income = \"Median Wage\"))%&gt;%\n  as_hux_table()\n\n\n\n\nCharacteristic\nAmarillo, TX, N = 114,201\nAustin-Round Rock, TX, N = 1,193,654\nBeaumont-Port Arthur, TX, N = 163,936\nBrownsville-Harlingen, TX, N = 161,922\nCollege Station-Bryan, TX, N = 111,576\nCorpus Christi, TX, N = 208,801\n\n\nMedian Wage\n34,000 (19,200, 56,000)\n43,000 (24,000, 75,000)\n40,000 (20,000, 67,000)\n26,000 (14,000, 47,000)\n30,000 (14,000, 51,111)\n35,000 (20,000, 58,000)\n\n\nMedian (IQR)\n\n\n\n\n\n\n\n\n\n4.10.1 How this differs from simple random sampling\nSo the big question I often get from students is “Do I really need to weight my analysis?” and of course, I say, “Of course!”. Weights don’t just inflate your data to the population, they serve a very important role in making sure your sample data aren’t biased in their scope. Bias can enter into survey data in many forms, but nonresponse bias can dramatically affect population based estimates if key segments of our target population respond at low rates. Surveys will often deal with this by rigorous data collection strategies, but often the survey designers have to account for the added probability of nonresponse by modification of their basic weights. Lohr (2019) describes how this is done using a variety of methods including raking and post stratification, which typically separate the sample into subdivisions based on one or more demographic characteristic and produce weights based on how the sample deviates from the population composition. These methods are robust and are commonly used in large national surveys including the Behavioral Risk Factor Surveillance system (BRFSS).\nOther reasons for weighting and why it matters are oversampling. Many surveys will do this in their data collection because an important element of their target population may be small in overall size, so they will sample more respondents from that population subgroup than their proportion in the larger population would predict. For example, if a survey wanted to get detailed data on recent refugees to a country, and this group is small, say .1 percent of the overall population, they may design their study to have 10% of their survey respondents be refugees. This oversampling can be accounted for in the survey weights so the additional respondents are down-weighted to represent their fraction of the target population, while still allowing the researchers to get a large sample from this group. Surveys such as the Early Childhood Longitudinal Surveys and the National Longitudinal Study of Adolescent to Adult Health (AddHealth) routinely over-sample specific groups. For example the AddHealth over-samples black adolescents with college educated parents, Cuban, Puerto Rican, Chinese, and physically disabled adolescents.\n\n\n4.10.2 How do weights affect our estimates?\nIn the code example below, I illustrate how not including sample weights affects the estimates generated. This in effect is how traditional statistical analysis assuming random sampling would do things. So in order to compare the weighted estimates to the unweighted estimates, all we need to do is use a non-survey design oriented method to produce our estimate, and compare it to the survey design oriented method. Note that many surveys are designed to be self-weighting, so weights are not provided nor necessary, again, read the documentation for your specific survey for what it recommends.\n\nsrs&lt;-ipums %&gt;%\n  filter(EMPSTAT %in% 1:2,\n         AGE &gt;= 16 & AGE &lt;= 65, \n         MET2013 != 0) %&gt;%\n  mutate(employed = as.factor(case_when(.$EMPSTAT == 1 ~ \"Employed\",\n                              .$EMPSTAT == 2 ~ \"Unemployed\" )),\n         met_name = haven::as_factor(MET2013)) %&gt;%\n  group_by(met_name)%&gt;%\n  summarize(emp_rate = mean(I(employed)==\"Employed\"),\n            emp_rate_se = sd(I(employed)==\"Employed\")/sqrt(n()))%&gt;%\n   # pivot_wider(id = met_name,\n   #            names_from = employed,\n   #            values_from = c(emp_rate, emp_rate_se) ) %&gt;%\n  head()\nsrs$estimate&lt;-\"Unweighted\"\n\n\nsurv.est&lt;-ipums %&gt;%\n  filter(EMPSTAT %in% 1:2,\n         AGE &gt;= 16 & AGE &lt;= 65, \n         MET2013 != 0) %&gt;%\n  mutate(employed = as.factor(case_when(.$EMPSTAT == 1 ~ \"Employed\",\n                              .$EMPSTAT == 2 ~ \"Unemployed\" )),\n         met_name = haven::as_factor(MET2013)) %&gt;%\n  as_survey_design(cluster = CLUSTER,\n                   strata = STRATA,\n                   weights = PERWT) %&gt;%\n  group_by(met_name)%&gt;%\n  summarize(emp_rate = survey_mean(I(employed)==\"Employed\")) %&gt;%  \n  #rename(emp_rate_surv = emp_rate, emp_rate_se_surv = emp_rate_se)%&gt;%\n  head()\n\nsurv.est$estimate&lt;-\"Weighted\"\n\n\nmerged &lt;- rbind(srs, surv.est)\np1&lt;-merged%&gt;%\n  ggplot(aes(y = emp_rate, x = met_name , color = estimate))+\n  geom_point(stat=\"identity\",\n             cex=2)+\n  geom_line(aes(group = met_name),\n            col = \"grey\")+\n  scale_y_continuous(labels = scales::percent)+\n  scale_color_discrete( name = \"Estimate Type\")%&gt;%\n  labs(x = \"MSA\", \n       y = \"Employment Rate Estimate\",\n       title = \"Employment rate in Texas MSAs\",\n       subtitle = \"Rate estimates\")+\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\np2&lt;-merged%&gt;%\n  ggplot(aes(y = emp_rate_se, x = met_name , color = estimate))+\n  geom_point(stat=\"identity\",\n             cex=2)+\n  geom_line(aes(group = met_name),\n            col = \"grey\")+\n  scale_color_discrete()%&gt;%\n  labs(x = \"MSA\", \n       y = \"Standard Error\",\n       title = \"\",\n       subtitle = \"Standard error of estimates\")+\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nlibrary(patchwork)\n\n(p1 + p2)\n\n\n\n\nWe see that the difference in the weighted and unweighted estimates do vary by MSA, as do the standard errors of the estimates, with the survey-design calculated standard errors nearly always being higher than those assuming simple random sampling. If our data came from a survey with more extreme oversampling we would likely see larger differences in these estimates, and it is generally advisable to include both weights and survey design elements in all analysis of complex survey data."
  },
  {
    "objectID": "survey.html#basic-statistical-testing-on-survey-data.",
    "href": "survey.html#basic-statistical-testing-on-survey-data.",
    "title": "3  Analysis of Survey Data",
    "section": "4.11 Basic statistical testing on survey data.",
    "text": "4.11 Basic statistical testing on survey data.\nTo perform basic bivariate statistical tests for frequency tables, the srvyr::svychisq and survey::svy_chisq are the primary tools you need. These will test for basic independence among rows and columns of a frequency table. Below is an example on how you would test for independence of labor force participation and gender in the IPUMS ACS data.\n\nipums %&gt;%\n  filter(EMPSTAT != 0,\n         AGE &gt;= 16 & AGE &lt;= 65) %&gt;%\n  mutate(lab_force_part = ifelse (test = EMPSTAT %in% c(1,2),\n                                  yes = 1,\n                                  no = 0), \n         gender = ifelse(test = SEX ==1,\n                         yes = \"Male\",\n                         no = \"Female\")) %&gt;%\n  as_survey_design(cluster = CLUSTER,\n                   strata = STRATA,\n                   weights = PERWT) %&gt;%\n  svychisq(~lab_force_part+gender,\n                    design = .)\n\n\n    Pearson's X^2: Rao & Scott adjustment\n\ndata:  NextMethod()\nF = 1955.4, ndf = 1, ddf = 172976, p-value &lt; 0.00000000000000022\n\n\nThe output above is for the survey adjusted chi square test (Rao and Scott 1984), which is actually calculated as an F-test in this case. We see a large F-test value, and a very small p value, which indicates that men and women have different labor force participation rates. Using our workflow from above, we can calculate the actual rates as well.\n\nipums %&gt;%\n  filter(EMPSTAT != 0,\n         AGE &gt;= 16 & AGE &lt;= 65) %&gt;%\n  mutate(lab_force_part = ifelse (test = EMPSTAT %in% c(1,2),\n                                  yes = 1,\n                                  no = 0), \n         gender = ifelse(test = SEX ==1,\n                         yes = \"Male\",\n                         no = \"Female\")) %&gt;%\n  as_survey_design(cluster = CLUSTER,\n                   strata = STRATA,\n                   weights = PERWT) %&gt;%\n  group_by(gender)%&gt;%\n  summarize(lf_part_rate = survey_mean(lab_force_part, na.rm=T)) %&gt;%  \n  head()%&gt;%\n  gt() %&gt;%\n    tab_header(title = \"Labor Force Participation Rates in Texas\")%&gt;%\n    cols_label(gender = \"Gender\",\n                 lf_part_rate = \"Labor Force Participation Rate\",\n                lf_part_rate_se = \"SE\")%&gt;%\n    fmt_number(columns = c( lf_part_rate,  lf_part_rate_se), \n                 decimals = 3, use_seps = TRUE)\n\n\n\n\n\n  \n    \n      Labor Force Participation Rates in Texas\n    \n    \n    \n      Gender\n      Labor Force Participation Rate\n      SE\n    \n  \n  \n    Female\n0.674\n0.002\n    Male\n0.797\n0.002\n  \n  \n  \n\n\n\n\nSo we see that males have a much higher labor force participation rate, compared to females, and this puts the differences that we observed from the chi square test into better context.\n\n4.11.1 Regression and survey design\nThe design of our surveys affect the most basic estimates we do, and likewise, the design affects the more complicated analysis as well. Regression models are the work horse of social science research and we will spend a significant amount of the chapters that follow on thorough inspection of them. In the context of this chapter, I felt like I need to show both how to include survey design in a regression model and illustrate that weighting and survey design matters in terms of the output from out models. This section is NOT a total coverage of these models, and is at best a short example. This example will go in a different direction and use data from the Demographic and Health Survey instead of the ACS.\nThe Demographic and Health Survey (DHS) data have been collected since the mid 1980’s in over 90 countries around the world, and the DHS provides a public model data set that represents data on real households, without a specific national context. These data are provided to let people learn how to use the data before applying for access. The model data can be downloaded freely from the DHS[^surveydata-3] as a SAS or STATA format, or from my Github site[^surveydata-4] for this book. Below, I will read in the data from Github and recode child growth stunting relative to the WHO standard as an outcome[^surveydata-5] , and child age, rural residence and gender as predictors.\nThe DHS household file is arrayed with a column for every child, so we must reshape the data from wide to long format using pivot_longer for the variables for child height relative to the WHO standard (hc70), child gender hc27, and child age hc1. The other variables are common to the household, so we do not have to reshape them (per the cols= line below). We then recode the outcome and the predictors for our regression example.\n\ndhs_model_hh &lt;- readRDS(\n  url(\"https://github.com/coreysparks/data/blob/master/dhs_model_hh.rds?raw=true\")\n  )\n\ndhs_model_hh_sub &lt;- dhs_model_hh%&gt;%\n  select(hc27_01:hc27_20, \n         hc70_01:hc70_20, \n         hc1_01:hc1_20,\n         hv021, hv025, hv270, hv005, hv021, hv022)%&gt;%\n  pivot_longer(cols = c(-hv021, -hv025, -hv270, -hv005, -hv021, -hv022), \n               names_to  = c(\".value\", \"child\"),\n               names_sep = \"_\") %&gt;%\n  na.omit()%&gt;%\n  mutate(stunting = car::Recode(hc70, recodes = \"-900:-200 = 1; 9996:9999 = NA; else = 0\"),\n         gender = ifelse(test = hc27 == 1, yes = \"male\", no = \"female\"),\n         hh_wealth = as.factor(hv270),\n         age = hc1,\n         age2 = hc1^2,\n         rural = ifelse(test = hv025 ==2, yes = \"rural\", no = \"urban\"),\n         wt = hv005/1000000,\n         psu = hv021,\n         strata = hv022)\n\n\ndhs_model_des&lt;- dhs_model_hh_sub%&gt;%\n  as_survey_design(cluster = psu,\n                   strata = strata,\n                   weights = wt,\n                   nest = TRUE)\n\nsummary(dhs_model_des)\n\nStratified Independent Sampling design (with replacement)\nCalled via srvyr\nProbabilities:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.1552  0.7872  1.2943  1.5297  2.0336  6.6422 \nStratum Sizes: \n            1   2  3  4   5  6   7   8   9  10  11 12  13 14  15  16  17 18  19\nobs        33 136 52 41 159 23 108 117 185 266 218 44 244 25 125 183 152 80 144\ndesign.PSU 33 136 52 41 159 23 108 117 185 266 218 44 244 25 125 183 152 80 144\nactual.PSU 33 136 52 41 159 23 108 117 185 266 218 44 244 25 125 183 152 80 144\n           20 21 22 23 24 25  26  27\nobs        62 83 74 74 73  6 122 124\ndesign.PSU 62 83 74 74 73  6 122 124\nactual.PSU 62 83 74 74 73  6 122 124\nData variables:\n [1] \"hv021\"     \"hv025\"     \"hv270\"     \"hv005\"     \"hv022\"     \"child\"    \n [7] \"hc27\"      \"hc70\"      \"hc1\"       \"stunting\"  \"gender\"    \"hh_wealth\"\n[13] \"age\"       \"age2\"      \"rural\"     \"wt\"        \"psu\"       \"strata\"   \n\n\nJust for completeness, I create a bar chart showing the percent stunted by the two main variables, gender and rural residence.\n\ndhs_model_des%&gt;%\n  group_by(gender, rural)%&gt;%\n  summarise(stunting = survey_mean( stunting,design =.,\n                              na.rm=T) )%&gt;%\n  ggplot()+\n  geom_bar(aes(x = gender, y  = stunting, fill = rural),\n           stat=\"identity\",\n           position=\"dodge\")+\n  labs(x = \"Child Gender\", \n       y = \"Percent Stunted\", \n       title = \"Percent Stunted by Gender and Rural Residence\",\n       subtitle = \"DHS Model Data\")+\n  scale_y_continuous(labels = scales::percent)\n\n\n\n\nNow I estimate two logistic regression models for the stunting outcome. The first assumes random sampling and the second includes the full survey design information.\nFor the unweighted regular logistic regression, we use the glm() function with family = binomial (link = \"logit\"). Theglm() function will be used extensivley in the chapters that follow.\n\nlibrary(broom)\nm1&lt;-glm(stunting ~ age + age2 + rural + gender,\n              data = dhs_model_hh_sub,\n              family = binomial(link = \"logit\"))\nm1%&gt;%\n  tidy()\n\n# A tibble: 5 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept) -1.67     0.158      -10.6   4.47e-26\n2 age          0.0976   0.0111       8.82  1.10e-18\n3 age2        -0.00145  0.000176    -8.23  1.91e-16\n4 ruralurban  -0.484    0.0934      -5.19  2.15e- 7\n5 gendermale   0.0461   0.0837       0.551 5.82e- 1\n\n\nFor the survey design model, you specify the design versus the data set name, otherwise the same code works just fine.\n\nm2 &lt;- dhs_model_des%&gt;%\n  svyglm(stunting ~ age + age2 +rural + gender,\n         design = .,\n         family = binomial)\n\nWarning in eval(family$initialize): non-integer #successes in a binomial glm!\n\nm2%&gt;%\n  tidy()\n\n# A tibble: 5 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept) -1.77     0.180      -9.83   2.03e-22\n2 age          0.108    0.0128      8.39   8.18e-17\n3 age2        -0.00164  0.000209   -7.84   6.77e-15\n4 ruralurban  -0.432    0.128      -3.38   7.25e- 4\n5 gendermale   0.00938  0.103       0.0906 9.28e- 1\n\n\nThere are lots of ways to make a table from a regression model, but stargazer (Hlavac 2018) is a simple way to present multiple models side by side.\nstargazer::stargazer(m1, m2,\n                     keep.stat = \"n\",\n                     model.names = F,\n                     column.labels = c(\"Unweighted model\", \"Weighted Model\"),\n                     type = \"latex\",\n                     header = FALSE, \n                     style = \"demography\",\n                     title = \"Output from Unweighted and Weighted Regression Models\")\nIn this case, we see that the coefficient estimates are very similar between the two models, but the coefficient standard errors are all smaller in the unweighted model. This is commonly what you see in this situation, because the survey deign model is actually using clustered standard errors instead of asymptotic standard errors Ibragimov and Müller (2016). While this example does not show an extreme difference, it is commonplace for t-statistics generated from clustered standard errors to have higher p-values than those using asymptotic standard errors. As a result, if the t-statistic is lower, and the p-value higher, you can easily get differences in your hypothesis tests for regression parameters. This is always something to be aware of as you analyze survey data.\n[^surveydata-3] https://dhsprogram.com/data/model-datasets.cfm [^surveydata-4] https://github.com/coreysparks/dem-stats-book [^surveydata-4] Per the guide to DHS statistics"
  },
  {
    "objectID": "survey.html#chapter-summary",
    "href": "survey.html#chapter-summary",
    "title": "3  Analysis of Survey Data",
    "section": "4.12 Chapter summary",
    "text": "4.12 Chapter summary\nThe goal of this chapter was to review the complexities of demographic survey data sources. The analysis of these kinds of data have to proceed in a way that adheres to the design of the survey as well as the goal of population-level inference from the survey data. The use of person or housing unit weights are necessary to ensure that our statistical summaries and analysis are representative of the population that the survey was designed to measure. The incorporation of the sample design elements of primary sampling units and sampling strata are integral to the correct calculation of standard errors for any survey-based estimates we create. R, like most major statistical programs have several ways of dealing with these issues, and the libraries survey and srvyr are very flexible in the types of analysis they can perform, and can incorporate any survey design into the subsequent analysis that we carry out."
  },
  {
    "objectID": "survey.html#references",
    "href": "survey.html#references",
    "title": "3  Analysis of Survey Data",
    "section": "4.13 References",
    "text": "4.13 References\n\n\n\n\nCameron, A. Colin, and Douglas L. Miller. 2015. “A Practitioner’s Guide to Cluster-Robust Inference.” Journal of Human Resources 50 (2): 317–72. https://doi.org/10.3368/jhr.50.2.317.\n\n\nCDC. 2020. “Weighting the BRFSS Data.” https://www.cdc.gov/brfss/annual_data/2019/pdf/weighting-2019-508.pdf.\n\n\nHlavac, Marek. 2018. Stargazer: Well-Formatted Regression and Summary Statistics Tables. Bratislava, Slovakia: Central European Labour Studies Institute (CELSI). https://CRAN.R-project.org/package=stargazer.\n\n\nIannone, Richard, Joe Cheng, and Barret Schloerke. 2021. Gt: Easily Create Presentation-Ready Display Tables. https://CRAN.R-project.org/package=gt.\n\n\nIbragimov, Rustam, and Ulrich K. Müller. 2016. “Inference with Few Heterogeneous Clusters.” The Review of Economics and Statistics 98 (1): 83–96. https://doi.org/10.1162/REST_a_00545.\n\n\nInternational, ICF. 2012. “Demographic and Health Survey Sampling and Household Listing Manual.” ICF International.\n\n\nLohr, Shron. 2019. Sampling: Design and Analysis. 2nd ed. Boca Raton: Routledg/CRC Press.\n\n\nLumley, Thomas. 2010. Complex Surveys: A Guide to Analysis Using R: A Guide to Analysis Using R. John Wiley; Sons.\n\n\nRao, JNK, and AJ Scott. 1984. “On Chi-Squared Tests For Multiway Contigency Tables with Proportions Estimated From Survey Data.” Annals of Statistics 12: 46–60.\n\n\nRuggles, S, S Flood, Josiah Grover, S Foster, R Goeken, Jose Pacas, Erin Schouweiler, and M Sobek. 2021. “Integrated Public Use Microdata Series: Version 11.0 [Machine-Readable Database].” https://doi.org/https://doi.org/10.18128/D010.V8.0.\n\n\nUS Census Bureau. 2014. “American Community Survey Design and Methodology (January 2014).” https://www2.census.gov/programs-surveys/acs/methodology/design_and_methodology/acs_design_methodology_report_2014.pdf.\n\n\nWalker, Kyle, and Matt Herman. 2021. Tidycensus: Load US Census Boundary and Attribute Data as ’Tidyverse’ and ’Sf’-Ready Data Frames. https://CRAN.R-project.org/package=tidycensus."
  },
  {
    "objectID": "survey.html#footnotes",
    "href": "survey.html#footnotes",
    "title": "3  Analysis of Survey Data",
    "section": "",
    "text": "The modern DHS collects information on couples, men and children, so the universe has been expanded away from just women of childbearing age and their children.↩︎\nhttp://www.danieldsjoberg.com/gtsummary/↩︎"
  },
  {
    "objectID": "macro.html",
    "href": "macro.html",
    "title": "4  Macrodemographic Analysis of Places",
    "section": "",
    "text": "5 Macro demographic data analysis\nPrior to the advent in the 1960’s of large scale social surveys like the General Social Survey (GSS), most demographic research was done not on individuals but on aggregates, because that’s how data were available. If you look at texts such as Keyfitz (1968), all of the examples are for national level calculations, and many nations did not have sufficient data availability to produce quality statistical summaries of their populations, resulting in publications such as the United Nations Population Division’s famous Manual X (1983), which gave pragmatic formulas to measure a wide variety of demographic indicators at the national level using basic inputs, usually available from census summaries.\nPaul Voss (2007) describes most demography (and certainly most demographic studies prior to the 1970’s and 1980’s) as Macro demography. Voss also mentions that prior to the availability of individual level microdata, all demography was macro-demography, and most demographic studies were spatial in nature, because demographic data were only available in spatial units corresponding to administrative areas. Typical types of geographic areas would be counties, census tracts, ZIP codes, state or nations.\nIn the macro-demographic perspective on demography, observations are typically places, areas, or some other aggregate level of individuals. We do not observe the individual people themselves often times. An example of this is if you were to have access to an aggregate count of deaths in a region, even if the deaths were classified by age and sex, you still would be dealing with data that ignores, or has no index to the more nuanced characteristics of the individual decedents themselves. That being said, data such as these are invaluable, and most demographic summaries of individual-level data would aggregate based on the characteristics of the individuals any way. The macro scale principal is illustrated below, where all of the variables we observe are a scale above the individual person.\nSuch macro-level propositions are hypothesized relationships among variables (\\(\\rightarrow\\)) measured at a macro scale (\\(Z\\) and \\(Y\\)), which ignores individual level data, mostly because we don’t observe individuals (\\(x\\) and \\(y\\)) in many of these kinds of analysis.\nIf all we looked at were the individuals within the population, we would be overwhelmed by the variation that we would see, and we wouldn’t be doing statistics anymore, we would be trying to process a million anecdotes, and the plural of anecdote is not data. By aggregating across basic demographic groups, such as age and sex, demographers begin to tease apart the differences that we are interested in. If we go a little further and, data willing, aggregate not only across these fundamental demographic groups, but also across some kind of place-based areal unit, then we adding an extremely important part of human existence: the where part of where we live.\nThis presents an attractive view of populations and typically data on places are more widely available, but there are caveats we must be aware of. If we are using purely aggregate data in our analysis, meaning that we do not have access to the individual level microdata, then our ability to observe variation within a place is extremely limited, if not impossible.\nThe goal of this chapter is to illustrate how places are a special unit of analysis, and the types of data we often see at the place level are very different from individual level surveys. Additionally, the analysis of place-based data is similar to survey data in that places are do not necessarily represent random observations, and so analyzing data on places often requires special modifications to statistical models. In this chapter, I show how the the linear regression model can be expanded in several ways and illustrate the generalized linear model as a very useful and extendable tool to analyze data on places and especially when we are analyzing rates as demographers often do."
  },
  {
    "objectID": "macro.html#getting-data-on-places",
    "href": "macro.html#getting-data-on-places",
    "title": "4  Macrodemographic Analysis of Places",
    "section": "5.1 Getting data on places",
    "text": "5.1 Getting data on places\nIn the macro-demographic perspective on demography, observations are typically places, areas, or some other aggregate level of individuals. We do not observe the individual people themselves often times. An example of this is if you were to have access to an aggregate count of deaths in a region, even if the deaths were classified by age and sex, you still would be dealing with data that ignores, or has no index to the more nuanced characteristics of the individual decedents themselves. That being said, data such as these are invaluable, and most demographic summaries of individual-level data would aggregate based on the characteristics of the individuals any way. The macro scale principal is illustrated below, where all of the variables we observe are a scale above the individual person.\n\n\n\nMacro Level Proposition\n\n\nSuch macro-level propositions are hypothesized relationships among variables (\\(\\rightarrow\\)) measured at a macro scale (\\(Z\\) and \\(Y\\)), which ignores individual level data, mostly because we don’t observe individuals (\\(x\\) and \\(y\\)) in many of these kinds of analysis.\nIf all we looked at were the individuals within the population, we would be overwhelmed by the variation that we would see, and we wouldn’t be doing statistics anymore, we would be trying to process a million anecdotes, and the plural of anecdote is not data. By aggregating across basic demographic groups, such as age and sex, demographers begin to tease apart the differences that we are interested in. If we go a little further and, data willing, aggregate not only across these fundamental demographic groups, but also across some kind of place-based areal unit, then we adding an extremely important part of human existence: the where part of where we live.\nThis presents an attractive view of populations and typically data on places are more widely available, but there are caveats we must be aware of. If we are using purely aggregate data in our analysis, meaning that we do not have access to the individual level microdata, then our ability to observe variation within a place is extremely limited, if not impossible.\nThe goal of this chapter is to illustrate how places are a special unit of analysis, and the types of data we often see at the place level are very different from individual level surveys. Additionally, the analysis of place-based data is similar to survey data in that places are do not necessarily represent random observations, and so analyzing data on places often requires special modifications to statistical models. In this chapter, I show how the the linear regression model can be expanded in several ways and illustrate the generalized linear model as a very useful and extendable tool to analyze data on places and especially when we are analyzing rates as demographers often do."
  },
  {
    "objectID": "macro.html#getting-data-on-places-1",
    "href": "macro.html#getting-data-on-places-1",
    "title": "4  Macrodemographic Analysis of Places",
    "section": "5.2 Getting data on places",
    "text": "5.2 Getting data on places\nTypically when thinking about data on places, we are really referring to some sort of administrative geography, such as nations, states, region, and census tracts. While these are often readily available (and I’ll show some R package that can easily get data from the web), we often have to use these as proxy measures of more interesting social spaces like neighborhoods and other types of activity spaces. These social spaces are harder to get data on, typically because they are more fluid in their definitions, and there is generally not a systematic effort to produce data on socially defined spaces on national scales. This is a big part of doing macro demography, defining the scale and the unit of analysis, both because we need to define the scope of our work, but also we are very much constrained by the availability of data for our projects. For instance, I may want to look at national scale inequality in mortality risk in neighborhoods in the United States, but you immediately face a couple of hurdles. No national data source identifies sub-city residential location for death certificates, also, what are neighborhoods? Again, they’re probably some socially defined space that may not be available from a national scale source. To get around this, we may have to settle for a state-level analysis, because state vital registration systems will often allow researchers to use more fine-scale geographic data on death certificates (such as latitude/longitude of the decedent’s residence), and once we have very fine scale geographic data on the vital events, we could potentially find data on some more socially defined spaces, perhaps from cities who often maintain geographic data on neighborhoods specific to that city. OK, so that’s fine, but then you still run into the “what’s my denominator” problem, where you have no baseline population data on the age and sex breakdown of the population, or even the population size of these places, because federal agencies don’t produce estimates for such small scale areas. This is frustrating. Often when advising students on their dissertation projects, I have to have this moment of truth where I lay out the problems of the mixing of geographic scales for their projects, and the hard reality of the lack of data on so many things they would like to study. Often what happens is that we have to proxy our ideal places with places for which we can find data. You see this a lot in the population health literature, where people want to analyze neighborhoods but all they have are census tracts. Tracts aren’t social spaces! They’re arbitrary areas of 3 to 5 thousand people, that change every 10 years, that the Census uses to count people. Likewise, counties are very rich areas to find data for, but they are not really activity spaces or neighborhoods, but they may be areas that have some policy making authority (such as county health departments) that could be relevant for something. States are also nice geographies, they’re very large, so you loose the ability to contextualize behavior on a fine spatial scale, but states make a lot of decisions that affect the lives of their residents, often more than national decisions. States have become very popular units of analysis in the health literature again, primarily as a result of differential adoption of portions of the Patient Protection and Affordable Care Act of 2010 (Soni, Hendryx, and Simon 2017; Courtemanche et al. 2019). This being said, many times when we do an analysis on places, that analysis has lots of limitations, which we must acknowledge, and analyses such as these are often called ecological analyses because we are examining associations at the macro scale, and we do not observe individual level outcomes."
  },
  {
    "objectID": "macro.html#us-contexts",
    "href": "macro.html#us-contexts",
    "title": "4  Macrodemographic Analysis of Places",
    "section": "5.3 US contexts",
    "text": "5.3 US contexts\nThe US Census bureau produces a wide variety of geographic data products that are the most widely used forms of geographic data for demographic studies in the United States. The TIGER Line Files data consist of geographic data with census bureau GEOIDs attached so they can be linked to any number of federal statistical products. They do not contain demographic data themselves, but are easily linked. The tigris package in R provides a direct way to download any TIGER line file data type directly in a R session as either a simple feature class or as a Spatial_DataFrame (Walker 2021).\nUsing the tigris package is very easy and its functions fit directly into the tidyverse as well. Below, I download two layers of information, first the state polygon for New York state, and the census tracts within the state and overlay the two datasets on each other. The package has a function for each type of geography that you would want, for example states() downloads state level geographies and tracts() does the same for census tracts. The functions have some common arguments, including cb = TRUE/FALSE so you can choose cartographic boundary files or not. Cartographic boundary files are lower resolution, smaller files that are often used for thematic mapping. Also year = will allow you to get different annual vintages of the data. The tracts() function also allows you to obtain geographies for specific counties within a state.\n\nlibrary(tigris)\n\nnyst &lt;- states(cb = TRUE,\n               year = 2010) %&gt;%\n  filter(NAME == \"New York\")\n\nnyst_ct &lt;- tracts(state = \"NY\",\n                  cb = TRUE,\n                  year = 2010)\n\nggplot(data=nyst)+\n  geom_sf(color = \"red\", \n          lwd = 2)+\n   geom_sf(data = nyst_ct,\n           fill = NA,\n           color = \"blue\") + \n  ggtitle(label = \"New York State Census Tracts\")\n\n\n\n\n\n5.3.1 Tidycensus\nAnother package the provides access to the US Census Bureau Decennial census summary file , the American Community Survey, Census population estimates, migration flow data and Census Public Use Microdata Sample (PUMS) data is tidycensus (Walker and Herman 2021). The tidycensus package primarily works to allow users to use the Census Bureau’s Application Programming Interface (API) to download Census summary file data for places within an R session. This removes the need to download separate files to your computer, and allows users to produce visualizations of Census data easily. The package is actively maintained and has several online tutorials on how to use it 1. Depending on which data source you are interested in, there are functions that allow extracts from them. The ACS data is accessed through the get_acs() function, likewise the decennial census data is accessed using the get_decennial() function. The package also allows users to test for differences in ACS estimates either across time or between areas using the significance() function.\nThe package requires users to obtain a developer API key from the Census Bureau’s developer page2 and install it on your local computer. The package has a function that helps you install the key to your .Renviron file. It is used like this:\n\ncensus_api_key(key = \"yourkeyhere\", install = TRUE)\n\nwhich only needs to be done once.\nA basic use of the tidycensus package is to get data and produce maps of the indicators. This is done easily because tidycensus fits directly into general dplyr and ggplot2 workflows. Below is an example of accessing 2019 ACS data on poverty rate estimates for New York census tracts from New York county, New York. The syntax takes several arguments indicating what level of census geography you want, the year of the estimates, the details of states and counties you may want, and which ACS tables you want. Here I use the Data Profile table for the percentage estimate of families with incomes below the poverty line. The output = \"wide\" option is useful if you get multiple estimates, as it arranges them into columns, one for each estimate.\n\nlibrary(tidycensus)\n\nnyny &lt;- get_acs(geography = \"tract\",\n                year = 2018,\n                state = \"NY\",\n                county = \"061\",\n                variables = \"DP03_0119PE\", \n                output = \"wide\",\n                geometry = TRUE)\n\nGetting data from the 2014-2018 5-year ACS\n\n\nDownloading feature geometry from the Census website.  To cache shapefiles for use in future sessions, set `options(tigris_use_cache = TRUE)`.\n\n\nUsing the ACS Data Profile\n\n\nThe tabular output shows the Estimate column ending in E and the ACS margin of error column ending in M.\n\nknitr::kable(x = head(nyny),\n             format = \"html\")\n\n\n\n\nGEOID\nNAME\nDP03_0119PE\nDP03_0119PM\ngeometry\n\n\n\n\n36061020101\nCensus Tract 201.01, New York County, New York\n0.0\n20.0\nMULTIPOLYGON (((-73.96155 4...\n\n\n36061020701\nCensus Tract 207.01, New York County, New York\n24.1\n17.0\nMULTIPOLYGON (((-73.95922 4...\n\n\n36061022200\nCensus Tract 222, New York County, New York\n18.5\n9.1\nMULTIPOLYGON (((-73.95068 4...\n\n\n36061022600\nCensus Tract 226, New York County, New York\n15.5\n9.4\nMULTIPOLYGON (((-73.94703 4...\n\n\n36061000600\nCensus Tract 6, New York County, New York\n37.5\n9.8\nMULTIPOLYGON (((-73.99256 4...\n\n\n36061001600\nCensus Tract 16, New York County, New York\n22.2\n8.5\nMULTIPOLYGON (((-73.99606 4...\n\n\n\n\n\n\n\nThe geometry = TRUE option also download the TIGER line file for the requested geography and merges it to the ACS estimates. This allows you to immediately map the estimates for the requested geographies.\n\n# Create map of estimates\nnyny %&gt;% \n  rename (Poverty_Rt = DP03_0119PE)%&gt;%\n  ggplot(aes(fill = Poverty_Rt))+\n  geom_sf()+\n  scale_fill_viridis_c()+\n  ggtitle ( label = \"Poverty Rate in New York Census Tracts\", \n            subtitle = \"2018 ACS Estimates\")\n\n\n\n\nThe tidycensus package had a great array of functions and the author Kyle Walker has published a book on using it FILL IN CITATION which covers its many uses.\nOne common task that we should do when visualizing ACS estimates is to examine the coefficient of variation in the estimates. This gives us an idea of how stable the estimates are. This can be particularly problematic as we use smaller and smaller geographies in our analysis. Below, I calculate the coefficient of variation for the estimates and map it. To get the standard error of the ACS estimate, I divide the margin of error by 1.645, following Census Bureau recommendations (Bureau 2019).\n\nnyny %&gt;% \n  mutate ( cv =ifelse(test = DP03_0119PE==0,\n                      yes = 0,\n                      no = (DP03_0119PM/1.645) / DP03_0119PE))%&gt;%\n  ggplot(aes(fill = cv))+\n  geom_sf()+\n  scale_fill_viridis_c()+\n  ggtitle ( label = \"Poverty Rate Coefficient of Variation\\n in New York Census Tracts\", \n            subtitle = \"2018 ACS Estimates\")\n\n\n\n\nwhich shows areas with the highest coefficient of variations mostly adjacent to Central Park and on the lower west side of Manhattan. These are also the areas with the lowest poverty rates in the city, so the estimates have low precision because so few respondents report incomes below the poverty line.\n\n\n5.3.2 IPUMS NHGIS\nThe IPUMS NHGIS project 3 is also a great source for demographic data on US places, and allows you to select many demographic tables for census data products going back to the 1790 census (Manson et al. 2021). When you perform an extract from the site, you can get both data tables and ESRI shapefiles for your requested geographies. The IPUMS staff have created several tutorials which go through how to construct a query from their site 4. Below, I use the sf library to read in the geographic data from IPUMS and the tabular data and join them.\n\nlibrary(sf)\nipums_co &lt;- read_sf(\"data/US_county_2020.shp\")\n\n\nim_dat &lt;- readr::read_csv(\"data/nhgis0025_ds231_2005_county.csv\")\n\nRows: 3143 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (7): GISJOIN, AREANAME, STATE, STATEA, COUNTY, COUNTYA, DATAFLAG\ndbl (5): YEAR, NOTECODE, AGWE001, AGWI001, AGWJ001\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nm_dat &lt;- left_join(x = ipums_co,\n                   y = im_dat,\n                   by = c(\"GISJOIN\" = \"GISJOIN\"))\n\nm_dat %&gt;%\n  filter(STATE == \"New York\" )%&gt;%\n  ggplot()+\n  geom_sf(aes (fill = AGWJ001))+\n  scale_fill_viridis_c()+\n  ggtitle(label = \"Infant Mortality Rate per 10,000 Live Births\",\n          subtitle = \"New York, 2005\")\n\n\n\n\n\n\n5.3.3 International data\nSources of international data exist in numerous sites on the internet. Personally, I frequently will use the DHS Spatial Data repository 5 to access data from DHS sampled countries. This repository allows you to obtain both spatial administrative boundary data, as well as key indicators of maternal and child health at sub-national levels. Additionally, the rdhs package allows you to perform queries from the spatial data repository and from the DHS microdata as well directly via the DHS API from within an R session (Watson, FitzJohn, and Eaton 2019), assuming you have registered with the DHS and have an approved project with them."
  },
  {
    "objectID": "macro.html#statistical-models-for-place-based-data",
    "href": "macro.html#statistical-models-for-place-based-data",
    "title": "4  Macrodemographic Analysis of Places",
    "section": "5.4 Statistical models for place-based data",
    "text": "5.4 Statistical models for place-based data\nData on places is often analysed in the same ways as data on individuals, with some notable complications. The remainder of this chapter introduces the regression framework for analyzing data at the macro level, first by a review of the linear model and its associated pitfalls, and then the generalized linear model with a specific focus on the analysis of demographic count outcomes that are commonly observed for places.\nIn the example below, I use data from the U.S. Health Resources and Services Administration Area Health Resource File (AHRF), which is a produced annually and includes a wealth of information on current and historical data on health infrastructure in U.S. counties, as well as data from the Census Bureau, and the National Center for Health Statistics. The AHRF is publicly available, and we can read the data directly from the HHS website as a SAS format .sas7bdat data set within a ZIP archive. R can read this file to your local computer then extract it using the commands below. I would strongly encourage you consulting the AHRF codebook available from the HRSA website6.\n\n#create temporary file on  your computer\ntemp &lt;- tempfile()\n\n#Download the SAS dataset as a ZIP compressed archive\ndownload.file(\"https://data.hrsa.gov/DataDownload/AHRF/AHRF_2019-2020_SAS.zip\", temp)\n\n#Read SAS data into R\nahrf&lt;-haven::read_sas(unz(temp,\n                          filename = \"ahrf2020.sas7bdat\"))\n\nrm(temp)\n\nNext, I remove many of the variables in the AHRF and recode several others. In the analysis examples that follow in this chapter, I will focus on the outcome of low birth weight births, measured at the county level.\n\nlibrary(tidyverse)\n\nahrf2&lt;-ahrf%&gt;%\n  mutate(cofips = f00004, \n         coname = f00010,\n         state = f00011,\n         popn =  f1198416,\n         births1618 =  f1254616, \n         lowbw1618 = f1255316,\n         fampov14 =  f1443214,\n         lbrate1618 = 1000*(f1255316/f1254616),  #Rate per 1000 births\n         rucc = as.factor(f0002013),\n         hpsa16 = case_when(.$f0978716 == 0 ~ 'no shortage',\n                            .$f0978716 == 1 ~ 'whole county shortage',\n                            .$f0978716 == 2 ~ 'partial county shortage'),\n         obgyn15_pc= 1000*( f1168415 / f1198416 ) )%&gt;%\n  mutate(rucc = droplevels(rucc, \"\"))%&gt;%\n  dplyr::select(births1618,\n                lowbw1618,\n                lbrate1618,\n                state,\n                cofips,\n                coname,\n                popn,\n                fampov14,\n                rucc,\n                hpsa16,\n                obgyn15_pc)%&gt;%\n  filter(complete.cases(.))%&gt;%\n  as.data.frame()\n\nIn order to make a nice looking map of the outcome, I use the tigris package to fetch geographic data for US states and counties, then merge the county data to the AHRF data using left_join()\n\noptions(tigris_class=\"sf\")\nlibrary(tigris)\nlibrary(sf)\nusco&lt;-counties(cb = T, year= 2016)\n\nusco$cofips&lt;-usco$GEOID\n\nsts&lt;-states(cb = T, year = 2016)\n\nsts&lt;-st_boundary(sts)%&gt;%\n  filter(!STATEFP %in% c(\"02\", \"15\", \"60\", \"66\", \"69\", \"72\", \"78\"))%&gt;%\n  st_transform(crs = 2163)\n\nahrf_m&lt;-left_join(usco, ahrf2,\n                    by = \"cofips\")%&gt;%\n  filter(is.na(lbrate1618)==F, \n         !STATEFP %in% c(\"02\", \"15\", \"60\", \"66\", \"69\", \"72\", \"78\"))%&gt;%\n  st_transform(crs = 2163)\n\nglimpse(ahrf_m)\n\nThere are a total of 2,418 observations in the data, because the HRSA restricts some counties with small numbers of births from the data.\nHere is a ggplot() histogram of the low birth weight rate for US counties.\n\nahrf_m%&gt;%\n  ggplot()+\n  geom_histogram(aes(x = lbrate1618))+\n  labs(title = \"Distribution of Low Birth Weight Rates in US Counties\",\n       subtitle = \"2016 - 2018\")+\n       xlab(\"Rate per 1,000 Live Births\")+\n  ylab (\"Frequency\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nHere, we do a basic map of the outcome variable for the continental US, and see the highest rates of low birth weight births in the US occur in the southeastern areas of the country. Notice, I do not color the boundaries of the counties in order to maximize the reader’s ability to see the variation, instead I show lines between states by overlaying the sts layer from above. I also add cartographic options of a scale bar and a north arrow, which I personally believe should be on any map shown to the public.\n\nlibrary(tmap)\n\ntm_shape(ahrf_m)+\n  tm_polygons(col = \"lbrate1618\",\n              border.col = NULL,\n              title=\"Low Birth Weight Rt\",\n              palette=\"Blues\",\n              style=\"quantile\",\n              n=5,\n              showNA=T, colorNA = \"grey50\")+\n   tm_format(format= \"World\",\n             main.title=\"US Low Birth Weight Rate by County\",\n            legend.position =  c(\"left\", \"bottom\"),\n            main.title.position =c(\"center\"))+\n  tm_scale_bar(position = c(.1,0))+\n  tm_compass()+\ntm_shape(sts)+\n  tm_lines( col = \"black\")\n\n\n\n\nWhen doing analysis of place-based data, maps are almost a fundamental aspect of the analysis and often convey much more information about the distribution of the outcome than either distribution plots or summary statistics."
  },
  {
    "objectID": "macro.html#the-linear-model-framework",
    "href": "macro.html#the-linear-model-framework",
    "title": "4  Macrodemographic Analysis of Places",
    "section": "5.5 The linear model framework",
    "text": "5.5 The linear model framework\nProbably the most used and often abused statistical model is the linear regression model, sometimes called the OLS model because it is typically estimated by the method of least squares. I do not plan on spending a lot of real estate in this this book talking about the linear model, mostly because lots of times it doesn’t get us very far, and there are much more thorough books on this subject, one of my personal favorites being John Fox’s text on applied regression (Fox 2016).\nThis model is typically shown as:\n\\[y_i = \\beta_0 +\\sum_k \\beta_k x_{ki} + \\epsilon_i\\]\nwith the \\(\\beta\\)’s being parameters that define the linear relationship between the independent variables \\(x_k\\), and \\(y\\), and \\(\\epsilon_i\\) being the unexplained, or residual portion of \\(y\\) that is included in the model. The model has several assumptions that we need to worry about, first being normality of the residuals, or\n\\[\\epsilon_i \\sim Normal(0, \\sigma_\\epsilon)\\]\nWhere \\(\\sigma_\\epsilon ^2\\) is the residual variance, or mean square error of the model. Under the strict assumptions of the linear model, the Gauss-Markov theorem says that the unbiased and minimum variance estimates of the \\(\\beta\\)’s is:\n\\[\n\\beta_k = \\frac{\\sum (y_i - \\bar{y})(x_i - \\bar{x})}{\\sum x_i - \\bar{x}^2} = \\frac{Cov(x,y)}{Var(x)}\n\\]\nWhich is often shown in the more compact matrix form:\n\\[\\beta_k = (X'X)^{-1} X'Y\\]\nWe could just as directly write the model in it’s distributional form as:\n\\[\ny_i \\sim Normal(\\beta_0 +\\sum_k \\beta_k x_{ki}, \\sigma_\\epsilon)\n\\]\nor even as:\n\\[\ny_i \\sim Normal(X' \\beta, \\sigma_\\epsilon)\n\\]\nWhich I prefer because it sets up the regression equation as the linear predictor, or linear combination (in the linear algebra sense) of the predictors and the model parameters for the mean of the outcome. This term, linear predictor, is a useful one, because as you get more and more accustomed to regression, you will see this same structure in every regression model you ever do. This is the fundamental workhorse of regression, where we get an estimated value for every combination of the observed predictors. Moreover, below when I present the Generalized Linear Model, it will be apparent that the linear predictor can be placed within a number of so-called link functions to ensure that the mean of the outcome agrees with the assumed distribution for the outcome."
  },
  {
    "objectID": "macro.html#estimating-the-linear-model-in-r",
    "href": "macro.html#estimating-the-linear-model-in-r",
    "title": "4  Macrodemographic Analysis of Places",
    "section": "5.6 Estimating the linear model in R",
    "text": "5.6 Estimating the linear model in R\nThe linear model is included in the base R stats package, and is accessed by the lm() function. In the example below, I use data from the U.S. Health Resources and Services Administration Area Health Resource File (AHRF) to estimate a model of the associations between the poverty rate, the rurality of the county and whether the county is a healthcare shortage area. This is a mixture of continuous (or partially continuous) and categorical predictors.\nThe basic lm() model syntax is specified as lm ( y ~ x_1 + x_2, data = dataname) with the ~ operator representing the formula for the model equation, with the outcome on the left and the predictors on the right. You also provide the name of the dataframe which contains the variables specified in the formula in a data= argument. For help on the function and to see the other potential arguments use ?lm.\n\nlm1 &lt;- lm (lbrate1618 ~  fampov14 + rucc + hpsa16, data = ahrf_m)\n\nThis stores the model data and parameter estimates in the object called lm1. You can name the object anything you wish, just try to avoid using other R commands as object names. For instance, I wouldn’t want to call an object mean or sd because those are names of functions. The basic way to see the model results is to use the summary() function on the model fit.\n\nsummary(lm1)\n\n\nCall:\nlm(formula = lbrate1618 ~ fampov14 + rucc + hpsa16, data = ahrf_m)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-90.114 -10.345  -1.229   9.287  77.778 \n\nCoefficients:\n                              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                   62.26920    1.18901  52.371  &lt; 2e-16 ***\nfampov14                       2.25805    0.06846  32.981  &lt; 2e-16 ***\nrucc02                        -1.91854    1.20082  -1.598 0.110249    \nrucc03                        -3.38658    1.25055  -2.708 0.006818 ** \nrucc04                        -6.65483    1.40639  -4.732 2.36e-06 ***\nrucc05                        -5.96283    1.93739  -3.078 0.002110 ** \nrucc06                        -3.94180    1.14317  -3.448 0.000575 ***\nrucc07                        -4.00730    1.28166  -3.127 0.001790 ** \nrucc08                         0.45112    2.11062   0.214 0.830770    \nrucc09                        -3.88365    2.29118  -1.695 0.090202 .  \nhpsa16partial county shortage -0.66219    1.00767  -0.657 0.511148    \nhpsa16whole county shortage    2.36214    1.25335   1.885 0.059601 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.36 on 2312 degrees of freedom\nMultiple R-squared:  0.3697,    Adjusted R-squared:  0.3667 \nF-statistic: 123.3 on 11 and 2312 DF,  p-value: &lt; 2.2e-16\n\n\nWe see that there is a significant positive association between family poverty and the low birth weight rate, we also see that there is a tendency for more rural areas to have lower low birth weight rates than the largest cities (Reference level = rucc01). There is a marginally significant association between a county being a healthcare shortage area and the low birth weight rate. Overall the model is explaining about a third of the variation in the outcome, as seen in the adjusted R-Square value of .3667.\n\n5.6.0.1 A not on interpretation\nIt is important to remember, when describing results for place-based data, to avoid using language centered on individuals. For instance, with reference to the fampov14 variable, we cannot say that families living in poverty are more likely to have a low birth weight birth, instead, we must focus on discussion on places with higher rates of poverty having a higher rate of low birth weight births. Ascribing individual risk from an ecological analysis is an example of the ecological fallacy often seen when doing place-based analysis, and we must be aware of it when framing our questions and describing our results.\nThe gtsummary package (Sjoberg et al. 2021) provides a very nice interface to produce much better looking summaries of models.\n\nlibrary(gtsummary)\n\nlm1%&gt;%\n  tbl_regression(add_estimate_to_reference_rows = TRUE)\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    % Families Below Poverty Level 2014-18\n2.3\n2.1, 2.4\n&lt;0.001\n    rucc\n\n\n\n        01\n0.00\n—\n\n        02\n-1.9\n-4.3, 0.44\n0.11\n        03\n-3.4\n-5.8, -0.93\n0.007\n        04\n-6.7\n-9.4, -3.9\n&lt;0.001\n        05\n-6.0\n-9.8, -2.2\n0.002\n        06\n-3.9\n-6.2, -1.7\n&lt;0.001\n        07\n-4.0\n-6.5, -1.5\n0.002\n        08\n0.45\n-3.7, 4.6\n0.8\n        09\n-3.9\n-8.4, 0.61\n0.090\n    hpsa16\n\n\n\n        no shortage\n0.00\n—\n\n        partial county shortage\n-0.66\n-2.6, 1.3\n0.5\n        whole county shortage\n2.4\n-0.10, 4.8\n0.060\n  \n  \n  \n    \n      1 CI = Confidence Interval"
  },
  {
    "objectID": "macro.html#assumptions-of-the-ols-model",
    "href": "macro.html#assumptions-of-the-ols-model",
    "title": "4  Macrodemographic Analysis of Places",
    "section": "5.7 Assumptions of the OLS model",
    "text": "5.7 Assumptions of the OLS model\nThe linear model has several assumptions that we need to be concerned with, the big four are\n\nNormality of residuals,\nConstant variance in residuals, or homoskedasticity, and\nLinearity of the regression function, and\nIndependence of observations\n\nThe normality assumption is linked to distributional assumption underlying the linear regression model. This states that the model residuals, calculated as \\(e_i = (\\beta_0 +\\sum_k \\beta_k x_{ki}) - y_i\\), or more compactly as \\(e_i =\\hat{y_i} - y_i\\) follow a normal distribution. If the errors around the mean function are not normally distributed, this can be an indicator that the linear model is not appropriate for the outcome under consideration. A commonly used graphical check of this is the quantile-quantile or Q-Q plot, which plots the residuals from a model against the hypothetical quantiles from a normal distribution.\nWe can check these for our model above easily:\n\nhist(residuals(lm1))\n\n\n\nplot(density(resid(lm1)),\n     main = \"Density plot of the residuals\")\ncurve(dnorm(x,0,sd(resid(lm1))),\n       col = \"blue\", lwd =2, add=TRUE)\n\n\n\nplot(lm1, which = 2)\n\n\n\n\nWhile the overall distribution of the residuals is fairly normal based on the histogram and the comparison to the normal density plot, the q-q plot shows that the tails of the distribution are not well modeled by the normal distribution because there are several observations that are too far below or above the theoretical line (dotted line).\nThe homoskedasticity assumption is also tied to the normal distributional assumption of the model, as see above, if we write the model in its distributional form, \\(y_i \\sim Normal(X' \\beta, \\sigma_\\epsilon)\\), the term $ _$ is a single parameter, meaning that we only have one of these in a linear regression model. This parameter determines the spread of the variation around the mean function. Larger values equal more spread around the mean, and smaller values equal less spread. A commonly used graphical procedure to detect lack of homoskedasticity, or heteroskedasticity, is an envelope plot, or a plot of the residuals against the fitted values from the model. Formal tests also exist including the Breusch-Pagan test and the modified version of this test developed by Cook and Weisberg [cite]\nA graphical check of this assumption is easily done from the model fit:\n\nplot(lm1, which = c(1,3))\n\n\n\n\n\n\n\nThese plots show some evidence that the error variances are non-constant. The first plot has the very characteristic “fish” or “cone” shape, where the error variances increase as the fitted values increase. We can also do a formal test using functions from the car package:\n\nlibrary(car)\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nncvTest(lm1)\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 469.4844, Df = 1, p = &lt; 2.22e-16\n\n\nWhich again shows more statistical evidence of the non-constant variance in residuals.\n\n5.7.0.1 Correction for non-constant variance\nThe assumption of homoskedasticity is important for two reasons, first is related to prediction from the model, but the second is related to the test statistics derived from the model. In order to test our hypothesis that \\(x\\) is related to \\(y\\), we form the ratio of the \\(\\beta_1\\) parameter to its error, this is typically either a \\(z\\), \\(t\\) or Wald \\(\\chi^2\\) statistic, depending on which procedure you’re using.\n\\[t = \\frac{\\hat{\\beta_1}}{se(\\beta_1))}\\] The term \\(se(\\beta_1)\\) is the estimated standard error of the parameter, and is calculated using the ratio of the residual standard deviation and the square root of the sums of squares of the \\(x\\):\n\\[se(\\beta_1) = \\frac{\\sigma_{\\epsilon}}{\\sqrt{\\sum(x - \\bar{x})^2}}\\]\nor in the matrix terms:\n\\[Var(\\beta) = \\sigma_{\\epsilon}(X'X)^{-1}\\]\nif the term \\(\\sigma_{\\epsilon}\\) is not constant then the standard error of each parameter in the model is incorrect. Corrections for heteroskedasticity are commonplace in the social sciences, and are usually attributed to White (1980) and MacKinnon and White (1985) with many additions since the original publication, notably Long and Ervin (2000). These corrections use the empirically observed error terms and avoid the assumption of common variance in all residuals.\nThe coeftest() function in the lmtest package is one option to correct for heteroskedasticity in regression models. It allows for various correction types, with the “HC3” type (Long and Ervin 2000) being the default for linear models. Below, I show the default tests assuming constant variance and the corrected tests.\n\nlibrary(sandwich)\nlibrary(lmtest)\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\ncoeftest(lm1)\n\n\nt test of coefficients:\n\n                               Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept)                   62.269198   1.189010 52.3706 &lt; 2.2e-16 ***\nfampov14                       2.258054   0.068465 32.9813 &lt; 2.2e-16 ***\nrucc02                        -1.918536   1.200819 -1.5977 0.1102488    \nrucc03                        -3.386577   1.250553 -2.7081 0.0068176 ** \nrucc04                        -6.654825   1.406391 -4.7318 2.359e-06 ***\nrucc05                        -5.962826   1.937392 -3.0778 0.0021101 ** \nrucc06                        -3.941799   1.143175 -3.4481 0.0005746 ***\nrucc07                        -4.007304   1.281664 -3.1266 0.0017901 ** \nrucc08                         0.451120   2.110622  0.2137 0.8307703    \nrucc09                        -3.883651   2.291182 -1.6950 0.0902020 .  \nhpsa16partial county shortage -0.662194   1.007671 -0.6572 0.5111478    \nhpsa16whole county shortage    2.362138   1.253348  1.8847 0.0596007 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncoeftest(lm1, vcov  = vcovHC(lm1, type = \"HC3\"))\n\n\nt test of coefficients:\n\n                              Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept)                   62.26920    1.14941 54.1749 &lt; 2.2e-16 ***\nfampov14                       2.25805    0.11349 19.8966 &lt; 2.2e-16 ***\nrucc02                        -1.91854    0.97531 -1.9671 0.0492896 *  \nrucc03                        -3.38658    1.06364 -3.1839 0.0014722 ** \nrucc04                        -6.65483    1.20922 -5.5034 4.137e-08 ***\nrucc05                        -5.96283    1.95622 -3.0481 0.0023288 ** \nrucc06                        -3.94180    1.09704 -3.5931 0.0003336 ***\nrucc07                        -4.00730    1.30130 -3.0795 0.0020981 ** \nrucc08                         0.45112    2.42803  0.1858 0.8526203    \nrucc09                        -3.88365    3.29696 -1.1779 0.2389381    \nhpsa16partial county shortage -0.66219    0.85442 -0.7750 0.4384068    \nhpsa16whole county shortage    2.36214    1.26921  1.8611 0.0628553 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe take away in this example is that non-constant variance can affect the standard errors of the model parameter estimates, and in turn affect the test statistics that we base all of our hypothesis tests on. In the particulars of this model there is not a lot of change, the rucc02 parameter is barley significant once using the corrected standard errors, otherwise we see a very similar pattern in terms of what is significant in the model.\n\n\n5.7.0.2 Clustered standard errors\nAnother commonly used correction in regression modeling is the clustered standard error. These are commonplace and almost the default in the Stata programming environment, and are widely used in the field of economics. Clustering of standard errors attempts to correct for clustering in the residuals from the regression model. Clustering can happen for a wide variety of reasons, and as we saw in the previous chapter on survey data analysis, is often an artifact of how the data are collected. With place-based data, we may have clustering because the places are close to each other, or because the share some other characteristic that we have not measured in our regression model. In the case of our regression model, and in our descriptive analysis of our outcome, the map shown prior may indicate some form of spatial correlation in the outcome. While there are models to deal with such non-independence in place-based data, they are not a subject I will touch on here. Instead, we may use the state which each county is in as a proxy for the spatial clustering in the outcome, as one example of potential of a clustering term.\n\ncoeftest(lm1,\n         vcov  = vcovCL(lm1, cluster = ahrf_m$state))\n\n\nt test of coefficients:\n\n                              Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept)                   62.26920    2.43737 25.5477 &lt; 2.2e-16 ***\nfampov14                       2.25805    0.28830  7.8322 7.243e-15 ***\nrucc02                        -1.91854    1.15345 -1.6633  0.096387 .  \nrucc03                        -3.38658    1.42381 -2.3785  0.017462 *  \nrucc04                        -6.65483    1.35050 -4.9277 8.912e-07 ***\nrucc05                        -5.96283    2.60314 -2.2906  0.022075 *  \nrucc06                        -3.94180    1.28117 -3.0767  0.002118 ** \nrucc07                        -4.00730    2.11299 -1.8965  0.058017 .  \nrucc08                         0.45112    2.59621  0.1738  0.862069    \nrucc09                        -3.88365    4.73238 -0.8207  0.411927    \nhpsa16partial county shortage -0.66219    1.18695 -0.5579  0.576971    \nhpsa16whole county shortage    2.36214    1.80407  1.3093  0.190549    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn this case, the rucc02 and rucc07 terms become marginally significant and the healthcare shortage areas lose their marginal significance.\n\n\n5.7.0.3 Weighted Least Squares\nAnother method of dealing with non-constant error variance is the method of weighted least squares. This method modifies the model somewhat to be:\n\\[y \\sim Normal(\\beta_0 +\\sum_k \\beta_k x_{ki}, \\sigma_{\\epsilon_i} )\\]\nWhere the term \\(\\sigma_{\\epsilon_i}\\) represents the different variances for each observation. The weights in the model are often variables that represent an underlying factor that affects the variance in the estimates. In demography this is often the population size of a place, as places with smaller population sizes often have more volatility to their rate estimates. This approach has been used in the spatial demographic modeling of county mortality rates in the United States by several authors (McLaughlin et al. 2007; Sparks and Sparks 2010).\n\nlm2 &lt;- lm(lbrate1618 ~  fampov14 + rucc + hpsa16,\n          data = ahrf_m,\n          weights = popn)\nsummary(lm2)\n\n\nCall:\nlm(formula = lbrate1618 ~ fampov14 + rucc + hpsa16, data = ahrf_m, \n    weights = popn)\n\nWeighted Residuals:\n   Min     1Q Median     3Q    Max \n-42957  -2122   -106   2333  27618 \n\nCoefficients:\n                              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                   65.54503    0.98536  66.519  &lt; 2e-16 ***\nfampov14                       1.83332    0.06338  28.928  &lt; 2e-16 ***\nrucc02                        -0.63464    0.66623  -0.953 0.340901    \nrucc03                        -1.67371    0.93793  -1.784 0.074479 .  \nrucc04                        -4.38387    1.31520  -3.333 0.000872 ***\nrucc05                        -3.31398    2.16308  -1.532 0.125642    \nrucc06                        -1.87074    1.35213  -1.384 0.166630    \nrucc07                        -2.12596    1.80385  -1.179 0.238693    \nrucc08                         3.92060    4.33118   0.905 0.365452    \nrucc09                        -0.21442    4.93030  -0.043 0.965315    \nhpsa16partial county shortage -1.85070    0.93518  -1.979 0.047938 *  \nhpsa16whole county shortage    0.21786    1.65417   0.132 0.895229    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4620 on 2312 degrees of freedom\nMultiple R-squared:  0.2832,    Adjusted R-squared:  0.2798 \nF-statistic: 83.05 on 11 and 2312 DF,  p-value: &lt; 2.2e-16\n\n\nWe see that by including the population size weights in the model, most of the parameters are no longer significant in the analysis, but the weights have not dealt with the non-constant variance issue totally:\n\nncvTest(lm2)\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 84.49228, Df = 1, p = &lt; 2.22e-16\n\n\nBut the overall size of the non-constant variance test is much lower than it was for the original model.\n\n\n5.7.1 Linearity assumption\nThe linearity assumption of the model assumes that the true underlying relationship in the data can be modeled using a linear combination of the predictors and the parameters. I think a lot of people think this means that you cannot include square or polynomial terms in a regression model, but that is not the case. The assumption is concerned with the linearity of the parameters, not the predictor variables themselves. For example the standard linear model with one predictor, x is written:\n\\[y = \\beta_0 + \\beta_1 x_1 + \\epsilon\\]\nWhich is clearly the equation for a straight line with y-intercept \\(\\beta_0\\) and slope \\(\\beta_1\\). We also see that the parameters combine in a linear (additive) fashion. This is the assumption of the model, and can also be seen when expressing this equation using vector notation\n\\[y = x' \\beta\\]\nBecause the term \\(x' \\beta\\) is the inner product of the \\(\\beta\\) parameters and the information from \\(x\\). If we include the square of \\(x\\) in the model:\n\\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_1^2 + \\epsilon\\] The same inner, additive product of the \\(\\beta\\)s and the \\(x\\)s is still the same. If we constructed a model that was non-linear function of the \\(\\beta\\)s, such as:\n\\[y = \\beta_0 + \\beta_1 x_1 *  e^{\\beta_2 x_1^2} + \\epsilon\\] The the model would no longer be linear in the parameters because we introduce a nonlinearity by exponentiating the \\(\\beta_2\\) parameter inside the mean function (not that this is a real model, just as an example).\nWhen this actually comes into our experience is when our data are actually generated by a nonlinear process, such as a time series with seasonality included, which may oscillate between seasons (such as temperature, or rainfall), for instance a situation such as this arises:\nWhere the variable y is actually generated from a cosine curve with noise. Since the linear regression of y on x forces the y to change linearly with x, the model is absolutely unable to recover the underlying pattern in the data.\n\n\nA note on splines\nIf the data clearly do not display a linear trend, I personally automatically look to splines as a means to model the non-linearity in relationship. Splines are a method of constructing a model based on connecting either linear or non-linear functions across a series of breaks or knots along the data in which the form of the function changes.\nMathematically, knots can be written as:\n$$ Y(x) =\n\\[\\begin{Bmatrix}\nF_1(x) \\text {  for  } x\\in [x_1, x_2]\\\\\nF_2(x) \\text {  for  } x\\in [x_2, x_3]\\\\\n\\cdots \\\\\nF_{k}(x) \\text {  for  } x\\in [x_{k-1}, x_k]\\\\\n\n\\end{Bmatrix}\\]\n$$\nWhere each of the \\(F_k (x)\\) functions imply a different form in the interval between \\(x\\in [x_{k-1}, x_k]\\), where the \\(k\\) breaks are at a given knot in the data. Most splines are nonlinear functions, usually cubic polynomials, and the spline model combines a series of these polynomials to model nonlinearities in the relationship between predictors and outcomes. A relatively recent invention, Generalized Additive Models or GAMs are a way to model an outcome with both linear and non-linear terms together. The GAM model forms the linear predictor of a model can be constructed as:\n\\[E(y)= \\beta_0 + f(x_1) + \\beta_1 x_2\\] where the \\(f(x_1)\\) term is a regression spline of one of the variables. The models can be a mixture of linear and smooth terms. Here is an example of using a B-spline within the lm() model to fit a smooth regression function to the messy nonlinear model above.\n\nlibrary(splines)\n\nsm&lt;- lm(y2~bs(t, df = 5))\n\nplot(t, y2, t=\"p\",\n     ylim=range(y2) * c(1, 1.2),\n     main=\"Nice Spline Model,\\nFit to Nonlinear Outcome\",\n     ylab =\"y\",\n     xlab= \"x\")\n\nt&lt;-seq(from = min(t),to = max(t), length.out = length(y2))\n\nlines(t, predict( lm (y2 ~ bs(t, df = 5)),\n                 data.frame(t = t), lwd = 1.5),\n      col=3)\n\n\n\n\nI really think splines, and GAMs are an excellent addition to the modeling world and have started teaching them in my own methods courses. In the world of demography, especially, with age affecting everything, there is no need to constantly assume relationships are purely linear, and splines offer an excellent method to explore such relationships.\n\nIf these assumptions are violated, then several things can happen. At best, our interpretation of the model coefficients could be wrong, meaning that, as seen above, our model would suggest one relationship from the data, but in fact because the model was misspecified, the relationship we discover is incorrect. Our poor linear model would predict a decline in the outcome, while the outcome itself is perfectly stationary, as shown by the dashed line.\nIn a more social science sensibility, the interpretation of the beta coefficients for the effect of \\(x\\) on \\(y\\) in this case will provide us a false conclusion of the relationship in the data. This is a really dangerous outcome for us in social science, because that’s why we’re doing statistics in the first place, to answer questions.\nThe normality assumption above primarily affect predictions from the model, which, since the normal distribution is bound on \\(-\\infty\\) to \\(\\infty\\), can easily lead to a prediction outside of the realm of possibility, say for a dichotomous outcome, or a count, neither of which can have predicted values beyond 0 and 1, or less than 0.\n\n\n5.7.2 Generalized Least Squares\nOnce our data start to violate the assumptions of the linear model, the model becomes less and less useful. For instance, why make all of the strict assumptions about homoskedastic (I am contractually required by the statistics union to say this at least once) variances in the model residuals in order to use OLS, when you can use it’s friend and brother, Generalized Least Squares (GLS, of course), which allows you to make useful and pragmatic changes to the OLS model structure to accommodate all of the fun and annoying things about real data, but still use the normal distribution to model our outcomes[^macrodem-9].\nGeneralized Least Squares adds a lot more flexibility to modeling normally distributed outcomes, basically by allowing us to modify the fundamental equations above to accommodate unequal variances, or the use of covariates or stratification variables on variances. Another way to write the OLS model above would be:\n\\(\\epsilon_i \\sim Normal(X'\\beta, I\\sigma_\\epsilon)\\) Where \\(I\\) is the identity matrix, which implies that for each observation, the variances in the residuals are all the same:\n\\[\n\\sigma_{\\epsilon}  = I * \\sigma_{\\epsilon} = \\begin{Bmatrix}\n1& 0& 0 \\\\\n0& 1& 0 \\\\\n0& 0& 1\\\\\n\\end{Bmatrix} *\\sigma_{\\epsilon} = \\begin{Bmatrix}\n\\sigma_{\\epsilon}& 0& 0 \\\\\n0& \\sigma_{\\epsilon}& 0 \\\\\n0& 0 & \\sigma_{\\epsilon} \\\\\n\\end{Bmatrix}\n\\]\nWhich shows the common variance for the three residuals. GLS allows us to relax this constant variance assumption, by at the minimum allowing the variances to be a function of a weighting variable (which produces Weighted Least Squares), or some covariate. In the most basic presentation of this principle, this makes the residuals have some other, non-constant form of:\n\\(\\sigma_{\\epsilon} = \\Omega\\) which in turn modifies the estimation equation for the \\(\\beta\\)’s to:\n\\(\\beta_{k_{GLS}} = (X' \\Omega^{-1} X)^{-1} X' \\Omega^{-1} Y\\) Applications of such models are more commonly seen in time series modeling and longitudinal analysis, where the residuals of the model often have an autoregressive form to allow individuals to be correlated with themselves over time, but when talking about place-based demography, more modifications of the model have been derived that allow for addressing another key assumption of independence among observations and nonconstant variances. This in fact is the realm of an entire field of econometrics, often called spatial econometrics (Anselin 1988; Chi and Zhu 2020; Elhorst 2014; LeSage and Pace 2009).\nThe gls() function in the nlme library is very flexible at modeling heteroskedasticity using several types of variance functions. The general principle of the gls model in terms of modeling heteroskedasticity is that the OLS model residual variance \\(\\sigma_{\\epsilon}\\) is now not a constant, but a function of covariates or different variances based on a stratification variable. This generates a model for the residual variances of the form:\n\\[Var(\\epsilon_i |\\beta) = \\sigma^2 g\\]\nWhere the \\(g\\) function can be the effect of a covariate, which would allow the variance to increase or decrease as function of that variable, or a set of strata, where the variance can be different in two or more groups. Below, I first show the model above fit using gls() instead of lm(), then extend the model to include heteroskedasticity based on the population size of the county, and state-specific variance terms (Pinheiro and Bates 2000).\n\nlibrary(nlme)\n\n\nAttaching package: 'nlme'\n\n\nThe following object is masked from 'package:dplyr':\n\n    collapse\n\nlm1g&lt;- gls(lbrate1618 ~  fampov14 + rucc + hpsa16,\n          data = ahrf_m)\n\nsummary(lm1g)\n\nGeneralized least squares fit by REML\n  Model: lbrate1618 ~ fampov14 + rucc + hpsa16 \n  Data: ahrf_m \n       AIC      BIC    logLik\n  19580.52 19655.22 -9777.261\n\nCoefficients:\n                                 Value Std.Error  t-value p-value\n(Intercept)                   62.26920 1.1890098 52.37063  0.0000\nfampov14                       2.25805 0.0684647 32.98129  0.0000\nrucc02                        -1.91854 1.2008189 -1.59769  0.1102\nrucc03                        -3.38658 1.2505531 -2.70806  0.0068\nrucc04                        -6.65483 1.4063909 -4.73185  0.0000\nrucc05                        -5.96283 1.9373921 -3.07776  0.0021\nrucc06                        -3.94180 1.1431747 -3.44812  0.0006\nrucc07                        -4.00730 1.2816639 -3.12664  0.0018\nrucc08                         0.45112 2.1106217  0.21374  0.8308\nrucc09                        -3.88365 2.2911822 -1.69504  0.0902\nhpsa16partial county shortage -0.66219 1.0076706 -0.65715  0.5111\nhpsa16whole county shortage    2.36214 1.2533476  1.88466  0.0596\n\n Correlation: \n                              (Intr) fmpv14 rucc02 rucc03 rucc04 rucc05 rucc06\nfampov14                      -0.381                                          \nrucc02                        -0.364 -0.121                                   \nrucc03                        -0.342 -0.114  0.462                            \nrucc04                        -0.298 -0.160  0.416  0.399                     \nrucc05                        -0.207 -0.144  0.305  0.293  0.269              \nrucc06                        -0.317 -0.245  0.521  0.501  0.453  0.336       \nrucc07                        -0.281 -0.242  0.467  0.449  0.410  0.304  0.525\nrucc08                        -0.140 -0.163  0.288  0.278  0.248  0.186  0.344\nrucc09                        -0.101 -0.226  0.274  0.265  0.241  0.182  0.334\nhpsa16partial county shortage -0.560 -0.131 -0.068 -0.078 -0.043 -0.024 -0.070\nhpsa16whole county shortage   -0.390 -0.235 -0.035 -0.045  0.017  0.015 -0.105\n                              rucc07 rucc08 rucc09 hps16pcs\nfampov14                                                   \nrucc02                                                     \nrucc03                                                     \nrucc04                                                     \nrucc05                                                     \nrucc06                                                     \nrucc07                                                     \nrucc08                         0.298                       \nrucc09                         0.292  0.204                \nhpsa16partial county shortage -0.058 -0.051 -0.037         \nhpsa16whole county shortage   -0.035 -0.132 -0.097  0.693  \n\nStandardized residuals:\n        Min          Q1         Med          Q3         Max \n-5.50851038 -0.63237431 -0.07515361  0.56768444  4.75446234 \n\nResidual standard error: 16.35905 \nDegrees of freedom: 2324 total; 2312 residual\n\n\nThese are the same results, in terms of the regression coefficients as returned by lm(). If we include a varFixed() term, we can regress the residual variance term on a covariate, in this case I select the population size.\n\nlm2g&lt;- gls(lbrate1618 ~  fampov14 + rucc + hpsa16,\n          data = ahrf_m, \n          weights = varFixed(~popn) )\nsummary(lm2g)\n\nGeneralized least squares fit by REML\n  Model: lbrate1618 ~ fampov14 + rucc + hpsa16 \n  Data: ahrf_m \n       AIC      BIC    logLik\n  21560.43 21635.12 -10767.21\n\nVariance function:\n Structure: fixed weights\n Formula: ~popn \n\nCoefficients:\n                                 Value Std.Error  t-value p-value\n(Intercept)                   64.51684 1.7992819 35.85700  0.0000\nfampov14                       2.02451 0.0672245 30.11574  0.0000\nrucc02                        -3.65253 2.0140861 -1.81349  0.0699\nrucc03                        -3.74951 1.9836516 -1.89021  0.0589\nrucc04                        -6.48571 2.3336913 -2.77916  0.0055\nrucc05                        -6.03115 2.9497652 -2.04462  0.0410\nrucc06                        -2.53166 1.6616043 -1.52362  0.1277\nrucc07                        -2.30731 1.7300102 -1.33369  0.1824\nrucc08                         0.54783 2.1293518  0.25727  0.7970\nrucc09                        -2.85405 2.1820264 -1.30798  0.1910\nhpsa16partial county shortage  0.18574 1.3009964  0.14277  0.8865\nhpsa16whole county shortage    5.22180 1.4198726  3.67766  0.0002\n\n Correlation: \n                              (Intr) fmpv14 rucc02 rucc03 rucc04 rucc05 rucc06\nfampov14                      -0.269                                          \nrucc02                        -0.537 -0.065                                   \nrucc03                        -0.550 -0.045  0.536                            \nrucc04                        -0.466 -0.088  0.463  0.466                     \nrucc05                        -0.363 -0.085  0.367  0.370  0.328              \nrucc06                        -0.603 -0.163  0.649  0.655  0.568  0.452       \nrucc07                        -0.591 -0.169  0.627  0.631  0.555  0.441  0.776\nrucc08                        -0.431 -0.174  0.508  0.513  0.439  0.350  0.637\nrucc09                        -0.409 -0.234  0.500  0.503  0.435  0.348  0.632\nhpsa16partial county shortage -0.464 -0.092 -0.062 -0.062 -0.063 -0.048 -0.109\nhpsa16whole county shortage   -0.429 -0.215 -0.016 -0.027  0.038  0.031 -0.050\n                              rucc07 rucc08 rucc09 hps16pcs\nfampov14                                                   \nrucc02                                                     \nrucc03                                                     \nrucc04                                                     \nrucc05                                                     \nrucc06                                                     \nrucc07                                                     \nrucc08                         0.609                       \nrucc09                         0.605  0.512                \nhpsa16partial county shortage -0.100 -0.101 -0.087         \nhpsa16whole county shortage    0.001 -0.099 -0.070  0.767  \n\nStandardized residuals:\n        Min          Q1         Med          Q3         Max \n-7.55351050 -0.48060084 -0.06047952  0.31143188  8.68535299 \n\nResidual standard error: 0.1103656 \nDegrees of freedom: 2324 total; 2312 residual\n\n\nThis model shows some very different effects after controlling for non-constant variance. For instance, the whole county shortage effect is now much more significant in the model, compared to the lm1 model.\nThe final model includes a separate variance for each state using the varIdent() term.\n\nlm3&lt;- gls(lbrate1618 ~  fampov14 + rucc + hpsa16,\n          data = ahrf_m, \n          weights = varIdent(form = ~1|factor(state) ) )\nsummary(lm3)\n\nGeneralized least squares fit by REML\n  Model: lbrate1618 ~ fampov14 + rucc + hpsa16 \n  Data: ahrf_m \n       AIC      BIC    logLik\n  19232.57 19583.07 -9555.287\n\nVariance function:\n Structure: Different standard deviations per stratum\n Formula: ~1 | factor(state) \n Parameter estimates:\n       21        01        04        05        06        08        09        11 \n1.0000000 1.6653572 1.4666877 1.3444082 1.4169906 2.3439093 0.4153761 0.9349384 \n       12        13        16        17        18        48        49        50 \n0.8691605 1.4618211 1.2492319 0.8531463 0.7272325 1.0046612 0.9955712 0.6782617 \n       22        23        24        25        26        27        28        29 \n1.5206083 0.5562842 1.0764207 0.3632039 0.8797361 0.8694446 2.1388815 1.0671070 \n       30        33        34        35        36        37        38        39 \n0.8593438 0.3518410 0.5857512 1.6675040 0.8834383 1.2756652 1.3451314 0.7590721 \n       40        41        42        44        45        46        47        51 \n0.9096844 0.9789355 0.7160118 0.7725377 1.4955639 2.2706999 0.9232718 1.4150226 \n       53        54        55        10        32        19        20        56 \n1.1651279 1.4044153 0.8102840 0.8493449 1.3739156 0.8028595 0.9380637 1.5713111 \n       31 \n1.1601422 \n\nCoefficients:\n                                 Value Std.Error  t-value p-value\n(Intercept)                   61.00371 0.9636746 63.30323  0.0000\nfampov14                       2.20339 0.0626079 35.19354  0.0000\nrucc02                        -1.55250 0.9300585 -1.66925  0.0952\nrucc03                        -2.67599 0.9989933 -2.67869  0.0074\nrucc04                        -6.75071 1.0541928 -6.40368  0.0000\nrucc05                        -7.07431 1.6290634 -4.34256  0.0000\nrucc06                        -5.33031 0.8964666 -5.94591  0.0000\nrucc07                        -6.20026 1.0294913 -6.02264  0.0000\nrucc08                        -0.38337 1.8548174 -0.20669  0.8363\nrucc09                        -3.80880 2.0564547 -1.85212  0.0641\nhpsa16partial county shortage -0.40513 0.8049182 -0.50332  0.6148\nhpsa16whole county shortage    0.69954 1.0571871  0.66170  0.5082\n\n Correlation: \n                              (Intr) fmpv14 rucc02 rucc03 rucc04 rucc05 rucc06\nfampov14                      -0.425                                          \nrucc02                        -0.330 -0.118                                   \nrucc03                        -0.300 -0.112  0.415                            \nrucc04                        -0.281 -0.137  0.396  0.369                     \nrucc05                        -0.155 -0.132  0.262  0.244  0.236              \nrucc06                        -0.275 -0.238  0.476  0.446  0.423  0.284       \nrucc07                        -0.232 -0.230  0.417  0.390  0.374  0.253  0.467\nrucc08                        -0.105 -0.142  0.235  0.221  0.207  0.140  0.280\nrucc09                        -0.058 -0.215  0.222  0.209  0.199  0.138  0.272\nhpsa16partial county shortage -0.592 -0.112 -0.037 -0.042 -0.028 -0.026 -0.037\nhpsa16whole county shortage   -0.390 -0.189 -0.015 -0.026  0.019  0.016 -0.106\n                              rucc07 rucc08 rucc09 hps16pcs\nfampov14                                                   \nrucc02                                                     \nrucc03                                                     \nrucc04                                                     \nrucc05                                                     \nrucc06                                                     \nrucc07                                                     \nrucc08                         0.236                       \nrucc09                         0.233  0.147                \nhpsa16partial county shortage -0.036 -0.032 -0.019         \nhpsa16whole county shortage   -0.031 -0.110 -0.076  0.654  \n\nStandardized residuals:\n        Min          Q1         Med          Q3         Max \n-3.71110304 -0.57404142  0.07995833  0.73280651  3.80484317 \n\nResidual standard error: 13.75474 \nDegrees of freedom: 2324 total; 2312 residual\n\n\nWhich also provides similar results to lm1. So, which model is better for this particular outcome? One way to examine relative model fit is to compare the Akaike Information Criteria (AIC) for the three models. The AIC consists of two components, one showing overall model deviance, or residual variance and a penalty term for the number of parameters in a model. A general form of it is:\n\\[\nAIC = -2LL(\\theta) + 2k\n\\] Where the term \\(-2LL(\\theta)\\) is the model -2 Log likelihood, or deviance, and \\(2k\\) is a penalty term with \\(k\\) being the number of parameters.\nSince the three models lm1g, lm2 and lm3 are all fit by gls(), and fit to the same dataset, we can compare them. We can even add another model with a smooth spline effect of poverty:\n\nlm3s&lt;- gls(lbrate1618 ~  bs(fampov14, df=4) + rucc + hpsa16,\n          data = ahrf_m, \n          weights = varIdent(form = ~1|factor(state) ) )\nsummary(lm3s)\n\nGeneralized least squares fit by REML\n  Model: lbrate1618 ~ bs(fampov14, df = 4) + rucc + hpsa16 \n  Data: ahrf_m \n       AIC      BIC    logLik\n  19166.97 19534.62 -9519.486\n\nVariance function:\n Structure: Different standard deviations per stratum\n Formula: ~1 | factor(state) \n Parameter estimates:\n       21        01        04        05        06        08        09        11 \n1.0000000 1.8015440 1.5283795 1.4399605 1.5393732 2.5102463 0.4584789 0.8961499 \n       12        13        16        17        18        48        49        50 \n0.9169369 1.5786894 1.2845194 0.9010857 0.7815679 1.0169676 1.0666628 0.7347100 \n       22        23        24        25        26        27        28        29 \n1.7020601 0.6180479 1.1587213 0.4065278 0.9556412 0.9113779 2.4411415 1.1501547 \n       30        33        34        35        36        37        38        39 \n0.9098260 0.3629158 0.6330773 1.6758907 0.9583268 1.3320870 1.3647418 0.8178477 \n       40        41        42        44        45        46        47        51 \n0.9867867 1.0609522 0.7586694 0.8150356 1.5564278 1.0001279 0.9815430 1.5006645 \n       53        54        55        10        32        19        20        56 \n1.2497825 1.4843275 0.8554764 0.9154187 1.4561259 0.8426759 1.0163896 1.7112344 \n       31 \n1.2457365 \n\nCoefficients:\n                                 Value Std.Error   t-value p-value\n(Intercept)                   65.28605  2.227553 29.308421  0.0000\nbs(fampov14, df = 4)1          4.58369  3.031480  1.512031  0.1307\nbs(fampov14, df = 4)2         50.98375  3.137696 16.248784  0.0000\nbs(fampov14, df = 4)3         72.74204  8.455071  8.603362  0.0000\nbs(fampov14, df = 4)4         15.96517 11.310702  1.411510  0.1582\nrucc02                        -1.78937  0.947673 -1.888176  0.0591\nrucc03                        -2.99112  1.007407 -2.969130  0.0030\nrucc04                        -6.96833  1.065192 -6.541851  0.0000\nrucc05                        -7.95180  1.605224 -4.953700  0.0000\nrucc06                        -5.72170  0.906461 -6.312131  0.0000\nrucc07                        -6.38566  1.028234 -6.210321  0.0000\nrucc08                        -0.84020  1.849541 -0.454273  0.6497\nrucc09                        -3.84206  2.006058 -1.915231  0.0556\nhpsa16partial county shortage -0.54729  0.798950 -0.685016  0.4934\nhpsa16whole county shortage    0.60030  1.043145  0.575469  0.5650\n\n Correlation: \n                              (Intr) b(14,d=4)1 b(14,d=4)2 b(14,d=4)3\nbs(fampov14, df = 4)1         -0.907                                 \nbs(fampov14, df = 4)2         -0.157 -0.005                          \nbs(fampov14, df = 4)3         -0.527  0.664     -0.533               \nbs(fampov14, df = 4)4         -0.089  0.053      0.318     -0.304    \nrucc02                         0.011 -0.166     -0.133     -0.073    \nrucc03                        -0.008 -0.129     -0.146     -0.034    \nrucc04                        -0.002 -0.132     -0.134     -0.060    \nrucc05                         0.006 -0.088     -0.128     -0.034    \nrucc06                         0.001 -0.142     -0.190     -0.081    \nrucc07                        -0.011 -0.120     -0.139     -0.100    \nrucc08                        -0.015 -0.040     -0.115     -0.019    \nrucc09                         0.006 -0.063     -0.076     -0.095    \nhpsa16partial county shortage -0.166 -0.091     -0.123     -0.029    \nhpsa16whole county shortage   -0.143 -0.042     -0.096     -0.055    \n                              b(14,d=4)4 rucc02 rucc03 rucc04 rucc05 rucc06\nbs(fampov14, df = 4)1                                                      \nbs(fampov14, df = 4)2                                                      \nbs(fampov14, df = 4)3                                                      \nbs(fampov14, df = 4)4                                                      \nrucc02                        -0.036                                       \nrucc03                        -0.038      0.435                            \nrucc04                        -0.035      0.414  0.388                     \nrucc05                        -0.032      0.282  0.265  0.255              \nrucc06                        -0.043      0.495  0.466  0.441  0.304       \nrucc07                        -0.035      0.433  0.406  0.390  0.269  0.482\nrucc08                        -0.019      0.243  0.231  0.216  0.150  0.290\nrucc09                        -0.114      0.231  0.216  0.207  0.146  0.283\nhpsa16partial county shortage -0.031     -0.012 -0.023 -0.009 -0.003 -0.013\nhpsa16whole county shortage   -0.054     -0.006 -0.019  0.026  0.027 -0.096\n                              rucc07 rucc08 rucc09 hps16pcs\nbs(fampov14, df = 4)1                                      \nbs(fampov14, df = 4)2                                      \nbs(fampov14, df = 4)3                                      \nbs(fampov14, df = 4)4                                      \nrucc02                                                     \nrucc03                                                     \nrucc04                                                     \nrucc05                                                     \nrucc06                                                     \nrucc07                                                     \nrucc08                         0.243                       \nrucc09                         0.245  0.152                \nhpsa16partial county shortage -0.015 -0.022 -0.015         \nhpsa16whole county shortage   -0.018 -0.107 -0.073  0.650  \n\nStandardized residuals:\n        Min          Q1         Med          Q3         Max \n-3.66477973 -0.58141552  0.08014607  0.72668723  3.87024262 \n\nResidual standard error: 12.88614 \nDegrees of freedom: 2324 total; 2309 residual\n\n\n\nAIC( lm1g,  lm2g, lm3, lm3s)\n\nWarning in AIC.default(lm1g, lm2g, lm3, lm3s): models are not all fitted to the\nsame number of observations\n\n\n     df      AIC\nlm1g 13 19580.52\nlm2g 13 21560.43\nlm3  61 19232.57\nlm3s 64 19166.97\n\n\nThe AIC() function calculates AIC for each model, and we see that lm3s has the lowest of the three, suggesting that the non-constant variance across states is a better representation than the effects of population size on the residual variance, and that the nonlinear effect of poverty is also present in this case.\n\n5.7.2.1 Further model comparisons\nR has a general method of comparing models using \\(F\\) tests or Likelihood Ratio Tests. These are often used when comparing nested models, where one model is a simplified version of another. We have such models above in our gls() models. The lm1g model is a simplified version of the lm3 model because it doesn’t contain the extra parameters modeling the unequal variances. The anova() method can compare the models to see if the extra parameters are explaining the model deviance (or variation) better.\n\nanova(lm1g, lm3)\n\n     Model df      AIC      BIC    logLik   Test  L.Ratio p-value\nlm1g     1 13 19580.52 19655.22 -9777.261                        \nlm3      2 61 19232.57 19583.07 -9555.287 1 vs 2 443.9476  &lt;.0001\n\n\nHere the likelihood ratio test L.Ratio shows a significant decrease in the logLik in model lm3, suggesting that it better explains the data than lm1g does. The likelihood ratio is calculated as \\(2*LL_2 - LL_1\\), or in the case above 2*(-9555.287--9777.261). The anova method is very useful for comparing alternative models and can be used on most of the models shown in this book.\n\n\n\n5.7.3 Predictions and marginal means\nWorking with fitted values from a regression is one of the least taught aspects of modeling. Since the models are fundamentally doing very fancy averaging, the estimated, or fitted values from the model can often be very useful to us as we try to explain the results of our models. Remember, the fitted values of the model are just:\n\\[\n\\hat{y}_i = \\sum \\hat{\\beta_k} x_{ki}\n\\]\nor the linear combination of the estimated model parameters and the \\(k\\) observed predictors, \\(x_{ki}\\). Most models in R have a fitted() method to extract the fitted values of the outcome variable for each observation.\nFor example, here are the first six fitted values from the original lm1 OLS model, the gls() heteroskedastic model lm3 model, and the first six values of the outcome:\n\nlibrary(gt)\n\nfits&lt;- data.frame(\n  name = head( ahrf_m$NAME ),\n  lm1 = head( fitted ( lm1 )),\n  lm3 = head( fitted( lm3 )), \n  observed = head( ahrf_m$lbrate1618 )\n  )\n\nfits%&gt;%\n  gt()\n\n\n\n\n\n  \n    \n    \n      name\n      lm1\n      lm3\n      observed\n    \n  \n  \n    Anderson\n83.16599\n79.91074\n87.78626\n    Bullitt\n79.44563\n78.00540\n84.47205\n    Clark\n85.43028\n84.16478\n89.20188\n    Daviess\n86.20507\n84.98880\n81.26411\n    Hardin\n82.15580\n81.27857\n78.47222\n    Harrison\n83.16599\n79.91074\n81.89655\n  \n  \n  \n\n\n\n\nWe can also easily construct the map of the observed, fitted values and residuals from the model using tmap.\n\nahrf_m$fitted_lm3 &lt;- predict(lm3)\n\nahrf_m$resid &lt;- resid(lm3)\n\nactual &lt;-tm_shape(ahrf_m)+\n  tm_polygons(\"lbrate1618\",\n              palette = \"Blues\",\n              n=6,\n              style=\"fisher\",\n              border.col = NULL,\n              colorNA = \"grey50\")+\n  tm_shape(sts)+\n  tm_lines( col = \"black\")\n\nfit&lt;-tm_shape(ahrf_m)+\n  tm_polygons(\"fitted_lm3\",\n              palette = \"Blues\",\n              n=6,\n              style=\"fisher\", \n              border.col = NULL,\n              colorNA = \"grey50\")+\n  tm_shape(sts)+\n  tm_lines( col = \"black\")\n\nresids&lt;-tm_shape(ahrf_m)+\n  tm_polygons(\"resid\",\n              palette = \"RdBu\",\n              n=6,\n              style=\"fisher\",\n              midpoint = NA,\n              border.col = NULL,\n              colorNA = \"grey50\")+\n  tm_shape(sts)+\n  tm_lines( col = \"black\")\n\n\ntmap_arrange(actual, fit, resids, nrow=2)\n\n\n\n\nThis map layout shows the observed rates, the fitted rates from the lm3 model and the residuals from the model. Mapping residuals from models run on place-based data can show areas within the data where the model is consistently under or over-estimating the outcome. For instance in the example above, we see consistently high residuals in several of the south eastern states, and the mountain west, and several areas where the residuals are consistently negative, meaning we are over-estimating the rate. This can be instructive as to other variables we may be leaving out of the model that follow similar spatial distributions of the residuals. If such spatially patterned residuals are observed, it is generally a good idea to consider a statistical model that incorporates some kind of spatially explicit model specification.\n\n5.7.3.1 More on predicted values\nThere are more systematic methods for generating predictions from a model that allow us to marginalize the estimates across other variables and to generate counter factual or hypothetical rates using combinations of \\(x\\) values that may not be observed in the data. The emmeans package is very good at this and also accommodates many types of models. For instance if we would like the marginal means for counties by their healthcare shortage area type, after controlling for the other variables in our model, we can request that.\n\nlibrary(emmeans)\nrg&lt;- ref_grid(lm3s)\n\nmu1 &lt;- emmeans(rg, specs =\"hpsa16\" )\nmu1%&gt;%\n  as.data.frame()%&gt;%\n  gt()\n\n\n\n\n\n  \n    \n    \n      hpsa16\n      emmean\n      SE\n      df\n      lower.CL\n      upper.CL\n    \n  \n  \n    no shortage\n83.82779\n0.8441041\n2261\n82.17249\n85.48309\n    partial county shortage\n83.28049\n0.4890943\n2261\n82.32137\n84.23962\n    whole county shortage\n84.42809\n0.7923959\n2261\n82.87419\n85.98198\n  \n  \n  \n\n\n\n\nWhich unsurprisingly does not tell us much, because the health care shortage area variable in lm3 was not a significant predictor of the low birth weight rate. We can include more than one margin in the function, and include specific values we wish to highlight. For instance, let’s compare three hypothetical counties with three different poverty rates, we can use the summary(ahrf_m$fampov14) to see the first and third quartiles of the distribution and the mean, we will use these as our theoretical values.\n\nrg&lt;-ref_grid(lm3,\n             at=list( fampov14 = c(1.8,7.8, 11.6, 14.3, 52.1) ) )\n\nmeans &lt;- emmeans(rg, specs = c(\"fampov14\", \"rucc\"))\n\nmeans%&gt;%\n  as.data.frame()%&gt;%\n  ggplot(aes(x=rucc, y=emmean))+\n  geom_line(aes(group=factor(fampov14), color=factor(fampov14)))+\n  theme_classic()\n\n\n\n\nWhich illustrates the differences in the poverty rates on the low birth weight rate across the rural-urban continuum. The use of these marginal means is very useful when illustrating the effects of covariates in regression models, and to effectively illustrate interactions in such models. For instance, if we estimate the model below, which interacts the poverty rate with the rural-urban continuum code:\n\nlm3i&lt;- gls(lbrate1618 ~  fampov14 * rucc + hpsa16,\n          data = ahrf_m, \n          weights = varIdent(form = ~1|factor(state) ) )\n\n\nrg&lt;-ref_grid(lm3i,\n             at=list( fampov14 = c(7.8, 11.6, 14.3) ) )\n\nmeans &lt;- emmeans(rg, specs = c(\"fampov14\", \"rucc\"))\n\nmeans%&gt;%\n  as.data.frame()%&gt;%\n  ggplot(aes(x=rucc, y=emmean))+\n  geom_line(aes(group=factor(fampov14), color=factor(fampov14)))+\n  theme_classic()\n\n\n\n\nWe can see that in the most rural areas, (the 09 level of rucc), the differences by poverty rate are less than in more metropolitan areas, such as the 02 or 03 levels.\n\n\n5.7.3.2 Use of OLS for place-based models\nThe OLS model and its extensions are a very useful staring place when analyzing data on places. For nothing more than the interpretive ease of the models estimates, it presents a very attractive choice for ecological modeling. The extension of the model through weighted and generalized least square allows for more flexible modeling to accommodate non-constant variance that often arises. Further extensions of the model by techniques of spatial econometrics further allow for direct incorporation of spatially correlated and lagged effects of covariates and model error terms to better deal with the idiosyncrasies of place-based data (Chi and Zhu 2020; Elhorst 2014; LeSage and Pace 2009). These methods have seen wide use in demographic research over the past twenty years. Despite the fundamental flexibility of the OLS model, it may still not present the best solution when modeling demographic rates. One glaring reason is that if our outcomes are measured as rates, which are effectively probabilities, then the model can easily lead to estimates of predicted values that are either negative or greater than one, either of which presenting an issue for limited outcomes. In fact, this is why I am a strong proponent of not using the linear model for estimating probabilities. The next section of the book turns to the use of the Generalized Linear Model (Nelder and Wedderburn 1972; McCullagh and Nelder 1998) as an alternative modeling strategy, especially when considering place-based data, when data are measured either as rates or as relative risks."
  },
  {
    "objectID": "macro.html#basics-of-generalized-linear-models",
    "href": "macro.html#basics-of-generalized-linear-models",
    "title": "4  Macrodemographic Analysis of Places",
    "section": "5.8 Basics of Generalized Linear Models",
    "text": "5.8 Basics of Generalized Linear Models\nUp until now, we have been relying on linear statistical models which assumed the Normal distribution for our outcomes. A broader class of regression models, are Generalized Linear Models (Nelder and Wedderburn 1972; McCullagh and Nelder 1998), or GLMs, which allow for the estimation of a linear regression specification for outcomes that are not assumed to come from a Normal distribution. GLMs are a class of statistical models with three underlying components: A probability density appropriate to the outcome, a link function and a linear predictor. The link function is some mathematical function that links the mean of the specified probability distribution to the linear predictor of regression parameters and covariates. For example, the Normal distribution used by the OLS model has the mean, \\(\\mu\\), which is typically estimated using the linear mean function : \\(\\mu = \\beta_0 + \\beta_1 x_1\\) Which describes the line that estimates the mean of the outcome variable as a linear function of \\(\\beta\\) parameters and the predictor variable \\(x_1\\). The OLS, or Gaussian GLM model uses an identity link meaning there is no transformation of the linear mean function as it is connected to the mean of the outcome. This can be written as:\n\\[g(u) = g(E(Y)) = \\beta_0 + \\beta_1 x_1\\]\nWhere \\(g()\\) is the link function, linking the mean of the Normal distribution to the linear mean function of the model. The equivalent GLM model to the lm1 model from the previous section is:\n\nglm1&lt;- glm(lbrate1618 ~  fampov14 + rucc + hpsa16,\n          data = ahrf_m, \n          family =gaussian)\nlibrary(texreg)\n\nVersion:  1.38.6\nDate:     2022-04-06\nAuthor:   Philip Leifeld (University of Essex)\n\nConsider submitting praise using the praise or praise_interactive functions.\nPlease cite the JSS article in your publications -- see citation(\"texreg\").\n\n\n\nAttaching package: 'texreg'\n\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\ntexreg(list(glm1, lm1), file = \"4_1.tex\")\n\nThe table was written to the file '4_1.tex'.\n\nlm1_t&lt;-lm1%&gt;%\n  tbl_regression()\n\n\n\nglm1_t&lt;-glm1%&gt;%\n  tbl_regression()\n \n\nt_m &lt;- tbl_merge(\n    tbls = list(lm1_t, glm1_t),\n    tab_spanner = c(\"**OLS**\", \"**GLM**\")\n  ) \n\nt_m\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      \n        OLS\n      \n      \n        GLM\n      \n    \n    \n      Beta\n      95% CI1\n      p-value\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    % Families Below Poverty Level 2014-18\n2.3\n2.1, 2.4\n&lt;0.001\n2.3\n2.1, 2.4\n&lt;0.001\n    rucc\n\n\n\n\n\n\n        01\n—\n—\n\n—\n—\n\n        02\n-1.9\n-4.3, 0.44\n0.11\n-1.9\n-4.3, 0.44\n0.11\n        03\n-3.4\n-5.8, -0.93\n0.007\n-3.4\n-5.8, -0.94\n0.007\n        04\n-6.7\n-9.4, -3.9\n&lt;0.001\n-6.7\n-9.4, -3.9\n&lt;0.001\n        05\n-6.0\n-9.8, -2.2\n0.002\n-6.0\n-9.8, -2.2\n0.002\n        06\n-3.9\n-6.2, -1.7\n&lt;0.001\n-3.9\n-6.2, -1.7\n&lt;0.001\n        07\n-4.0\n-6.5, -1.5\n0.002\n-4.0\n-6.5, -1.5\n0.002\n        08\n0.45\n-3.7, 4.6\n0.8\n0.45\n-3.7, 4.6\n0.8\n        09\n-3.9\n-8.4, 0.61\n0.090\n-3.9\n-8.4, 0.61\n0.090\n    hpsa16\n\n\n\n\n\n\n        no shortage\n—\n—\n\n—\n—\n\n        partial county shortage\n-0.66\n-2.6, 1.3\n0.5\n-0.66\n-2.6, 1.3\n0.5\n        whole county shortage\n2.4\n-0.10, 4.8\n0.060\n2.4\n-0.09, 4.8\n0.060\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\nWhich shows the exact same output for both models, as it should be. The output shown by summary(lm1) and summary(glm1) is different though, but the same results can be recovered.\n\nsummary(glm1)\n\n\nCall:\nglm(formula = lbrate1618 ~ fampov14 + rucc + hpsa16, family = gaussian, \n    data = ahrf_m)\n\nCoefficients:\n                              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                   62.26920    1.18901  52.371  &lt; 2e-16 ***\nfampov14                       2.25805    0.06846  32.981  &lt; 2e-16 ***\nrucc02                        -1.91854    1.20082  -1.598 0.110249    \nrucc03                        -3.38658    1.25055  -2.708 0.006818 ** \nrucc04                        -6.65483    1.40639  -4.732 2.36e-06 ***\nrucc05                        -5.96283    1.93739  -3.078 0.002110 ** \nrucc06                        -3.94180    1.14317  -3.448 0.000575 ***\nrucc07                        -4.00730    1.28166  -3.127 0.001790 ** \nrucc08                         0.45112    2.11062   0.214 0.830770    \nrucc09                        -3.88365    2.29118  -1.695 0.090202 .  \nhpsa16partial county shortage -0.66219    1.00767  -0.657 0.511148    \nhpsa16whole county shortage    2.36214    1.25335   1.885 0.059601 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 267.6184)\n\n    Null deviance: 981607  on 2323  degrees of freedom\nResidual deviance: 618734  on 2312  degrees of freedom\nAIC: 19599\n\nNumber of Fisher Scoring iterations: 2\n\n\nThis output shows the same coefficients, and hypothesis test results compared to summary(lm1), but the residual variances are reported differently. The GLM summary reports the Null and Residual deviance instead of the Residual standard errors reported by summary(lm1). If we take the residual deviance and divide it by the residual degrees of freedom, and take the square root, we get the residual standard error reported by summary(lm1):\n\nsqrt(glm1$deviance/glm1$df.residual)\n\n[1] 16.35905\n\nsummary(lm1)$sigma\n\n[1] 16.35905\n\n\nThe deviance in the GLM model is calculated in the same way as the residual sums of squares:\n\nsum((fitted(lm1)-ahrf_m$lbrate1618 )^2)\n\n[1] 618733.7\n\nglm1$deviance\n\n[1] 618733.7\n\n\nWe do not need to assume the identity function is the only one for the Gaussian GLM, for instance, the logarithmic link function can change the model to:\n\\[\nln(Y) = \\beta_0 + \\beta_1 x_1 \\\\\nY = exp(\\beta_0 + \\beta_1 x_1) \\\\\nY = exp(\\beta_0) * exp(\\beta_1 x_1)\n\\] Which changes the model to no longer be additive in terms of the parameters for the logarithmic link function.\n\nglm2&lt;- glm(lbrate1618 ~  fampov14 + rucc + hpsa16,\n          data = ahrf_m, \n          family =gaussian(link = \"log\"))\nAIC(glm1, glm2)\n\n     df      AIC\nglm1 13 19599.34\nglm2 13 19687.44\n\n\nIn this case, the identity link function is preferred because of the lower AIC.\nDifferent distributions have different link functions….\nThe identity link function is appropriate for the Normal distribution, because this distribution can take any value from \\(- \\infty\\) to \\(\\infty\\), and so the linear mean function can also take those values, theoretically. Other distributions may not have this wide of a numeric range, so appropriate link functions have to be used to transform the linear mean function to the scale of the mean of a particular distribution. The most common distributions for the generalized linear model and their common link functions are shown below, along with common expressions for the mean and variance of their respective distributions.\n\n\n\n\n\n\n\n\n\n\nDistribution\nMean\nVariance\nLink Function\nRange of Outcome\n\n\n\n\nGaussian\n\\(\\mu\\)\n\\(\\sigma^2\\)\nIdentity\n\\((-\\infty , \\infty)\\)\n\n\nBinomial\n\\(\\pi\\)\n\\(n\\pi(1-\\pi)\\)\n\\(log \\left (\\frac{\\pi}{1-\\pi} \\right )\\)\n\\(\\frac{0,1,2,...n}{n}\\)\n\n\nPoisson\n\\(\\lambda\\)\n\\(\\lambda\\)\n\\(log (\\lambda)\\)\n\\((0,1,2,...)\\)\n\n\nGamma\n\\(\\mu\\)\n\\(\\phi \\mu^2\\)\n\\(log (\\mu)\\)\n\\((0, \\infty)\\)\n\n\nNegative Binomial\n\\(n(1-p)/p\\)\n\\(n(1-p)/p^2\\)\n\\(log (\\mu)\\)\n\\((0,1,2,...)\\)\n\n\nStudent-t\n\\(\\mu\\)\n\\(\\frac{\\sigma^2 \\nu}{\\nu-2}\\)\nIdentity\n\\(-\\infty , \\infty\\)\n\n\n\nWhile these are not all possible distributions for the GLM, these are distributions that are both widely used and commonly present not only in R but in other software as well. The VGAM package adds a much wider selection of both univariate and bivariate distributions for discrete and continuous outcomes.\n\n5.8.1 Binomial Distribution\nYou have probably seen the binomial distribution in either a basic statistics course, remember the coin flips? Or in the context of a logistic regression model. There are two ways the binomial distribution is typically used, the first is the context of logistic regression, where a special case of the binomial is used, called the Bernoulli distribution. This is the case of the binomial when there is basically a single coin flip, and you’re trying to estimate the probability that it is heads (or tails). This is said to be a single trial, and the outcome is either 1 or 0 (heads or tails). We will spend time in chapter 5 discussing the logistic regression model in the context of individual level data.\nThe second way the binomial is used is when you have multiple trials, and you’re trying to estimate the probability of the event occurring over these trials. In this case, your number of trials, \\(n\\) can be large, and your number of successes, \\(y\\) is the random variable under consideration. This usage of the binomial has a wide applicability for place-based demographic analysis, as the basic distribution for a demographic rate. I will commonly refer to this as the count-based binomial distribution.\nThe mean of the binomial distribution is a proportion or a probability, \\(\\pi\\), which tells you the probability of the event of interest occurs. Any model using the binomial distributor will be geared towards estimating this probability. The good thing is that, when we have count data, not just 1’s and 0’s, the same thing happens. The ratio or successes (\\(y\\)) to trials (\\(n\\)) is used to estimate \\(\\pi\\) and we build a model for that mean rate:\n\\[\\text{Binomial} \\binom{n}{y} = \\frac{y}{n} = \\pi = \\text{some function of predictors}\\]\nThe ratio \\(\\frac{y}{n}\\) is a rate or probability, and as such has very strict bounds. Probabilities cannot be less than 0 or greater than 1, so again, we should not use the Normal distribution here, since it is valid for all real numbers. Instead, we are using the binomial, but we still run into the problem of having a strictly bounded value, \\(\\pi\\) that we are trying to estimate with a linear function.\nEnter the link function again.\nThe binomial distribution typically uses either a logit or probit link function, but others such as the complementary log-log link function are also used in certain circumstances. For now we will use the logit function.\nThe logit transforms the probability, \\(\\pi\\), which is bound on the interval \\([0,1]\\) into a new unbounded interval similar to the normal distribution of \\([-\\infty, \\infty]\\). The transformation is knows a the log-odds transformation, or logit for short. The odds of an event happening are the probability that something happens, divided by the probability it does not happen, in this case:\n\\[\\text{odds}({\\pi}) = \\frac{\\pi}{(1-\\pi)}\\]\nWhich is bound on the interval \\([0, \\infty]\\), when we take the natural log of the odds, the value is transformed into the linear space, of \\([-\\infty, \\infty]\\).\n\\[\\text{log-odds }({\\pi}) = log  \\left ( \\frac{\\pi}{(1-\\pi)}  \\right) \\]\nThis can be modeled using a linear function of covariates now, without worrying about the original boundary problem:\n\\[log  \\left ( \\frac{\\pi}{1-\\pi}  \\right) = \\beta_0 +\\beta_1 x_1\\]\nor more compactly:\n\\[logit (\\pi)  = \\beta_0 +\\beta_1 x_1\\]\n\n5.8.1.1 Binomial regression\nThe glm() function can estimate the count binomial model using the syntax cbind( y, n-y) in the outcome portion of the model formula.\n\nglmb&lt;- glm(cbind(lowbw1618, births1618-lowbw1618) ~  fampov14 + rucc + hpsa16,\n          data = ahrf_m, \n          family = binomial)\n\nglmb%&gt;%\n  tbl_regression()\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      log(OR)1\n      95% CI1\n      p-value\n    \n  \n  \n    % Families Below Poverty Level 2014-18\n0.02\n0.02, 0.02\n&lt;0.001\n    rucc\n\n\n\n        01\n—\n—\n\n        02\n-0.01\n-0.02, -0.01\n0.003\n        03\n-0.02\n-0.04, -0.01\n&lt;0.001\n        04\n-0.06\n-0.08, -0.04\n&lt;0.001\n        05\n-0.04\n-0.07, -0.01\n0.004\n        06\n-0.02\n-0.04, -0.01\n0.014\n        07\n-0.03\n-0.06, -0.01\n0.016\n        08\n0.04\n-0.02, 0.10\n0.2\n        09\n-0.03\n-0.10, 0.03\n0.3\n    hpsa16\n\n\n\n        no shortage\n—\n—\n\n        partial county shortage\n-0.02\n-0.03, 0.00\n0.011\n        whole county shortage\n0.00\n-0.02, 0.03\n0.8\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nThe output above shows the results of the model. The coefficients are on the log-odds scale, and typically would be converted to an odds-ratio by exponentiating them.\n\nglmb%&gt;%\n  tbl_regression(exponentiate=TRUE)\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    % Families Below Poverty Level 2014-18\n1.02\n1.02, 1.02\n&lt;0.001\n    rucc\n\n\n\n        01\n—\n—\n\n        02\n0.99\n0.98, 1.0\n0.003\n        03\n0.98\n0.96, 0.99\n&lt;0.001\n        04\n0.95\n0.93, 0.96\n&lt;0.001\n        05\n0.96\n0.93, 0.99\n0.004\n        06\n0.98\n0.96, 1.0\n0.014\n        07\n0.97\n0.94, 0.99\n0.016\n        08\n1.05\n0.98, 1.11\n0.2\n        09\n0.97\n0.90, 1.03\n0.3\n    hpsa16\n\n\n\n        no shortage\n—\n—\n\n        partial county shortage\n0.98\n0.97, 1.00\n0.011\n        whole county shortage\n1.00\n0.98, 1.03\n0.8\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nIn this case, the odds ratio interpretation is not as clear as in the case of the Bernoulli case, in the context of individuals. When I interpret the coefficients for the count binomial, I describe them as percent changes in the mean. For example, the fampov14 odds ratio is 1.02, I describe this result as: The low birth weight rate increases by 2 percent for every 1 percentage point increase in the poverty rate. We can see this by using the fitted values from emmeans, here I generate two cases where fampov14 is exactly 1 percentage point different, and you can see the difference in the estimated rates.\n\nrg &lt;- ref_grid(glmb,\n               at=list( fampov14 = c(5, 6) ) ) \nemmeans(rg,\n        specs = \"fampov14\",\n        type = \"response\")\n\n fampov14   prob       SE  df asymp.LCL asymp.UCL\n        5 0.0735 0.000474 Inf    0.0726    0.0745\n        6 0.0750 0.000470 Inf    0.0741    0.0760\n\nResults are averaged over the levels of: rucc, hpsa16 \nConfidence level used: 0.95 \nIntervals are back-transformed from the logit scale \n\n\nWe can calculate the percentage change in these two estimates:\n\n(.0750 - .0735)/.0750\n\n[1] 0.02\n\n\nand confirm that it is 2 percent.\n\n\n5.8.1.2 Application of the binomial to age standardization\nThe Binomial is very useful for conducting standardization of rates between groups to measure the differences. To show an example of how to do age standardization, I use data from the CDC Wonder Compressed Mortality file. The data are for the states of Texas and California for the year 2016, and are the numbers of deaths and population at risk in 13 age groups.\n\ntxca &lt;- readr::read_delim(\"data/CMF_TX_CA_age.txt\",\n                          delim = \"\\t\",\n                          quote = \"\\\"\",\n                          skip=1,\n                          col_names = c(\"State\",\n                                        \"State_Code\",\n                                        \"Age_Group\",\n                                        \"Age_Group_Code\",\n                                        \"Deaths\",\n                                        \"Population\",\n                                        \"Crude.Rate\")\n                          )%&gt;%\n  filter(Age_Group != \"Not Stated\")%&gt;%\n  mutate(Population = as.numeric(Population), Deaths = as.numeric(Deaths))\n\nRows: 28 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (7): State, State_Code, Age_Group, Age_Group_Code, Deaths, Population, C...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ntxca%&gt;%\n  gt()\n\n\n\n\n\n  \n    \n    \n      State\n      State_Code\n      Age_Group\n      Age_Group_Code\n      Deaths\n      Population\n      Crude.Rate\n    \n  \n  \n    California\n06\n&lt; 1 year\n1\n2057\n498832\n412.4\n    California\n06\n1-4 years\n1-4\n397\n1988540\n20.0\n    California\n06\n5-9 years\n5-9\n253\n2539626\n10.0\n    California\n06\n10-14 years\n10-14\n277\n2522475\n11.0\n    California\n06\n15-19 years\n15-19\n1076\n2579986\n41.7\n    California\n06\n20-24 years\n20-24\n2090\n2812191\n74.3\n    California\n06\n25-34 years\n25-34\n5201\n5917785\n87.9\n    California\n06\n35-44 years\n35-44\n7032\n5159932\n136.3\n    California\n06\n45-54 years\n45-54\n16370\n5195297\n315.1\n    California\n06\n55-64 years\n55-64\n34176\n4688718\n728.9\n    California\n06\n65-74 years\n65-74\n45834\n3089002\n1483.8\n    California\n06\n75-84 years\n75-84\n59121\n1535300\n3850.8\n    California\n06\n85+ years\n85+\n88331\n722333\n12228.6\n    Texas\n48\n&lt; 1 year\n1\n2287\n405899\n563.4\n    Texas\n48\n1-4 years\n1-4\n433\n1613272\n26.8\n    Texas\n48\n5-9 years\n5-9\n276\n2038319\n13.5\n    Texas\n48\n10-14 years\n10-14\n319\n2029062\n15.7\n    Texas\n48\n15-19 years\n15-19\n999\n1970588\n50.7\n    Texas\n48\n20-24 years\n20-24\n1829\n2005169\n91.2\n    Texas\n48\n25-34 years\n25-34\n4405\n4085728\n107.8\n    Texas\n48\n35-44 years\n35-44\n6292\n3726287\n168.9\n    Texas\n48\n45-54 years\n45-54\n13636\n3519013\n387.5\n    Texas\n48\n55-64 years\n55-64\n28663\n3116019\n919.9\n    Texas\n48\n65-74 years\n65-74\n37506\n2008449\n1867.4\n    Texas\n48\n75-84 years\n75-84\n44144\n957001\n4612.7\n    Texas\n48\n85+ years\n85+\n51173\n387790\n13196.1\n  \n  \n  \n\n\n\n\n\ntxca%&gt;%\n  group_by(State)%&gt;%\n  summarise(p_pop = Population/sum(Population ))%&gt;%\n  ungroup()%&gt;%\n  mutate(age = forcats::fct_relevel(txca$Age_Group,\"&lt; 1 year\",\n                                    \"1-4 years\",\n                                     \"5-9 years\",\n                                    \"10-14 years\",\n                                    \"15-19 years\",\n                                    \"20-24 years\",\n                                    \"25-34 years\",\n                                    \"35-44 years\",\n                                    \"45-54 years\",\n                                    \"55-64 years\",\n                                    \"65-74 years\",\n                                    \"75-84 years\", \"85+ years\"\n                                    ))%&gt;%\n  ggplot(aes(x = age, y = p_pop,group=State, color= State))+\n  geom_line(lwd=2)+\n  ylab(\"% in Age\" )+\n  xlab (\"Age group\")+\n  ggtitle(\"Age distribution in Texas and California\")+\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n\n\n`summarise()` has grouped output by 'State'. You can override using the\n`.groups` argument.\n\n\n\n\n\nWe see that the age distribution of Texas is slightly younger than California, also the large peak at 25-34 year age group is because the data adopt a 10 year age interval after age 25.\nTo do age standardization using the regression framework, we have to control for the differences in the age structure of the two populations (Texas and California) by regressing the mortality rate on the Age structure, the difference between the states after doing this is the difference in the age-standardized rate.\n\ntxca&lt;- txca%&gt;%\n  mutate(Age_Group = forcats::fct_relevel(txca$Age_Group,\"&lt; 1 year\",\n                                    \"1-4 years\",\n                                     \"5-9 years\",\n                                    \"10-14 years\",\n                                    \"15-19 years\",\n                                    \"20-24 years\",\n                                    \"25-34 years\",\n                                    \"35-44 years\",\n                                    \"45-54 years\",\n                                    \"55-64 years\",\n                                    \"65-74 years\",\n                                    \"75-84 years\", \"85+ years\"\n                                    ))\n\nglmb_s &lt;- glm(cbind(Deaths, Population-Deaths) ~ factor(Age_Group)+State,\n              data=txca,\n              family=binomial)\n\nglmb_s%&gt;%\n  tbl_regression(exp = TRUE)\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    factor(Age_Group)\n\n\n\n        &lt; 1 year\n—\n—\n\n        1-4 years\n0.05\n0.04, 0.05\n&lt;0.001\n        5-9 years\n0.02\n0.02, 0.03\n&lt;0.001\n        10-14 years\n0.03\n0.02, 0.03\n&lt;0.001\n        15-19 years\n0.09\n0.09, 0.10\n&lt;0.001\n        20-24 years\n0.17\n0.16, 0.18\n&lt;0.001\n        25-34 years\n0.20\n0.19, 0.21\n&lt;0.001\n        35-44 years\n0.31\n0.30, 0.32\n&lt;0.001\n        45-54 years\n0.72\n0.70, 0.75\n&lt;0.001\n        55-64 years\n1.70\n1.65, 1.75\n&lt;0.001\n        65-74 years\n3.48\n3.38, 3.59\n&lt;0.001\n        75-84 years\n9.07\n8.80, 9.35\n&lt;0.001\n        85+ years\n30.4\n29.5, 31.3\n&lt;0.001\n    State\n\n\n\n        California\n—\n—\n\n        Texas\n1.20\n1.19, 1.21\n&lt;0.001\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nWhich shows that Texas has a standardized mortality rate 20 percent higher than California."
  },
  {
    "objectID": "macro.html#poisson-distribution",
    "href": "macro.html#poisson-distribution",
    "title": "4  Macrodemographic Analysis of Places",
    "section": "5.9 Poisson distribution",
    "text": "5.9 Poisson distribution\nAnother distribution commonly used in the analysis of place-based data is the Poisson distribution. The Poisson is applicable to outcomes that are positive integers, and is commonly used in epidemiology as a model of relative risk of a disease or mortality. The Poisson has a single parameter, the mean, \\(\\lambda\\), and it is really the average count for the outcome (\\(y\\)). We have several ways of modeling a count outcome with the Poisson\n\nPure count model If each area or place has the same total area, risk set, or population size, then we can model the mean as-is. This would lead to a model that looks like:\n\n\\[log(\\lambda)= \\beta_0 + \\beta_1 x_1\\]\nWhen we see the \\(\\beta_1\\) parameter in this model in computer output, it is on the log-scale, since that is the scale of the outcome for the Poisson model. In order to interpret the \\(\\beta_1\\), we have to exponentiate it. When we do this, the parameter is interpreted as the percentage change in the mean of the outcome, for a 1 unit change in \\(x_1\\). For instance if we estimate a model and see in the output that \\(\\beta_1 = \\text{.025}\\), then \\(\\exp(\\beta_1) = \\text{exp}(\\text{.025}) = \\text{1.025}\\), or for a 1 unit increase in \\(x_1\\), the mean of \\(y\\) increases by 1.025. So if the mean of \\(y\\) is 10, when \\(x_1\\) = 0, then the mean is \\(10*(1.025*1)\\) or \\(10.25\\) when \\(x_1\\) = 1. This application of the Poisson is rare in demographic research because places rarely have either equal populations or areas, so the Rate Model or the Relative Risk Model are much more commonly used.\n\nRate model The second type of modeling strategy used in the Poisson model is for a rate of occurrence. This model includes an offset term in the model to incorporate unequal population sizes, this is the most common way the data are analyzed in demographic research. This offset term can be thought of as the denominator for the rate, and we can show how it is included in the model.\n\nIf \\(n\\) is the population size for each place, then, we want to do a regression on the rate of occurrence of our outcome. The rate is typically expressed as a proportion, or probability \\(rate = \\frac{y}{n}\\), as seen in the Binomial distribution earlier:\n\\[\nlog(y/n)= \\beta_0 + \\beta_1 x_1 \\\\\nlog(y) - log(n)= \\beta_0 + \\beta_1 x_1\\\\\nlog(y)= \\beta_0 + \\beta_1 x_1 + log(n)\n\\]\nSimilar to the example from before, when interpreting the effect of \\(\\beta_1\\) in this model, we also have to exponentiate it. In this case, the interpretation would not be related to the overall count, but to the rate of occurrence. So, if as before, the \\(\\beta_1 = \\text{.025}\\), then \\(\\exp(\\beta_1) = \\text{exp}(\\text{.025}) = \\text{1.025}\\), or for a 1 unit increase in \\(x_1\\), the rate of occurrence of \\(y\\) increases by a factor of 1.025.\nThis model includes the natural log of the population size in the offset() function on the right side of the model formula. R will not estimate a regression coefficient for this term, and as in the equation above, the term just represents a scale factor for the outcome.\nNote on offsets It is important to ensure that all of the populations in a particular analysis have non-zero counts, because if the model sees \\(log(0)\\) in data, it will generate an error, because this is not a number.\n\nlog(0)\n\n[1] -Inf\n\n\nThe Poisson model with a population offset is specified as:\n\nglmp_s &lt;- glm(Deaths ~ offset(log(Population)) + factor(Age_Group) + State,\n              data=txca,\n              family=poisson)\n\nglmp_s%&gt;%\n  tbl_regression(exp = TRUE)\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      IRR1\n      95% CI1\n      p-value\n    \n  \n  \n    factor(Age_Group)\n\n\n\n        &lt; 1 year\n—\n—\n\n        1-4 years\n0.05\n0.04, 0.05\n&lt;0.001\n        5-9 years\n0.02\n0.02, 0.03\n&lt;0.001\n        10-14 years\n0.03\n0.03, 0.03\n&lt;0.001\n        15-19 years\n0.10\n0.09, 0.10\n&lt;0.001\n        20-24 years\n0.17\n0.16, 0.18\n&lt;0.001\n        25-34 years\n0.20\n0.19, 0.21\n&lt;0.001\n        35-44 years\n0.31\n0.30, 0.32\n&lt;0.001\n        45-54 years\n0.72\n0.70, 0.75\n&lt;0.001\n        55-64 years\n1.69\n1.64, 1.74\n&lt;0.001\n        65-74 years\n3.44\n3.33, 3.54\n&lt;0.001\n        75-84 years\n8.73\n8.47, 9.00\n&lt;0.001\n        85+ years\n26.6\n25.8, 27.4\n&lt;0.001\n    State\n\n\n\n        California\n—\n—\n\n        Texas\n1.19\n1.18, 1.19\n&lt;0.001\n  \n  \n  \n    \n      1 IRR = Incidence Rate Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nThis result is very close to that from the Binomial model, where we see after age standardization, Texas has a 19 percent higher mortality rate overall than California."
  },
  {
    "objectID": "macro.html#relative-risk-analysis",
    "href": "macro.html#relative-risk-analysis",
    "title": "4  Macrodemographic Analysis of Places",
    "section": "5.10 Relative risk analysis",
    "text": "5.10 Relative risk analysis\nThe third type of model for the Poisson distribution focuses on the idea of the relative risk of an event, and uses the Standardized risk ratio as its currency.\n\nThe Standardized risk ratio incorporates differential exposure due to population size as an expected count of the outcome in the offset term, and are typically seen in epidemiological studies. The expected count \\(E\\), incorporates the different population sizes of each area by estimating the number of events that should occur, if the area followed a given rate of occurrence. The expected count is calculated by multiplying the average rate of occurrence, \\(r\\), by the population size, \\(n\\): \\(E_i = r * n_i\\), where \\(r = \\frac{\\sum y_i}{\\sum n_i}\\), is the overall rate in the population. This method is commonly referred to as internal standardization because we are using the data at hand to estimate the overall rate of occurrence, versus using a rate from some other published source.\n\nThe model for the mean of the outcome would look like this:\n\\[log(y)= \\beta_0 + \\beta_1 x_1  + log(E)\\].\nAnd is specified very similarly to the rate model above. First, I show how to calculate the expected number of deaths in the data. A naive method of calculating the expected counts is to use the crude death rate as \\(r\\), or we can use an age-specific death rate.\n\n#crude death rate\ntxca$E&lt;- txca$Population*(sum(txca$Deaths/sum(txca$Population)))\n\nIn this calculation, the Population variable is multiplied by \\(r\\), which is the sum of all deaths, divided by all populations.\nThe age-specific expected count is a little more involved. We first have to sum all deaths and populations by age, then calculate the age specific rate, then join this back to the original data based on the Age_Group variable. Is this the only way to do this, no, but it works in this example. Alternatively, we could get another age schedule of mortality rates and merge it to these data and standardize our data to that mortality schedule.\n\n#Age specific death rate\ntxca2&lt;- txca%&gt;%\n  group_by(Age_Group)%&gt;%\n  summarise(ndeaths = sum(Deaths), npop=sum(Population))%&gt;%\n  mutate(r_age = ndeaths/npop)%&gt;%\n  ungroup()%&gt;%\n  left_join(., txca, by = \"Age_Group\")%&gt;%\n  arrange(State, Age_Group)%&gt;%\n  mutate(E_age = Population * r_age)\n\ntxca2%&gt;%\n  select(State, Age_Group, Deaths, Population, E, E_age)%&gt;%\n  gt()\n\n\n\n\n\n  \n    \n    \n      State\n      Age_Group\n      Deaths\n      Population\n      E\n      E_age\n    \n  \n  \n    California\n&lt; 1 year\n2057\n498832\n3375.789\n2395.1055\n    California\n1-4 years\n397\n1988540\n13457.219\n458.2383\n    California\n5-9 years\n253\n2539626\n17186.631\n293.4640\n    California\n10-14 years\n277\n2522475\n17070.564\n330.3049\n    California\n15-19 years\n1076\n2579986\n17459.763\n1176.4386\n    California\n20-24 years\n2090\n2812191\n19031.184\n2287.7627\n    California\n25-34 years\n5201\n5917785\n40047.939\n5682.6280\n    California\n35-44 years\n7032\n5159932\n34919.255\n7736.8039\n    California\n45-54 years\n16370\n5195297\n35158.583\n17888.9759\n    California\n55-64 years\n34176\n4688718\n31730.368\n37750.7084\n    California\n65-74 years\n45834\n3089002\n20904.471\n50503.1685\n    California\n75-84 years\n59121\n1535300\n10389.969\n63613.0044\n    California\n85+ years\n88331\n722333\n4888.307\n90772.2323\n    Texas\n&lt; 1 year\n2287\n405899\n2746.875\n1948.8945\n    Texas\n1-4 years\n433\n1613272\n10917.635\n371.7617\n    Texas\n5-9 years\n276\n2038319\n13794.093\n235.5360\n    Texas\n10-14 years\n319\n2029062\n13731.447\n265.6951\n    Texas\n15-19 years\n999\n1970588\n13335.731\n898.5614\n    Texas\n20-24 years\n1829\n2005169\n13569.754\n1631.2373\n    Texas\n25-34 years\n4405\n4085728\n27649.701\n3923.3720\n    Texas\n35-44 years\n6292\n3726287\n25217.225\n5587.1961\n    Texas\n45-54 years\n13636\n3519013\n23814.522\n12117.0241\n    Texas\n55-64 years\n28663\n3116019\n21087.305\n25088.2916\n    Texas\n65-74 years\n37506\n2008449\n13591.951\n32836.8315\n    Texas\n75-84 years\n44144\n957001\n6476.396\n39651.9956\n    Texas\n85+ years\n51173\n387790\n2624.325\n48731.7677\n  \n  \n  \n\n\n\n\nIn this table, you can see the age-specific expected counts of deaths are much more in line with the age-specific mortality rate, and the numbers of expected deaths are much more similar to the observed pattern of deaths, when compared to the expected counts derived from the crude death rate.\n\nglmp_E &lt;- glm(Deaths ~ offset(log(E)) + factor(Age_Group)+State,\n              data=txca2,\n              family=poisson)\n\nglmp_Eage &lt;- glm(Deaths ~ offset(log(E_age)) + factor(Age_Group)+State,\n              data=txca2,\n              family=poisson)\n\nm1 &lt;- glmp_E%&gt;%\n  tbl_regression(exp = T)\nm2 &lt;- glmp_Eage%&gt;%\n  tbl_regression(exp = T)\n\nm_all &lt;- tbl_merge(list(m1, m2))\n\nm_all\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      \n        Table 1\n      \n      \n        Table 2\n      \n    \n    \n      IRR1\n      95% CI1\n      p-value\n      IRR1\n      95% CI1\n      p-value\n    \n  \n  \n    factor(Age_Group)\n\n\n\n\n\n\n        &lt; 1 year\n—\n—\n\n—\n—\n\n        1-4 years\n0.05\n0.04, 0.05\n&lt;0.001\n1.00\n0.93, 1.08\n&gt;0.9\n        5-9 years\n0.02\n0.02, 0.03\n&lt;0.001\n1.00\n0.91, 1.09\n&gt;0.9\n        10-14 years\n0.03\n0.03, 0.03\n&lt;0.001\n1.00\n0.92, 1.09\n&gt;0.9\n        15-19 years\n0.10\n0.09, 0.10\n&lt;0.001\n1.00\n0.95, 1.06\n&gt;0.9\n        20-24 years\n0.17\n0.16, 0.18\n&lt;0.001\n1.01\n0.96, 1.05\n0.8\n        25-34 years\n0.20\n0.19, 0.21\n&lt;0.001\n1.01\n0.97, 1.04\n0.7\n        35-44 years\n0.31\n0.30, 0.32\n&lt;0.001\n1.01\n0.97, 1.04\n0.8\n        45-54 years\n0.72\n0.70, 0.75\n&lt;0.001\n1.01\n0.98, 1.04\n0.6\n        55-64 years\n1.69\n1.64, 1.74\n&lt;0.001\n1.01\n0.98, 1.04\n0.6\n        65-74 years\n3.44\n3.33, 3.54\n&lt;0.001\n1.01\n0.98, 1.04\n0.5\n        75-84 years\n8.73\n8.47, 9.00\n&lt;0.001\n1.01\n0.98, 1.04\n0.5\n        85+ years\n26.6\n25.8, 27.4\n&lt;0.001\n1.02\n0.99, 1.05\n0.3\n    State\n\n\n\n\n\n\n        California\n—\n—\n\n—\n—\n\n        Texas\n1.19\n1.18, 1.19\n&lt;0.001\n1.19\n1.18, 1.19\n&lt;0.001\n  \n  \n  \n    \n      1 IRR = Incidence Rate Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nThis result is identical in terms of the difference between states, as the rate model above, and the results are invariant to the choice of the standard used, although when the age-specific expected count is used, the overall mortality pattern becomes insignificant in the model. This is an example of, despite different denominator/offset terms, we can achieve the same comparison from either the rate model or the model for relative risks."
  },
  {
    "objectID": "macro.html#overdispersion",
    "href": "macro.html#overdispersion",
    "title": "4  Macrodemographic Analysis of Places",
    "section": "5.11 Overdispersion",
    "text": "5.11 Overdispersion\nWhen using the Poisson GLM, you often run into overdispersion. What’s overdispersion you might ask? For the Poisson distribution, the mean and the variance are functions of one another (variance = mean for Poisson). So when you have more variability than you expect in your data, you have overdispersion. This basically says that your data do not fit your model, and is a problem because overdispersion leads to standard errors for our model parameters that are too small typically. But, we can fit other models that do not make such assumptions, or allow there to be more variability.\n\n5.11.0.1 Checking for overdispersion\nAn easy check on this is to compare the residual deviance to the residual degrees of freedom. They ratio should be 1 if the model fits the data.\n\nscale&lt;-sqrt(glmp_E$deviance/glmp_E$df.residual)\nscale\n\nHere, we see for the Poisson model, the scale factor is over 6, which shows evidence of overdispersion in the data. The residual deviance can also be used as a goodness of fit test for the model, because the deviance has been shown to be distributed as a \\(\\chi^2\\) distribution, with degrees of freedom equal to the residual d.f. (n-p):\n\n1-pchisq(glmp_E$deviance,\n         df = glmp_E$df.residual)\n\nSo, this p value is 0, which means the model does not fit the data. If the goodness of fit test had a p-value over 5 percent, we could conclude that the model in fact did fit the data."
  },
  {
    "objectID": "macro.html#modeling-overdispersion-via-a-quasi-distribution",
    "href": "macro.html#modeling-overdispersion-via-a-quasi-distribution",
    "title": "4  Macrodemographic Analysis of Places",
    "section": "5.12 Modeling Overdispersion via a Quasi distribution",
    "text": "5.12 Modeling Overdispersion via a Quasi distribution\nFor the Poisson and the Binomial, we can fit a “quasi” distribution that adds an extra parameter to allow the mean-variance relationship to not be constant. For Poisson we get:\n\\(Var(Y) = \\lambda * \\phi\\), instead of \\(Var(Y) = \\lambda\\)\nThis accessory parameter \\(\\phi\\) allows us to include a rough proxy for a dispersion parameter for the distribution. Naturally this is fixed at 1 for the normal Poisson model, and estimated in the quasi models, we can look to see if is much bigger than 1. If overdispersion is present and not accounted for you could identify a relationship as being significant when it is not!\n\nglmqp &lt;- glm(Deaths ~ offset(log(E)) + factor(Age_Group)+State,\n              data=txca,\n              family=quasipoisson)\n\nglmqp%&gt;%\n  tbl_regression(exp = TRUE)\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      IRR1\n      95% CI1\n      p-value\n    \n  \n  \n    factor(Age_Group)\n\n\n\n        &lt; 1 year\n—\n—\n\n        1-4 years\n0.05\n0.03, 0.07\n&lt;0.001\n        5-9 years\n0.02\n0.01, 0.04\n&lt;0.001\n        10-14 years\n0.03\n0.02, 0.05\n&lt;0.001\n        15-19 years\n0.10\n0.07, 0.13\n&lt;0.001\n        20-24 years\n0.17\n0.13, 0.22\n&lt;0.001\n        25-34 years\n0.20\n0.16, 0.25\n&lt;0.001\n        35-44 years\n0.31\n0.25, 0.39\n&lt;0.001\n        45-54 years\n0.72\n0.59, 0.89\n0.008\n        55-64 years\n1.69\n1.40, 2.06\n&lt;0.001\n        65-74 years\n3.44\n2.85, 4.19\n&lt;0.001\n        75-84 years\n8.73\n7.25, 10.6\n&lt;0.001\n        85+ years\n26.6\n22.1, 32.4\n&lt;0.001\n    State\n\n\n\n        California\n—\n—\n\n        Texas\n1.19\n1.14, 1.23\n&lt;0.001\n  \n  \n  \n    \n      1 IRR = Incidence Rate Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nWhile the overall pattern and substantive interpretation of the regular Poisson and the quasiPoisson models are identical, the standard errors of the parameters are not. We can see this by forming their ratios. The standard errors of the model parameters can be extracted from the summary()$coefficients of a model, specifically the second column of this table.\n\nsum1&lt;- summary(glmp_E)$coef[, 2]\nsum2&lt;- summary(glmqp)$coef[, 2]\n\ndata.frame(Poisson = sum1,\n           QPoisson= sum2,\n           coef = names(coef(glmp_E)))%&gt;%\n  ggplot(aes(y = QPoisson/Poisson, x= coef))+\n  geom_point()+\n  ylab(\"Ratio of Standard Errors\" )+\n  xlab (\"Parameter\")+\n  ggtitle(\"Ratio of Standard Errors in Poisson and QuasiPoisson Models\")+\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\nThis highly meaningful plot shows the ratio of all the standard errors is the same and is equal to the square root of the quasiPoisson dispersion parameter \\(\\phi\\), which can be seen in summary(glmqp)$dispersion, in this case it is 6.3079315, which is extremely close to the scale we calculated earlier. The quasiPoisson model has a disadvantage however, in that, per the ?glm documentation, is fit via quasi-likelihood methods and as such cannot be compared using AIC to other models.\n\n5.12.1 Modeling dispersion properly\nThe dispmod package implements the method of Breslow (1984) which adds an extra parameter to account for the overdispersion in the Poisson, and is fit via regular maximum likelihood, so it is comparable to other models. Effectively, this takes either a model fit via glm() with either a Binomial or Poisson family, and performs a re-weighted estimate of the model parameters.\n\nlibrary(dispmod)\n\nglmp_d &lt;- glm.poisson.disp(glmp_E,\n                           verbose = F)\n\nglmp_d%&gt;%\n  tbl_regression(exp = T)\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      IRR1\n      95% CI1\n      p-value\n    \n  \n  \n    factor(Age_Group)\n\n\n\n        &lt; 1 year\n—\n—\n\n        1-4 years\n0.05\n0.04, 0.05\n&lt;0.001\n        5-9 years\n0.02\n0.02, 0.03\n&lt;0.001\n        10-14 years\n0.03\n0.02, 0.03\n&lt;0.001\n        15-19 years\n0.10\n0.09, 0.10\n&lt;0.001\n        20-24 years\n0.17\n0.16, 0.19\n&lt;0.001\n        25-34 years\n0.20\n0.18, 0.22\n&lt;0.001\n        35-44 years\n0.31\n0.29, 0.34\n&lt;0.001\n        45-54 years\n0.72\n0.66, 0.79\n&lt;0.001\n        55-64 years\n1.70\n1.56, 1.85\n&lt;0.001\n        65-74 years\n3.45\n3.16, 3.76\n&lt;0.001\n        75-84 years\n8.73\n8.01, 9.52\n&lt;0.001\n        85+ years\n26.4\n24.2, 28.8\n&lt;0.001\n    State\n\n\n\n        California\n—\n—\n\n        Texas\n1.24\n1.20, 1.29\n&lt;0.001\n  \n  \n  \n    \n      1 IRR = Incidence Rate Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nIn this example the difference between California and Texas is now larger at a 24% higher mortality rate in Texas. How does this model compare to the regular Poisson model?\n\nAIC(glmp_E, glmp_d)\n\n       df      AIC\nglmp_E 14 773.7668\nglmp_d 14  90.3267\n\n\nThe dispersed model shows a much lower AIC suggesting that the dispersion accounted for in the the Breslow model is likely important. We can see how the dispersed Poisson model’s standard errors compare to those of the regular Poisson using the same plot we saw earlier:\n\n\n\n\n\nIn this case, the standard errors are not just a monotone transformation of the Poisson model errors, they each have their own scaling, which shows the dispersed Poisson model is doing something different compared to the quasiPoisson model by not using a single scaling factor for all the standard errors."
  },
  {
    "objectID": "macro.html#other-count-models",
    "href": "macro.html#other-count-models",
    "title": "4  Macrodemographic Analysis of Places",
    "section": "5.13 Other count models",
    "text": "5.13 Other count models\nThe Poisson and Binomial models are probably the most commonly used models for rates and proportions, but other models exist that offer more flexibility. Specifically the Beta distribution and the Negative Binomial distribution.\n\n5.13.1 Beta distribution model\nThe Beta distribution is a commonly used model for the prior distribution for the mean of the Binomial, as it has values between 0 and 1, but it is not used as often as an outcome distribution. It is indeed quite flexible as an outcome distribution for proportions because of its range of support, but also because it has an additional dispersion parameter, unlike the Binomial, so the pitfalls of overdispersion often present in the Binomial can be avoided. The betareg package (Grün, Kosmidis, and Zeileis 2012) implements the Beta regression model. The distribution has density:\n\\[\nf(y:p,q) = \\frac{\\Gamma(p +\nq)}{\\Gamma(p)\\Gamma(q)}x^{p - 1}(1 - x)^{q - 1}\n\\] Where \\(\\Gamma\\) is the Gamma function. The distribution, as parameterized in betareg has mean and variance:\n\\[\n\\mu = p/(p+q) \\\\\nVar = \\mu (1-\\mu)/ (1+\\phi)\n\\]\nWhere \\(phi\\) is a precision parameter. The model is linked to the linear predictor using one of the same link functions as the Binomial model: Logit, Probit or Complementary Log-Log.\n\\[g(\\mu_i) = X'\\beta = \\eta_i\\] The Logit link is parameterized as:\n\\[logit(g(\\mu)) = log(\\mu / (1-\\mu))\\] As described in their article on the betareg package (Cribari-Neto and Zeileis 2010), the model can also include a separate model for dispersion, so you can model the variance in the data as well as the mean.\nThe betareg::betareg() function implements model, and here is an application of the model to the low birth weight data used earlier:\n\nlibrary(betareg)\nglm_beta&lt;-betareg(I(lbrate1618/1000) ~  fampov14 + rucc + hpsa16, \n                  data=ahrf_m)\n\nsummary(glm_beta)\n\n\nCall:\nbetareg(formula = I(lbrate1618/1000) ~ fampov14 + rucc + hpsa16, data = ahrf_m)\n\nStandardized weighted residuals 2:\n    Min      1Q  Median      3Q     Max \n-7.0393 -0.6234 -0.0110  0.6338  3.7500 \n\nCoefficients (mean model with logit link):\n                                Estimate Std. Error  z value Pr(&gt;|z|)    \n(Intercept)                   -2.6387868  0.0150612 -175.204  &lt; 2e-16 ***\nfampov14                       0.0252868  0.0007936   31.862  &lt; 2e-16 ***\nrucc02                        -0.0188140  0.0150716   -1.248 0.211917    \nrucc03                        -0.0400036  0.0157581   -2.539 0.011129 *  \nrucc04                        -0.0798596  0.0177856   -4.490 7.12e-06 ***\nrucc05                        -0.0734111  0.0242750   -3.024 0.002493 ** \nrucc06                        -0.0494087  0.0142816   -3.460 0.000541 ***\nrucc07                        -0.0498176  0.0159746   -3.119 0.001817 ** \nrucc08                         0.0038585  0.0252486    0.153 0.878541    \nrucc09                        -0.0636793  0.0274465   -2.320 0.020334 *  \nhpsa16partial county shortage -0.0019801  0.0128139   -0.155 0.877196    \nhpsa16whole county shortage    0.0278359  0.0156183    1.782 0.074706 .  \n\nPhi coefficients (precision model with identity link):\n      Estimate Std. Error z value Pr(&gt;|z|)    \n(phi)  304.733      8.948   34.06   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood:  6347 on 13 Df\nPseudo R-squared: 0.355\nNumber of iterations: 23 (BFGS) + 3 (Fisher scoring) \n\n\nAnd the model with a separate model for the dispersion can be estimated by including a second formula separated by a | as:\n\nlibrary(betareg)\nglm_beta_o&lt;-betareg(I(lbrate1618/1000) ~  fampov14 + rucc + hpsa16 | scale(popn), \n                  data=ahrf_m)\n\nsummary(glm_beta_o)\n\n\nCall:\nbetareg(formula = I(lbrate1618/1000) ~ fampov14 + rucc + hpsa16 | scale(popn), \n    data = ahrf_m)\n\nStandardized weighted residuals 2:\n    Min      1Q  Median      3Q     Max \n-6.9251 -0.6169 -0.0023  0.6500  3.7111 \n\nCoefficients (mean model with logit link):\n                                Estimate Std. Error  z value Pr(&gt;|z|)    \n(Intercept)                   -2.6421609  0.0149203 -177.085  &lt; 2e-16 ***\nfampov14                       0.0251141  0.0007938   31.639  &lt; 2e-16 ***\nrucc02                        -0.0124912  0.0146382   -0.853  0.39348    \nrucc03                        -0.0327516  0.0154238   -2.123  0.03372 *  \nrucc04                        -0.0721414  0.0175194   -4.118 3.83e-05 ***\nrucc05                        -0.0655256  0.0241470   -2.714  0.00666 ** \nrucc06                        -0.0415704  0.0139537   -2.979  0.00289 ** \nrucc07                        -0.0417441  0.0156940   -2.660  0.00782 ** \nrucc08                         0.0118585  0.0252317    0.470  0.63837    \nrucc09                        -0.0550535  0.0274518   -2.005  0.04491 *  \nhpsa16partial county shortage -0.0048007  0.0127864   -0.375  0.70733    \nhpsa16whole county shortage    0.0270952  0.0156837    1.728  0.08406 .  \n\nPhi coefficients (precision model with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  5.72714    0.02936 195.063  &lt; 2e-16 ***\nscale(popn)  0.08419    0.02929   2.874  0.00405 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood:  6355 on 14 Df\nPseudo R-squared: 0.3548\nNumber of iterations: 21 (BFGS) + 5 (Fisher scoring) \n\n\nThe AIC can be used to determine if the dispersion model is preferred to the regular beta regression.\n\nAIC(glm_beta, glm_beta_o)\n\n           df       AIC\nglm_beta   13 -12668.24\nglm_beta_o 14 -12682.02\n\n\nIn this case, the dispersion model is preferred since the AIC is 13.78 points lower for the dispersion model.\n\n\n5.13.2 Negative binomial model\nWhen overdispersion is present, many researchers will automatically turn to the Negative Binomial as an alternative. The Negative Binomial sounds like it is some alternative to the Binomial distribution, but it is more like the Poisson. It effectively adds a second parameter to a model for integer counts. The model has been parameterized in several ways. The most common are the NB1 and NB2 models. The difference is how the model allows the variance to increase with the mean. The NB1 model allows for variance to increase linearly with the mean:\n\\[\nY \\sim NB (\\lambda, \\lambda+ \\theta \\lambda) \\\\\nE(Y) = \\lambda \\\\\n\\text{   } var(Y) = \\lambda+\\theta\\lambda \\\\\n\\lambda = log(\\eta) \\\\\n\\eta = \\beta_0 + \\beta_1 x_1+ log(n)\n\\] The NB2 model, which is the most commonly implemented in software, allows the variance to increase as a square with the mean.\n\\[\nY \\sim NB (\\lambda, \\lambda+ \\theta \\lambda^2) \\\\\nE(Y) = \\lambda \\\\\n\\text{   } var(Y) = \\lambda+ \\theta \\lambda^2 \\\\\n\\lambda = log(\\eta) \\\\\n\\eta = \\beta_0 + \\beta_1 x_1+ log(n)\n\\]\nThe base R glm() does not offer the NB1 model by default, but the standard MASS package (Venables and Ripley 2002) has the glm.nb() function. An alternative is the highly flexible gamlss package(Rigby and Stasinopoulos 2005), which includes the NB2 model, as well as a host of other distributions, plus the ability to model mean and variance, similar to how betareg() was achieving for the Beta distribution.\n\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:gtsummary':\n\n    select\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nglmnb&lt;- glm.nb(lowbw1618 ~ offset(log(births1618)) + fampov14 + rucc + hpsa16 ,\n               data=ahrf_m)\n\n\nglmnb%&gt;%\n  tbl_regression()\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      log(IRR)1\n      95% CI1\n      p-value\n    \n  \n  \n    % Families Below Poverty Level 2014-18\n0.03\n0.02, 0.03\n&lt;0.001\n    rucc\n\n\n\n        01\n—\n—\n\n        02\n-0.02\n-0.04, 0.01\n0.2\n        03\n-0.04\n-0.07, -0.02\n&lt;0.001\n        04\n-0.08\n-0.11, -0.05\n&lt;0.001\n        05\n-0.07\n-0.12, -0.03\n&lt;0.001\n        06\n-0.06\n-0.08, -0.03\n&lt;0.001\n        07\n-0.07\n-0.10, -0.04\n&lt;0.001\n        08\n-0.01\n-0.07, 0.06\n0.9\n        09\n-0.09\n-0.16, -0.01\n0.021\n    hpsa16\n\n\n\n        no shortage\n—\n—\n\n        partial county shortage\n-0.01\n-0.03, 0.01\n0.4\n        whole county shortage\n0.00\n-0.03, 0.03\n&gt;0.9\n  \n  \n  \n    \n      1 IRR = Incidence Rate Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nThe NB2 version of the model can be estimated using gamlss() with family=NBII:\n\nlibrary(gamlss)\n\nLoading required package: gamlss.data\n\n\n\nAttaching package: 'gamlss.data'\n\n\nThe following object is masked from 'package:datasets':\n\n    sleep\n\n\nLoading required package: gamlss.dist\n\n\nLoading required package: parallel\n\n\nRegistered S3 method overwritten by 'gamlss':\n  method   from\n  print.ri bit \n\n\n **********   GAMLSS Version 5.4-12  ********** \n\n\nFor more on GAMLSS look at https://www.gamlss.com/\n\n\nType gamlssNews() to see new features/changes/bug fixes.\n\nglmnb2&lt;-gamlss(lowbw1618 ~ offset(log(births1618)) + fampov14 + rucc + hpsa16,\n               family = NBII,\n               data=ahrf_m)\n\nGAMLSS-RS iteration 1: Global Deviance = 27778.63 \nGAMLSS-RS iteration 2: Global Deviance = 26730.85 \nGAMLSS-RS iteration 3: Global Deviance = 25316.94 \nGAMLSS-RS iteration 4: Global Deviance = 23326.65 \nGAMLSS-RS iteration 5: Global Deviance = 20580.7 \nGAMLSS-RS iteration 6: Global Deviance = 18480.22 \nGAMLSS-RS iteration 7: Global Deviance = 18277.94 \nGAMLSS-RS iteration 8: Global Deviance = 18277.03 \nGAMLSS-RS iteration 9: Global Deviance = 18277.03 \nGAMLSS-RS iteration 10: Global Deviance = 18277.03 \n\nsummary(glmnb2)\n\n******************************************************************\nFamily:  c(\"NBII\", \"Negative Binomial type II\") \n\nCall:  gamlss(formula = lowbw1618 ~ offset(log(births1618)) +  \n    fampov14 + rucc + hpsa16, family = NBII, data = ahrf_m) \n\nFitting method: RS() \n\n------------------------------------------------------------------\nMu link function:  log\nMu Coefficients:\n                                Estimate Std. Error  t value Pr(&gt;|t|)    \n(Intercept)                   -2.6762604  0.0122536 -218.407   &lt;2e-16 ***\nfampov14                       0.0189986  0.0006901   27.532   &lt;2e-16 ***\nrucc02                        -0.0124200  0.0080482   -1.543   0.1229    \nrucc03                        -0.0166503  0.0113856   -1.462   0.1438    \nrucc04                        -0.0401176  0.0163552   -2.453   0.0142 *  \nrucc05                        -0.0291354  0.0254089   -1.147   0.2516    \nrucc06                         0.0058763  0.0161169    0.365   0.7154    \nrucc07                         0.0068020  0.0211064    0.322   0.7473    \nrucc08                         0.0941871  0.0493463    1.909   0.0564 .  \nrucc09                         0.0208654  0.0546114    0.382   0.7024    \nhpsa16partial county shortage -0.0203470  0.0117608   -1.730   0.0838 .  \nhpsa16whole county shortage    0.0082867  0.0195254    0.424   0.6713    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nSigma link function:  log\nSigma Coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.72887    0.04354   16.74   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n------------------------------------------------------------------\nNo. of observations in the fit:  2324 \nDegrees of Freedom for the fit:  13\n      Residual Deg. of Freedom:  2311 \n                      at cycle:  10 \n \nGlobal Deviance:     18277.03 \n            AIC:     18303.03 \n            SBC:     18377.79 \n******************************************************************\n\n\nAgain, between these two different specifications, we see very similar results. We can compare each of the models, starting with the Gaussian model and proceeding through the NB2 model above (although the Beta regression appears to be on a different scale since the outcome is specified as probability, versus a count, so it is not comparable to the rest) using the AIC() function:\n\nAIC(glm1, glmb, glmp_b, glmnb, glmnb2)\n\n       df      AIC\nglm1   13 19599.34\nglmb   12 20980.41\nglmp_b 12 20517.93\nglmnb  13 16866.26\nglmnb2 13 18303.03\n\n\nWe see that among these five model specifications, the Binomial fits the worst and the NB1 model, estimated by glm.nb() fits the best, with the lowest AIC among these five models.\n\n\n\n\nAnselin, Luc. 1988. Spatial Econometrics: Methods and Models. Dordrecht: Springer Netherlands.\n\n\nBreslow, N. E. 1984. “Extra-Poisson Variation in Log-Linear Models.” Applied Statistics 33 (1): 38. https://doi.org/10.2307/2347661.\n\n\nBureau, US Census. 2019. “Worked Examples for Approximating Standard Errors Using American Community Survey Data.” https://www2.census.gov/programs-surveys/acs/tech_docs/accuracy/2018_ACS_Accuracy_Document_Worked_Examples.pdf.\n\n\nChi, Guangqing, and Jun Zhu. 2020. Spatial Regression Models for the Social Sciences. Los Angeles: SAGE.\n\n\nCourtemanche, Charles, James Marton, Benjamin Ukert, Aaron Yelowitz, and Daniela Zapata. 2019. “Effects of the Affordable Care Act on Health Behaviors After 3 Years.” Eastern Economic Journal 45 (1): 7–33. https://doi.org/10.1057/s41302-018-0119-4.\n\n\nCribari-Neto, Francisco, and Achim Zeileis. 2010. “Beta Regression in r.” Journal of Statistical Software 34 (2). https://doi.org/10.18637/jss.v034.i02.\n\n\nDivision, United Nations Population. 1983. “Manual X: Indirect Techniques for Demographic Estimation.” New York, NY. https://www.un.org/en/development/desa/population/publications/manual/estimate/demographic-estimation.asp.\n\n\nElhorst, J. Paul. 2014. Spatial Econometrics: From Cross-Sectional Data to Spatial Panels. 1st ed. 2014. SpringerBriefs in Regional Science. Berlin, Heidelberg: Springer Berlin Heidelberg : Imprint: Springer. https://doi.org/10.1007/978-3-642-40340-8.\n\n\nFox, John. 2016. Applied Regression Analysis and Generalized Linear Models. Third Edition. Los Angeles: SAGE.\n\n\nGrün, Bettina, Ioannis Kosmidis, and Achim Zeileis. 2012. “Extended Beta Regression in R: Shaken, Stirred, Mixed, and Partitioned.” Journal of Statistical Software 48 (11): 1–25. https://doi.org/10.18637/jss.v048.i11.\n\n\nKeyfitz, Nathan. 1968. Introduction to the Mathematics of Population. Addison-Wesley Publishing Company.\n\n\nLeSage, James P., and R. Kelley Pace. 2009. Introduction to Spatial Econometrics. Statistics, Textbooks and Monographs. Boca Raton: CRC Press.\n\n\nLong, J. Scott, and Laurie H. Ervin. 2000. “Using Heteroscedasticity Consistent Standard Errors in the Linear Regression Model.” The American Statistician 54 (3): 217–24. https://doi.org/10.1080/00031305.2000.10474549.\n\n\nMacKinnon, James G, and Halbert White. 1985. “Some Heteroskedasticity-Consistent Covariance Matrix Estimators with Improved Finite Sample Properties.” Journal of Econometrics 29 (3): 305–25. https://doi.org/10.1016/0304-4076(85)90158-7.\n\n\nManson, Steven, Jonathan Schroeder, David Van Riper, Tracy Kugler, and Steven Ruggles. 2021. IPUMS National Historical Geographic Information System: Version 16.0 [Dataset]. https://doi.org/http://doi.org/10.18128/D050.V16.0.\n\n\nMcCullagh, P., and John A. Nelder. 1998. Generalized Linear Models. 2nd ed. Monographs on Statistics and Applied Probability 37. Boca Raton: Chapman & Hall/CRC.\n\n\nMcLaughlin, Diane K., C. Shannon Stokes, P. Johnelle Smith, and A Nonoyama. 2007. “Differential Mortality Across the United States: The Influence of Place-Based Inequality.” In Sociology of Spatial Inequality, 141–62. State University of New York Press.\n\n\nNelder, J. A., and R. W. M. Wedderburn. 1972. “Generalized Linear Models.” Journal of the Royal Statistical Society: Series A (General) 135 (3): 370–84. https://doi.org/10.2307/2344614.\n\n\nPinheiro, José C., and Douglas M. Bates. 2000. Mixed-Effects Models in S and S-PLUS. Statistics and Computing. New York: Springer.\n\n\nRigby, R. A., and D. M. Stasinopoulos. 2005. “Generalized Additive Models for Location, Scale and Shape,(with Discussion).” Applied Statistics 54.3: 507–54.\n\n\nSjoberg, Daniel D., Karissa Whiting, Michael Curry, Jessica A. Lavery, and Joseph Larmarange. 2021. “Reproducible Summary Tables with the Gtsummary Package.” The R Journal 13: 570–80. https://doi.org/10.32614/RJ-2021-053.\n\n\nSoni, Aparna, Michael Hendryx, and Kosali Simon. 2017. “Medicaid Expansion Under the Affordable Care Act and Insurance Coverage in Rural and Urban Areas.” The Journal of Rural Health 33 (2): 217–26. https://doi.org/10.1111/jrh.12234.\n\n\nSparks, Patrice Johnelle, and Corey S. Sparks. 2010. “An Application of Spatially Autoregressive Models to the Study of US County Mortality Rates: An Application of Spatially Autoregressive Models.” Population, Space and Place 16 (6): 465–81. https://doi.org/10.1002/psp.564.\n\n\nVenables, W. N., and B. D. Ripley. 2002. Modern Applied Statistics with s. Fourth. New York: Springer. https://www.stats.ox.ac.uk/pub/MASS4/.\n\n\nVoss, Paul R. 2007. “Demography as a Spatial Social Science.” Population Research and Policy Review 26 (5-6): 457–76. https://doi.org/10.1007/s11113-007-9047-4.\n\n\nWalker, Kyle. 2021. Tigris: Load Census TIGER/Line Shapefiles. https://CRAN.R-project.org/package=tigris.\n\n\nWalker, Kyle, and Matt Herman. 2021. Tidycensus: Load US Census Boundary and Attribute Data as ’Tidyverse’ and ’Sf’-Ready Data Frames. https://CRAN.R-project.org/package=tidycensus.\n\n\nWatson, Oliver J, Rich FitzJohn, and Jeffrey W Eaton. 2019. “Rdhs: An r Package to Interact with the Demographic and Health Surveys (DHS) Program Datasets.” Wellcome Open Research 4: 103. https://doi.org/10.12688/wellcomeopenres.15311.1.\n\n\nWhite, Halbert. 1980. “A Heteroskedasticity-Consistent Covariance Matrix Estimator and a Direct Test for Heteroskedasticity.” Econometrica 48 (4): 817. https://doi.org/10.2307/1912934."
  },
  {
    "objectID": "macro.html#footnotes",
    "href": "macro.html#footnotes",
    "title": "4  Macrodemographic Analysis of Places",
    "section": "",
    "text": "\\(\\bar{y}= \\text{mean of y}\\), \\(\\bar{x}= \\text{mean of x}\\)↩︎\nhttp://api.census.gov/data/key_signup.html↩︎\nLink to AHRF codebook - “https://data.hrsa.gov/DataDownload/AHRF/AHRF_USER_TECH_2019-2020.zip”↩︎\nMore on this below↩︎\nMore on this below↩︎\nLink to AHRF codebook - “https://data.hrsa.gov/DataDownload/AHRF/AHRF_USER_TECH_2019-2020.zip”↩︎"
  },
  {
    "objectID": "micro.html",
    "href": "micro.html",
    "title": "5  Event History Models for Microdemography",
    "section": "",
    "text": "6 Micro-demography\nDemographic studies began to focus on individual level outcomes once the availability of individual level census and social/health survey data became more prevalent in the 1960’s and 1970’s. Corresponding with this availability of individual level data, demography began to be influenced by sociological thought, which brought a proper theoretical perspective to demographic studies, which, before this period were dominated by methodological issues and descriptions of macro scale demographic processes we saw in the previous chapter.\nThis chapter focuses on how we model individual level observational data, with a focus on data from demographic surveys. In doing this, I focus on two primary goals, first to illustrate how to conduct commonly used regression analysis of demographic outcomes as measured in surveys, drawing heavily on data from the ******. Secondly, I describe the event-history framework that is used in much of social science when we analyze changes in outcomes between time points, or in describing the time to some demographic event, such as having a child or dying."
  },
  {
    "objectID": "micro.html#event-history-framework",
    "href": "micro.html#event-history-framework",
    "title": "5  Event History Models for Microdemography",
    "section": "6.1 Event history framework",
    "text": "6.1 Event history framework\n\n6.1.1 When to conduct an event history analysis?\n\nWhen you questions include\n\nWhen or Whether\nWhen &gt; how long until an event occurs\nWhether &gt; does an event occur or not\n\nIf your question does not include either of these ideas (or cannot be made to) then you do not need to do event history analysis\n\n\n\n6.1.2 Basic Propositions\n\nSince most of the methods we will discuss originate from studies of mortality, they have morbid names\n\nSurvival – This is related to how long a case lasts until it experiences the event of interest\nHow long does it take?\nRisk – How likely is it that the case will experience the event\nWill it happen or not?\n\n\n\n\n6.1.3 Focus on comparison\n\nMost of the methods we consider are comparative by their nature\nHow long does a case with trait x survive, compared to a case with trait y?\nHow likely is it for a person who is married to die of homicide relative to someone who is single?\nGenerally we are examining relative risk and relative survival\n\n\n\n6.1.4 Some terminology\n\nState – discrete condition an individual may occupy that occur within a state space. Most survival analysis methods assume a single state to state transition\nState space – full set of state alternatives\nEpisodes/Events/Transitions – a change in states\nDurations – length of an episode\nTime axis – Metric for measuring durations (days, months, years)\n\n\n\n\nState Space Illustration\n\n\n\n\n6.1.5 Issues in event history data\n\n6.1.5.1 Censoring\n\nCensoring occurs when you do not actually observe the event of interest within the period of data collection\n\ne.g. you know someone gets married, but you never observe them having a child\ne.g. someone leaves alcohol treatment and is never observed drinking again\n\n\n\n\n\nCensoring\n\n\n\n\n6.1.5.2 Non-informative censoring\n\nThe individual is not observed because the observer ends the study period\nThe censoring is not related to any trait or action of the case, but related to the observer\n\nWe want most of our censoring to be this kind\n\n\n\n\n6.1.5.3 Informative censoring\n\nThe individual is not observed because they represent a special case\nThe censoring IS related to something about the individual, and these people differ inherently from uncensored cases\nPeople that are censored ARE likely to have experience the event\n\n\n\n6.1.5.4 Right censoring\n\nAn event time is unknown because it is not observed.\n\nThis is easier to deal with\n\n\n\n\n6.1.5.5 Left censoring\n\nAn event time is unknown because it occurred prior to the beginning of data collection, but not when\n\nThis is difficult to deal with\n\n\n\n\n6.1.5.6 Interval censoring\n\nThe event time is known to have occurred within a period of time, but it is unknown exactly when\n\nThis can be dealt with"
  },
  {
    "objectID": "micro.html#time-scales",
    "href": "micro.html#time-scales",
    "title": "5  Event History Models for Microdemography",
    "section": "6.2 Time Scales",
    "text": "6.2 Time Scales\n\nContinuous time\n\nTime is measured in very precise, unique increments &gt; miles until a tire blows out\nEach observed duration is unique\n\nDiscrete time\n\nTime is measured in discrete lumps &gt; semester a student leaves college\nEach observed duration is not necessarily unique, and takes one of a set of discrete values\n\n\n\n\n\nTime Scales\n\n\n\n6.2.0.1 Making continuous outcomes discrete\n\nIdeally you should measure the duration as finely as possible (see Freedman et al)\nOften you may choose to discretize the data &gt; take continuous time and break it into discrete chunks\nProblems\n\nThis removes possibly informative information on duration variability\nAny discrete dividing point is arbitrary\nYou may arrive at different conclusions given the interval you choose\nYou lose information about late event occurrence\nLose all information on mean or average durations"
  },
  {
    "objectID": "micro.html#kinds-of-studies-with-event-history-data",
    "href": "micro.html#kinds-of-studies-with-event-history-data",
    "title": "5  Event History Models for Microdemography",
    "section": "6.3 Kinds of studies with event history data",
    "text": "6.3 Kinds of studies with event history data\n\nCross sectional\n\nMeasured at one time point (no change observed)\nCan measure lots of things at once\n\nPanel data\n\nMultiple measurements at discrete time points on the same individuals\nCan look at change over time\n\nEvent history\n\nContinuous measurement of units over a fixed period of time, focusing on change in states\nThink clinical follow-ups\n\nLongitudinal designs\n\nProspective designs\nStudies that follow a group (cohort) and follow them over time\nExpensive and take a long time, but can lead to extremely valuable information on changes in behaviors\n\nRetrospective designs\nTaken at a cross section\nAsk respondents about events that have previously occurred.\nGenerate birth/migration/marital histories for individuals\nProblems with recall bias\nDHS includes a detailed history of births over the last 5 years\nRecord linkage procedures\n\nBegin with an event of interest (birth, marriage) and follow individuals using various record types\nBirth &gt; Census 1880 &gt; Census 1890 &gt; Marriage &gt; Birth of children &gt; Census 1900 &gt; Tax records &gt;Death certificate\nMostly used in historical studies\nModern studies link health surveys to National Death Index (NHANES, NHIS)"
  },
  {
    "objectID": "micro.html#some-arrangements-for-event-history-data",
    "href": "micro.html#some-arrangements-for-event-history-data",
    "title": "5  Event History Models for Microdemography",
    "section": "7.1 Some arrangements for event history data",
    "text": "7.1 Some arrangements for event history data\n\n7.1.1 Counting process data\n\nThis is what we are accustomed to in the life table\n\n\nt1&lt;-data.frame(Time_start=c(1,2,3,4),\n               Time_end=c(2,3,4,5),\n               Failing=c(25,15,12,20),\n               At_Risk=c(100, 75, 60, 40))\nt1%&gt;%\n  kable()%&gt;%\n  column_spec(1:4, border_left = T, border_right = T)%&gt;%\n  kable_styling()\n\n\n\n\nTime_start\nTime_end\nFailing\nAt_Risk\n\n\n\n\n1\n2\n25\n100\n\n\n2\n3\n15\n75\n\n\n3\n4\n12\n60\n\n\n4\n5\n20\n40\n\n\n\n\n\n\n#knitr::kable(t1,format = \"html\", caption = \"Counting Process data\" ,align = \"c\", )\n\n\n\n7.1.2 Case - duration, or person level data\n\nThis is the general form of continuous time survival data.\n\n\nt2&lt;-data.frame(ID = c(1,2,3,4),\n               Duration=c(5, 2, 9 , 6), \n               Event_Occurred=c(\"Yes (1)\",\"Yes (1)\",\"No (0)\", \"Yes (1)\" ))\nt2%&gt;%\n  kable()%&gt;%\n  column_spec(1:3, border_left = T, border_right = T)%&gt;%\n  kable_styling(row_label_position = \"c\", position = \"center\" )\n\n\n\n\nID\nDuration\nEvent_Occurred\n\n\n\n\n1\n5\nYes (1)\n\n\n2\n2\nYes (1)\n\n\n3\n9\nNo (0)\n\n\n4\n6\nYes (1)\n\n\n\n\n\n\n#knitr::kable(t2, format = \"html\", caption = \"Case-duration data\", align = \"c\")\n\nThis can be transformed into person-period data, or discrete time data.\n\n\n7.1.3 Person – Period data\n\nExpress exposure as discrete periods\nEvent occurrence is coded at each period\n\n\nt3&lt;-data.frame(ID=c(rep(1, 5), rep(2, 2), rep(3, 9), rep(4, 6)),\n                    Period = c(seq(1:5), seq(1:2), seq(1:9), seq(1:6)),\n                    Event=c(0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0))\nt3%&gt;%\n  kable()%&gt;%\n  column_spec(1:3, border_left = T, border_right = T)%&gt;%\n  kable_styling(row_label_position = \"c\", position = \"center\" )\n\n\n\n\nID\nPeriod\nEvent\n\n\n\n\n1\n1\n0\n\n\n1\n2\n0\n\n\n1\n3\n0\n\n\n1\n4\n0\n\n\n1\n5\n1\n\n\n2\n1\n0\n\n\n2\n2\n1\n\n\n3\n1\n0\n\n\n3\n2\n0\n\n\n3\n3\n0\n\n\n3\n4\n0\n\n\n3\n5\n0\n\n\n3\n6\n0\n\n\n3\n7\n0\n\n\n3\n8\n0\n\n\n3\n9\n0\n\n\n4\n1\n0\n\n\n4\n2\n0\n\n\n4\n3\n0\n\n\n4\n4\n0\n\n\n4\n5\n0\n\n\n4\n6\n0"
  },
  {
    "objectID": "micro.html#homage-to-the-life-table",
    "href": "micro.html#homage-to-the-life-table",
    "title": "5  Event History Models for Microdemography",
    "section": "7.2 Homage to the life table",
    "text": "7.2 Homage to the life table\nIn life tables, we had lots of functions of the death process. Some of these were more interesting than others, with two being of special interest to use here. These are the \\(l(x)\\) and \\(q(x, n)\\) functions. If you recall, \\(l(x)\\) represents the population size of the stationary population that is alive at age \\(x\\), and the risk of dying between age \\(x, x+n\\) is \\(q(x, n)\\).\nThese are genearlized more in the event history analysis literature, but we can still describe the distrubion of survival time using three functions. These are the Survival Function, \\(S(t)\\), the probability density function, \\(f(t)\\), and the hazard function, \\(h(t)\\). These three are related and we can derive one from the others.\nNow we must generalize these ideas to incorporate them into the broader event-history framework\nSurvival/duration times measure the time to a certain event.\nThese times are subject to random variations, and are considered to be random iid (independent and identically distributed; random) variates from some distribution\n\nThe distribution of survival times is described by 3 functions\nThe survivorship function, \\(S(t)\\)\nThe probability density function, \\(f(t)\\)\nThe hazard function, \\(h(t)\\)\n\n\n\n\n3 functions\n\n\n\nFt&lt;-cumsum(dlnorm(x = seq(0, 110, 1), meanlog = 4.317488, sdlog = 2.5)) #mean of 75 years, sd of 12.1 years\nft&lt;-diff(Ft)\nSt&lt;-1-Ft\nht&lt;-ft/St[1:110]\nplot(Ft, ylim=c(0,1))\nlines(St, col=\"red\")\n\n\n\nplot(ht, col=\"green\")\n\n\n\n\n\nThese three are mathematically related, and if given one, we can calculate the others\n\nThese 3 functions each represent a different aspect of the survival time distribution.\n\nThe fundamental problem in survival analysis is coming up with a way to estimate these functions.\n\n\n7.2.1 Defining the functions\nLet T denote the survival time, our goal is to characterize the distribution of T using these 3 functions.\nLet T be a discrete(or continuous) iid random variable and let \\(t_i\\), be an occurrence of that variable, such that \\(Pr(t_i)=Pr(T=t_i)\\)\n\n\n7.2.2 The distribution function, or pdf\nLike any other random variates survival times have a simple distribution function that gives the probability of observing a particular survival time within a finite interval\nThe density function is defined as the limit of the probability that an individual fails (experiences the event) in a short interval \\(t+\\Delta t\\) (read delta t), per width of \\(\\Delta t\\), or simply the probability of failure in a small interval per unit time, \\(f(t_i) = Pr(T=t_i)\\).\nIf \\(F(t)\\) is the cumulative distribution function for T, given by:\n\\[ F(t) = \\int_{0}^{t} f(u) du = Pr(T \\leqslant t )\\] Which is the probability of observing a value of T prior to the current value, t.\nThe density function is then:\n\\[ f(t) = \\frac{F(t)}{d(t)} = F'(t)\\] or\n\\[ f(t) = \\lim_{\\delta t \\rightarrow 0} \\frac{F(t+\\Delta t) - F(t)}{\\Delta t}\\]\nThe density function gives the unconditional instantaneous failure rate in the (very small) interval between t and dt, \\(\\Delta t\\)\n\n\n7.2.3 Survival Function\nThe survival function, S(t) is expressed:\n\\[ S(t) = 1 - F(t) = Pr (T \\geqslant t)\\]\nWhich is the probability that T takes a value larger than t. i.e. the event happens some time after the present time.\nAt \\(t = 0, S(t) =1 \\text { and at } t= \\infty, \\text { and } S(t) =0\\)\nAs time passes, S(t) decreases, and is called a strictly decreasing function of time.\nEmpirically, S(t) takes the form of a step function:\n\nSt&lt;- c(1, cumprod(1-(t1$Failing/t1$At_Risk)))\n\nplot(St, type=\"s\", xlab = \"Time\", ylab=\"S(t)\", ylim=c(0,1))\n\n\n\n\n\n\n7.2.4 The hazard function\nThe hazard function relates death, f(t), and survival, S(t), to one another\n\\[h(t) = \\frac{f(t)}{S(t)}\\] \\[h(t) = \\lim_{\\Delta t \\rightarrow 0} \\frac{Pr(t \\leqslant T \\leqslant t + \\Delta t | T \\geqslant t)}{\\Delta t}\\]\nWhich is the failure rate per unit time in the interval t, \\(t+\\Delta t\\), the hazard may increase or decrease with time, or stay the same. This is really dependent on the distribution of failure times."
  },
  {
    "objectID": "micro.html#relationships-among-the-three-functions",
    "href": "micro.html#relationships-among-the-three-functions",
    "title": "5  Event History Models for Microdemography",
    "section": "7.3 Relationships among the three functions",
    "text": "7.3 Relationships among the three functions\nIf \\(ft = \\frac{dF(t)}{dt}\\) and \\(S(t) = 1- F(t)\\) and, \\(h(t) = \\frac{f(t)}{S(t)}\\), then we can write:\n\\[f(t) = \\frac{-dS(t)}{dt}\\]\nand the hazard function as:\n\\[h(t) = \\frac{-d \\text{ log } S(t)}{dt}\\]\nIf we integrate this and let \\(S(0)=1\\), then\n\\[S(t) = exp^{-\\int_{0}^t h(u) du} = e^{-H(t)}\\]\nwhere the quantity, \\(H(t)\\) is called the cumulative hazard function and, \\(H(t) = \\int h(u) du\\), then\n\\[H(t) = -\\text{log }  S(t)\\]\nThe density can be written as:\n\\[f(t) = h(t) e ^{-H(t)}\\] and\n\\[h(t) = \\frac{h(t) e^{-H(t)}}{e^{-H(t)}} = \\frac{f(t)}{S(t)}\\]\n\n7.3.1 More on the hazard function…\nUnlike the f(t) or S(t), h(t) describes the risk an individual faces of experiencing the event, given they have survived up to that time.\nThis kind of conditional probability is of special interest to us.\nWe can extend this framework to include effects of individual characteristics on one’s risk, thus not only introducing dependence on time, but also on these characteristics (covariates).\nWe can re-express the hazard rate with both these conditions as:\n\\[h(t|x) = \\lim_{\\Delta t \\rightarrow 0} \\frac{Pr(t \\leqslant T \\leqslant t + \\Delta t | T \\geqslant t, x)}{\\Delta t}\\] ## Logistic regression When a variable is coded as binary, meaning either 1 or 0, the Bernoulli distribution is used, as in the logistic regression model. When coded like this, the model tries to use the other measured information to predict the 1 value versus the 0 value. So in the basic sense, we want to construct a model like:\n\\[Pr(y=1) =\\pi =  \\text{some function of predictors}\\]\n\n\n\n\nEntwisle, Barbara. 2007. “Putting People into Place.” Demography 44 (4): 687–703. https://doi.org/10.1353/dem.2007.0045.\n\n\nLumley, Thomas. 2004. “Analysis of Complex Survey Samples.” Journal of Statistical Software 9 (1): 1–19.\n\n\n———. 2021. svyVGAM: Design-Based Inference in Vector Generalised Linear Models. https://CRAN.R-project.org/package=svyVGAM."
  }
]